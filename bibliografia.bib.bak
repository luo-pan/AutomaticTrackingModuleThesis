% This file was created with JabRef 2.10b2.
% Encoding: UTF8


@InProceedings{AlHaj2010,
  Title                    = {{R}eactive {O}bject {T}racking with a {S}ingle {PTZ} {C}amera},
  Author                   = {Al Haj, Murad and Bagdanov, Andrew D. and Gonzalez, Jordi and Roca, F. Xavier},
  Booktitle                = {20th International Conference on Pattern Recognition (ICPR)},
  Year                     = {2010},

  Address                  = {Turkey, Istanbul},
  Month                    = {August},
  Pages                    = {1690 - 1693},
  Publisher                = {IEEE},

  Abstract                 = {{I}n this paper we describe a novel approach to reactive tracking of moving targets with a pan-tilt-zoom camera. {T}he approach uses an extended {K}alman filter to jointly track the object position in the real world, its velocity in 3{D} and the camera intrinsics, in addition to the rate of change of these parameters. {T}he filter outputs are used as inputs to {PID} controllers which continuously adjust the camera motion in order to reactively track the object at a constant image velocity while simultaneously maintaining a desirable target scale in the image plane. {W}e provide experimental results on simulated and real tracking sequences to show how our tracker is able to accurately estimate both 3{D} object position and camera intrinsics with very high precision over a wide range of focal lengths.},
  Doi                      = {10.1109/ICPR.2010.418},
  File                     = {:sources/05595809.pdf:PDF},
  Keywords                 = {Cameras, Estimation, Kalman filters, Position measurement, Target tracking, Three dimensional displays},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.06.07}
}

@Article{Alenya2014,
  Title                    = {{T}o{F} cameras for active vision in robotics},
  Author                   = {Alenyà, G. and Foix, S. and Torras, C.},
  Journal                  = {Sensors and Actuators A: Physical},
  Year                     = {2014},
  Pages                    = {10 - 22},
  Volume                   = {218},

  Abstract                 = {{A}bstract {T}o{F} cameras are now a mature technology that is widely being adopted to provide sensory input to robotic applications. {D}epending on the nature of the objects to be perceived and the viewing distance, we distinguish two groups of applications: those requiring to capture the whole scene and those centered on an object. {I}t will be demonstrated that it is in this last group of applications, in which the robot has to locate and possibly manipulate an object, where the distinctive characteristics of {T}o{F} cameras can be better exploited. {A}fter presenting the physical sensor features and the calibration requirements of such cameras, we review some representative works highlighting for each one which of the distinctive {T}o{F} characteristics have been more essential. {E}ven if at low resolution, the acquisition of 3{D} images at frame-rate is one of the most important features, as it enables quick background/foreground segmentation. {A} common use is in combination with classical color cameras. {W}e present three developed applications, using a mobile robot and a robotic arm, to exemplify with real images some of the stated advantages.},
  Doi                      = {10.1016/j.sna.2014.07.014},
  File                     = {:sources/1-s2.0-S0924424714003458-main.pdf:PDF},
  ISSN                     = {0924-4247},
  Keywords                 = {Time-of-Flight cameras, 3D perception for manipulation, Depth calibration, Outdoor imaging, Complex-shape objects},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0924424714003458}
}

@Article{Arulampalam2002,
  Title                    = {{A} tutorial on particle filters for online nonlinear/non-{G}aussian {B}ayesian tracking},
  Author                   = {M. Sanjeev Arulampalam and Simon Maskell and Neil Gordon and Tim Clapp},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2002},

  Month                    = {February},
  Number                   = {2},
  Pages                    = {174 - 188},
  Volume                   = {50},

  Abstract                 = {{I}ncreasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-{G}aussianity in order to model accurately the underlying dynamics of a physical system. {M}oreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. {I}n this paper, we review both optimal and suboptimal {B}ayesian algorithms for nonlinear/non-{G}aussian tracking problems, with a focus on particle filters. {P}article filters are sequential {M}onte {C}arlo methods based on point mass (or "particle") representations of probability densities, which can be applied to any state-space model and which generalize the traditional {K}alman filtering methods. {S}everal variants of the particle filter such as {SIR}, {ASIR}, and {RPF} are introduced within a generic framework of the sequential importance sampling ({SIS}) algorithm. {T}hese are discussed and compared with the standard {EKF} through an illustrative example},
  Doi                      = {10.1109/78.978374},
  File                     = {:sources/00978374.pdf:PDF},
  Keywords                 = {Bayesian methods, Costs, Filtering, Kalman filters, Monte Carlo methods, Nonlinear dynamical systems, Particle filters, Particle tracking, Signal processing, Tutorial},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.19}
}

@Book{Austin2010,
  Title                    = {{U}nmanned {A}ircraft {S}ystems - {UAVS} design, development and deployment},
  Author                   = {Austin, Reg},
  Publisher                = {John Wiley and Sons},
  Year                     = {2010},

  File                     = {:sources/Unmanned Air Systems UAV Design, Development and Deployment.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Bahn2011,
  Title                    = {{A} motion-information-based vision-tracking system with a stereo camera on mobile robots},
  Author                   = {Bahn, Wook and Park, Jaehong and Lee, Chang-hun and Kim, Kwang-soo and Lee, Teajae and Shaikh, M.M. and Kim, Kwang-soo and Cho, Dong-Il},
  Booktitle                = {IEEE Conference on Robotics, Automation and Mechatronics},
  Year                     = {2011},

  Address                  = {China, Qingdao},
  Month                    = {September},
  Pages                    = {252 - 257},
  Publisher                = {EEE},

  Abstract                 = {{T}his paper presents a vision-tracking system for a mobile robot, using robot motion and stereo vision data. {T}he mobile robot has an actuator module which pans and tilts an integrated stereo camera. {T}he proposed system controls the actuator to maintain the line of sight of the stereo camera towards a stationary target by using the robot motion data. {T}he robot motion data are obtained from a gyroscope and encoders of the mobile robot. {T}he stereo vision data from the camera is used to compensate for errors in the motion measurements, and to prevent a long term error accumulation. {T}his vision-based compensation is used only when the robot stops or moves slowly, because the long vision processing times can cause the loop time to overrun. {T}he proposed system is experimentally evaluated while the robot moves on a trajectory.},
  Doi                      = {10.1109/RAMECH.2011.6070491},
  File                     = {:sources/06070491.pdf:PDF},
  Keywords                 = {Cameras, Mobile robots, Robot kinematics, Robot vision systems, Stereo vision, Vectors},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Baker2004,
  Title                    = {{L}ucas-{K}anade 20 {Y}ears {O}n: {A} {U}nifying {F}ramework},
  Author                   = {Baker, Simon and Matthews, Iain},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2004},

  Month                    = {February},
  Number                   = {3},
  Pages                    = {221-255},
  Volume                   = {56},

  Abstract                 = {{S}ince the {L}ucas-{K}anade algorithm was proposed in 1981 image alignment has become one of the most widely used techniques in computer vision. {A}pplications range from optical flow and tracking to layered motion, mosaic construction, and face coding. {N}umerous algorithms have been proposed and a wide variety of extensions have been made to the original formulation. {W}e present an overview of image alignment, describing most of the algorithms and their extensions in a consistent framework. {W}e concentrate on the inverse compositional algorithm, an efficient algorithm that we recently proposed. {W}e examine which of the extensions to {L}ucas-{K}anade can be used with the inverse compositional algorithm without any significant loss of efficiency, and which cannot. {I}n this paper, {P}art 1 in a series of papers, we cover the quantity approximated, the warp update rule, and the gradient descent approximation. {I}n future papers, we will cover the choice of the error function, how to allow linear appearance variation, and how to impose priors on the parameters.},
  Doi                      = {10.1023/B:VISI.0000011205.11775.fd},
  File                     = {:sources/Baker&Matthews.pdf:PDF},
  Keywords                 = {image alignment, Lucas-Kanade, a unifying framework, additive vs. compositional algorithms, forwards vs. inverse algorithms, the inverse compositional algorithm, efficiency, steepest descent, Gauss-Newton, Newton, Levenberg-Marquardt},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.08}
}

@Article{Baker2011,
  Title                    = {{A} {D}atabase and {E}valuation {M}ethodology for {O}ptical {F}low},
  Author                   = {Baker, Simon and Scharstein, Daniel and Lewis, J. P. and Roth, Stefan and Black, Michael J. and Szeliski, Richard},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {1 - 31},
  Volume                   = {92},

  Abstract                 = {{T}he quantitative evaluation of optical flow algorithms by {B}arron et al. (1994) led to significant advances in performance. {T}he challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. {I}nstead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. {W}e propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. {T}o that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. {I}n addition to the average angular error used by {B}arron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. {I}n {O}ctober 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. {W}e also made the data freely available on the web at http://​vision.​middlebury.​edu/​flow/​. {S}ubsequently a number of researchers have uploaded their results to our website and published papers using the data. {A} significant improvement in performance has already been achieved. {I}n this paper we analyze the results obtained to date and draw a large number of conclusions from them.},
  Doi                      = {10.1007/s11263-010-0390-2},
  File                     = {:sources/art%3A10.1007%2Fs11263-010-0390-2.pdf:PDF},
  Keywords                 = {Optical flow, Survey, Algorithms, Database, Benchmarks, Evaluation, Metrics},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.14}
}

@InProceedings{Benezeth2008,
  Title                    = {{R}eview and evaluation of commonly-implemented background subtraction algorithms},
  Author                   = {Benezeth, Y. and Jodoin, P.-M. and Emile, B. and Laurent, H. and Rosenberger, C.},
  Booktitle                = {19th International Conference on Pattern Recognition},
  Year                     = {2008},

  Address                  = {USA, FL, Tampa},
  Month                    = {December},
  Pages                    = {1 - 4},
  Publisher                = {IEEE},

  Abstract                 = {{L}ocating moving objects in a video sequence is the first step of many computer vision applications. {A}mong the various motion-detection techniques, background subtraction methods are commonly implemented, especially for applications relying on a fixed camera. {S}ince the basic inter-frame difference with global threshold is often a too simplistic method, more elaborate (and often probabilistic) methods have been proposed. {T}hese methods often aim at making the detection process more robust to noise, background motion and camera jitter. {I}n this paper, we present commonly-implemented background subtraction algorithms and we evaluate them quantitatively. {I}n order to gauge performances of each method, tests are performed on a wide range of real, synthetic and semi-synthetic video sequences representing different challenges.},
  Doi                      = {10.1109/ICPR.2008.4760998},
  File                     = {:sources/04760998.pdf:PDF},
  Keywords                 = {Application software, Background noise, Cameras, Computer vision, Jitter, Motion detection, Noise robustness, Performance evaluation, Testing, Video sequences},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@TechReport{Bouguet2000,
  Title                    = {{P}yramidal implementation of the {L}ucas {K}anade feature tracker - {D}escription of the algorithm},
  Author                   = {Bouguet, Jean-Yves},
  Institution              = {Intel Corporation - Microprocessor Research Labs},
  Year                     = {2000},

  File                     = {:sources/algo_tracking.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.08}
}

@InProceedings{Bradski1998,
  Title                    = {{R}eal time face and object tracking as a component of a perceptual user interface},
  Author                   = {Gary R. Bradski},
  Booktitle                = {Fourth IEEE Workshop Applications of Computer Vision (WACV)},
  Year                     = {1998},

  Address                  = {USA, NJ, Princeton},
  Month                    = {October},
  Pages                    = {214 - 219},
  Publisher                = {IEEE},

  Abstract                 = {{A}s a step towards a perceptual user interface, an object tracking algorithm is developed and demonstrated tracking human faces. {C}omputer vision algorithms that are intended to form part of a perceptual user interface must be fast and efficient. {T}hey must be able to track in real time and yet not absorb a major share of computational resources. {A}n efficient, new algorithm is described here based on the mean shift algorithm. {T}he mean shift algorithm robustly finds the mode (peak) of probability distributions. {W}e first describe histogram based methods of producing object probability distributions. {I}n our case, we want to track the mode of an object's probability distribution within a video scene. {S}ince the probability distribution of the object can change and move dynamically in time, the mean shift algorithm is modified to deal with dynamically changing probability distributions. {T}he modified algorithm is called the {C}ontinuously {A}daptive {M}ean {S}hift ({CAMSHIFT}) algorithm. {CAMSHIFT} is then used as an interface for games and graphics},
  Doi                      = {10.1109/ACV.1998.732882},
  File                     = {:sources/00732882.pdf:PDF},
  Keywords                 = {Computer vision, Face detection, Games, Histograms, Humans, Layout, Probability distribution, Robustness, Time sharing computer systems, User interfaces},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.11}
}

@InProceedings{Buttazzo1996,
  Title                    = {{R}eal-time issues in advanced robotics applications},
  Author                   = {Buttazzo, G.},
  Booktitle                = {Proceedings of the Eighth Euromicro Workshop on Real-Time Systems},
  Year                     = {1996},

  Address                  = {Italy, L'Aquila},
  Pages                    = {133 - 138},
  Publisher                = {IEEE},

  Abstract                 = {{T}he aim of this paper is to discuss some important real-time issues involved in the design of complex robotic applications. {I}n particular the problem of evaluating why and when a robotics application needs real-time computing is discussed first. {T}he second issue concerns the definition of time constraints for each task of the robot. {W}e show how time constraints, such as periods and deadlines, can be derived from the application, even though they are not explicitly specified in the requirements. {A}s a third issue, we describe a general hierarchical control architecture, which can be built on top of a real-time system, to greatly simplify the development of complex robotics applications having real-time requirements},
  Doi                      = {10.1109/EMWRTS.1996.557843},
  File                     = {:sources/00557843.pdf:PDF},
  Keywords                 = {Actuators, Computer applications, Mechanical sensors, Real time systems, Robot sensing systems, Robot vision systems, Sensor phenomena and characterization, Sensor systems, Thermal conductivity, Time factors},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.10.30}
}

@Article{Campoy2009,
  Title                    = {{C}omputer {V}ision {O}nboard {UAV}s for {C}ivilian {T}asks},
  Author                   = {Campoy, Pascual and Correa, Juan F. and Mondragón, Ivan and Martínez, Carol and Olivares, Miguel and Mejías, Luis and Artieda, Jorge},
  Journal                  = {Journal of Intelligent and Robotic Systems},
  Year                     = {2009},

  Month                    = {March},
  Note                     = {Publisher: Springer Netherlands},
  Pages                    = {105-135},
  Volume                   = {54},

  Abstract                 = {{C}omputer vision is much more than a technique to sense and recover environmental information from an {UAV}. {I}t should play a main role regarding {UAV}s’ functionality because of the big amount of information that can be extracted, its possible uses and applications, and its natural connection to human driven tasks, taking into account that vision is our main interface to world understanding. {O}ur current research’s focus lays on the development of techniques that allow {UAV}s to maneuver in spaces using visual information as their main input source. {T}his task involves the creation of techniques that allow an {UAV} to maneuver towards features of interest whenever a {GPS} signal is not reliable or sufficient, e.g. when signal dropouts occur (which usually happens in urban areas, when flying through terrestrial urban canyons or when operating on remote planetary bodies), or when tracking or inspecting visual targets—including moving ones—without knowing their exact {UMT} coordinates. {T}his paper also investigates visual servoing control techniques that use velocity and position of suitable image features to compute the references for flight control. {T}his paper aims to give a global view of the main aspects related to the research field of computer vision for {UAV}s, clustered in four main active research lines: visual servoing and control, stereo-based visual navigation, image processing algorithms for detection and tracking, and visual {SLAM}. {F}inally, the results of applying these techniques in several applications are presented and discussed: this study will encompass power line inspection, mobile target tracking, stereo distance estimation, mapping and positioning.},
  Doi                      = {10.1007/s10846-008-9256-z},
  File                     = {:sources/art%3A10.1007%2Fs10846-008-9256-z.pdf:PDF},
  Keywords                 = {UAV, Visual servoing, Image processing, Feature detection, Tracking, SLAM},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Book{Challa2011,
  Title                    = {{F}undamentals of {O}bject {T}racking},
  Author                   = {Challa, Subhash and Morelande, Mark R. and Mušicki, Darko and Evans, Robin J.},
  Publisher                = {Cambridge University Press},
  Year                     = {2011},

  Doi                      = {http://dx.doi.org/10.1017/CBO9780511975837},
  File                     = {:sources/0521876281_tracking.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.02.25}
}

@Article{Chaumette2007,
  Title                    = {{V}isual servo control. {II}. {A}dvanced approaches},
  Author                   = {Chaumette, Francois and Hutchinson, Seth},
  Journal                  = {IEEE Robotics \& Automation Magazine},
  Year                     = {2007},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {109 - 118},
  Volume                   = {14},

  Abstract                 = {{T}his article is the second of a two-part tutorial on visual servo control. {I}n this tutorial, we have only considered velocity controllers. {I}t is convenient for most of classical robot arms. {H}owever, the dynamics of the robot must of course be taken into account for high speed task, or when we deal with mobile nonholonomic or underactuated robots. {A}s for the sensor, geometrical features coming from a classical perspective camera is considered. {F}eatures related to the image motion or coming from other vision sensors necessitate to revisit the modeling issues to select adequate visual features. {F}inally, fusing visual features with data coming from other sensors at the level of the control scheme will allow to address new research topics},
  Doi                      = {10.1109/MRA.2007.339609},
  File                     = {:sources/04141039.pdf:PDF},
  Keywords                 = {Image sensors, Manipulators, Mobile robots, Robot sensing systems, Robot vision systems, Sensor phenomena and characterization, Servosystems, Tutorials, Velocity control},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.02}
}

@Article{Chaumette2006,
  Title                    = {{V}isual servo control. {I}. {B}asic approaches},
  Author                   = {Chaumette, Francois and Hutchinson, Seth},
  Journal                  = {IEEE Robotics \& Automation Magazine},
  Year                     = {2006},

  Month                    = {December},
  Number                   = {4},
  Pages                    = {82 - 90},
  Volume                   = {13},

  Abstract                 = {{T}his paper is the first of a two-part series on the topic of visual servo control using computer vision data in the servo loop to control the motion of a robot. {I}n this paper, we describe the basic techniques that are by now well established in the field. {W}e first give a general overview of the formulation of the visual servo control problem. {W}e then describe the two archetypal visual servo control schemes: image-based and position-based visual servo control. {F}inally, we discuss performance and stability issues that pertain to these two schemes, motivating the second article in the series, in which we consider advanced techniques},
  Doi                      = {10.1109/MRA.2006.250573},
  File                     = {:sources/04015997.pdf:PDF},
  Keywords                 = {Automatic control, Cameras, Computer vision, Mobile robots, Motion control, Robot motion, Robot vision systems, Robotics and automation, Servomechanisms, Servosystems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.02}
}

@InProceedings{Chen2009,
  Title                    = {{A} {S}urvey of {A}utonomous {C}ontrol for {UAV}},
  Author                   = {Chen, Hai and Wang, Xin-Min and Li, Yan},
  Booktitle                = {International Conference on Artificial Intelligence and Computational Intelligence},
  Year                     = {2009},

  Address                  = {China, Shanghai},
  Month                    = {November},
  Pages                    = {267 - 271},
  Publisher                = {IEEE},
  Volume                   = {2},

  Abstract                 = {{T}his paper presents the autonomous control for {UAV}. {T}he autonomous control concept and autonomous control level ({ACL}) metrics that can measure autonomy of {UAV}s are introduced. {C}ompared with manned aircraft control task in battlefield, the functions of autonomous {UAV} system are organized. {A}ccording to the laws of increasing precision with decreasing intelligent ({IPDI}), the architecture of autonomous control for {UAV} is given. {T}he architecture can be divided into three levels: execution level, coordination level and organization level. {T}he constraint conditions and realizations of coordination level and organization level are studied comprehensively. {T}he key hardware and software technologies for multi-tasks are modularized depending on the requirements of mission. {T}he software technologies are distributed to stages of flying particularly.},
  Doi                      = {10.1109/AICI.2009.147},
  File                     = {:sources/05375937.pdf:PDF},
  Keywords                 = {Artificial intelligence, Automatic control, Automation, Communication system control, Computer architecture, Control systems, Educational institutions, Force control, Telecommunication control, Unmanned aerial vehicles},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Chen2012,
  Title                    = {{A}daptive visual servo control of {UAV} {G}round-{T}arget-{A}utonomous-{T}racking {S}ystem},
  Author                   = {Chen, Longsheng and Jiang, Yang and Wang, Changkug},
  Booktitle                = {10th World Congress on Intelligent Control and Automation},
  Year                     = {2012},

  Address                  = {China, Beijing},
  Month                    = {July},
  Pages                    = {133 - 137},
  Publisher                = {IEEE},

  Abstract                 = {{A} novel adaptive servo control method is proposed for {UAV} {GTATS}({G}round-{T}arget-{A}utonomous-{T}racking {S}ystem),which consists of basic control law for {UAV} and visual tracking controller for {GTATS}. {T}he adaptive servo control method only depends on target information in the image plane and {K}alman filtering technology. {B}ased on this proposed method, a dynamic motion target can be tracked without target's 3{D} velocity. {S}ynchronously, in order to estimate the optimal system state and target image velocity which is used later by the visual tracking controller, a self-tuning {K}alman filter is adopted to estimate interesting parameters on-line in real-time. {F}urther, {B}ecause the visual tracking controller is working entirely in image space, the dynamic characteristics of the image signal are analyzed and a kinematics model is developed for the target in the image plane by the geometrical relations among the {UAV}, the target and the camera. {F}inally, {T}he performance of the controller is demonstrated by theoretical stability analysis.},
  Doi                      = {10.1109/WCICA.2012.6357854},
  File                     = {:sources/06357854.pdf:PDF},
  Keywords                 = {Kalman filters, Adaptive control, Autonomous aerial vehicles, Geometry, Parameter estimation, Self-adjusting systems, Stability, State estimation, Target tracking, Vehicle dynamics, Velocity control, Visual servoing},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Article{Chen2012a,
  Title                    = {{K}alman {F}ilter for {R}obot {V}ision: {A} {S}urvey},
  Author                   = {Shih-Yuan Chen},
  Journal                  = {IEEE Transactions on Industrial Electronics},
  Year                     = {2012},

  Month                    = {August},
  Number                   = {11},
  Pages                    = {4409 - 4420},
  Volume                   = {59},

  Abstract                 = {{K}alman filters have received much attention with the increasing demands for robotic automation. {T}his paper briefly surveys the recent developments for robot vision. {A}mong many factors that affect the performance of a robotic system, {K}alman filters have made great contributions to vision perception. {K}alman filters solve uncertainties in robot localization, navigation, following, tracking, motion control, estimation and prediction, visual servoing and manipulation, and structure reconstruction from a sequence of images. {I}n the 50th anniversary, we have noticed that more than 20 kinds of {K}alman filters have been developed so far. {T}hese include extended {K}alman filters and unscented {K}alman filters. {I}n the last 30 years, about 800 publications have reported the capability of these filters in solving robot vision problems. {S}uch problems encompass a rather wide application area, such as object modeling, robot control, target tracking, surveillance, search, recognition, and assembly, as well as robotic manipulation, localization, mapping, navigation, and exploration. {T}hese reports are summarized in this review to enable easy referral to suitable methods for practical solutions. {R}epresentative contributions and future research trends are also addressed in an abstract level.},
  Doi                      = {10.1109/TIE.2011.2162714},
  File                     = {:sources/05985520.pdf:PDF},
  Keywords                 = {Computer vision, Kalman filters, Robot kinematics, Robot vision, Tracking, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.15}
}

@InProceedings{Cheng2009,
  Title                    = {{I}mproved visual tracking using the technique of image pyramid},
  Author                   = {Cheng, Chi-Cheng and Ho, Chung-Hsing},
  Booktitle                = {IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  Year                     = {2009},

  Address                  = {China, Guilin},
  Month                    = {December},
  Pages                    = {659 - 664},
  Publisher                = {IEEE},

  Abstract                 = {{T}he advantage of the optical-flow-based visual servo methods is that features of the moving object do not need to be known in advance. {T}herefore, they can fit for demands of versatile positioning and tracking tasks in the real world. {N}evertheless, it is difficult to achieve satisfactory tracking performance for fast moving objects using these standard approaches. {T}he purpose of this paper is to implement a robust image servo scheme that incorporates a sliding-mode control algorithm and the image pyramid technique to track an unknown image pattern in three-dimensional motion. {T}he sliding mode controller deals with possible system uncertainties. {T}he image pyramid builds a group of pictures with different resolutions arranged in layers of a hierarchical structure for the same image. {L}ow resolution images are applied for following a fast moving object to maintain acceptable tracking performance. {S}witching between layers is performed as the detected motion crosses over a pre-defined maximum critical speed. {I}t is verified that the tracking performance can be greatly improved using the proposed control framework.},
  Doi                      = {10.1109/ROBIO.2009.5420596},
  File                     = {:sources/05420596.pdf:PDF},
  Keywords                 = {Biomedical optical imaging, Brightness, Control systems, Data mining, Image motion analysis, Image resolution, Optical sensors, Servomechanisms, Servosystems, Sliding mode control},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.27}
}

@Article{Cheng1995,
  Title                    = {{M}ean shift, mode seeking, and clustering},
  Author                   = {Yizong Cheng},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {1995},

  Month                    = {August},
  Number                   = {8},
  Pages                    = {790 - 799},
  Volume                   = {17},

  Abstract                 = {{M}ean shift, a simple interactive procedure that shifts each data point to the average of data points in its neighborhood is generalized and analyzed in the paper. {T}his generalization makes some k-means like clustering algorithms its special cases. {I}t is shown that mean shift is a mode-seeking process on the surface constructed with a “shadow” kernal. {F}or {G}aussian kernels, mean shift is a gradient mapping. {C}onvergence is studied for mean shift iterations. {C}luster analysis if treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. {A}pplications in clustering and {H}ough transform are demonstrated. {M}ean shift is also considered as an evolutionary strategy that performs multistart global optimization},
  Doi                      = {10.1109/34.400568},
  File                     = {:sources/00400568.pdf:PDF},
  Keywords                 = {Algorithm design and analysis, Clustering algorithms, Computer science, Convergence, Iterative algorithms, Kernel, Surface treatment},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.10}
}

@InProceedings{Cheung2004,
  Title                    = {{R}obust techniques for background subtraction in urban traffic video},
  Author                   = {Cheung, Sen-ching S. and Kamath, Chandrika},
  Booktitle                = {Proceedings of the SPIE},
  Year                     = {2004},
  Pages                    = {881-892},
  Volume                   = {5308},

  Abstract                 = {{I}dentifying moving objects from a video sequence is a fundamental and critical task in many computer-vision applications. {A} common approach is to perform background subtraction, which identifies moving objects from the portion of a video frame that differs significantly from a background model. {T}here are many challenges in developing a good background subtraction algorithm. {F}irst, it must be robust against changes in illumination. {S}econd, it should avoid detecting non-stationary background objects such as swinging leaves, rain, snow, and shadow cast by moving objects. {F}inally, its internal background model should react quickly to changes in background such as starting and stopping of vehicles. {I}n this paper, we compare various background subtraction algorithms for detecting moving vehicles and pedestrians in urban traffic video sequences. {W}e consider approaches varying from simple techniques such as frame differencing and adaptive median filtering, to more sophisticated probabilistic modeling techniques. {W}hile complicated techniques often produce superior performance, our experiments show that simple techniques such as adaptive median filtering can produce good results with much lower computational complexity.},
  Doi                      = {10.1117/12.526886},
  File                     = {:sources/10.1.1.214.6703.pdf:PDF},
  Keywords                 = {Background subtraction, urban traffic video},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Chou2012,
  Title                    = {{M}ono vision particle filter based object tracking with a mobile robot},
  Author                   = {Yu-Cheng Chou and Bo-Shiun Huang},
  Booktitle                = {IEEE/ASME International Conference on Mechatronics and Embedded Systems and Applications (MESA)},
  Year                     = {2012},

  Address                  = {China, Suzhou},
  Month                    = {July},
  Pages                    = {87 - 92},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper proposes a new mono vision, particle filter based object tracking method that predicts a moving object's position within an image, and computes the corresponding real-world position relative to a mobile robot. {A}ccordingly, the mobile robot moves toward the moving object to keep it at the central field of view. {E}xperimental results show that the proposed method allows our mobile robot to track a target moving at a moderate speed.},
  Doi                      = {10.1109/MESA.2012.6275542},
  File                     = {:sources/06275542.pdf:PDF},
  Keywords                 = {Cameras, Equations, MONOS devices, Mathematical model, Mobile robots, Particle filters},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.11}
}

@Article{Comaniciu2002,
  Title                    = {{M}ean shift: a robust approach toward feature space analysis},
  Author                   = {Dorin Comaniciu and Peter Meer},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2002},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {603 - 619},
  Volume                   = {24},

  Abstract                 = {{A} general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. {T}he basic computational module of the technique is an old pattern recognition procedure: the mean shift. {F}or discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. {T}he relation of the mean shift procedure to the {N}adaraya-{W}atson estimator from kernel regression and the robust {M}-estimators; of location is also established. {A}lgorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. {I}n these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. {E}xtensive experimental results illustrate their excellent performance},
  Doi                      = {10.1109/34.1000236},
  File                     = {:sources/01000236.pdf:PDF},
  Keywords                 = {Density functional theory, Image analysis, Image color analysis, Image resolution, Image segmentation, Kernel, Pattern recognition, Robustness, Smoothing methods},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.10}
}

@InProceedings{Comaniciu1999,
  Title                    = {{M}ean shift analysis and applications},
  Author                   = {Dorin Comaniciu and Peter Meer},
  Booktitle                = {The Proceedings of the Seventh IEEE International Conference on Computer Vision},
  Year                     = {1999},

  Address                  = {Greece, Kerkyra},
  Month                    = {September},
  Pages                    = {1197 - 1203},
  Publisher                = {IEEE},
  Volume                   = {2},

  Abstract                 = {{A} nonparametric estimator of density gradient, the mean shift, is employed in the joint, spatial-range (value) domain of gray level and color images for discontinuity preserving filtering and image segmentation. {P}roperties of the mean shift are reviewed and its convergence on lattices is proven. {T}he proposed filtering method associates with each pixel in the image the closest local mode in the density distribution of the joint domain. {S}egmentation into a piecewise constant structure requires only one more step, fusion of the regions associated with nearby modes. {T}he proposed technique has two parameters controlling the resolution in the spatial and range domains. {S}ince convergence is guaranteed, the technique does not require the intervention of the user to stop the filtering at the desired image quality. {S}everal examples, for gray and color images, show the versatility of the method and compare favorably with results described in the literature for the same images},
  Doi                      = {10.1109/ICCV.1999.790416},
  File                     = {:sources/00790416.pdf:PDF},
  Keywords                 = {Application software, Color, Computer vision, Convergence, Image quality, Image segmentation, Kernel, Lattices, Pixel, Spatial resolution},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.09}
}

@Article{Comaniciu2003,
  Title                    = {{K}ernel-based object tracking},
  Author                   = {Dorin Comaniciu and Visvanathan Ramesh and Peter Meer},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2003},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {564 - 577},
  Volume                   = {25},

  Abstract                 = {{A} new approach toward target representation and localization, the central component in visual tracking of nonrigid objects, is proposed. {T}he feature histogram-based target representations are regularized by spatial masking with an isotropic kernel. {T}he masking induces spatially-smooth similarity functions suitable for gradient-based optimization, hence, the target localization problem can be formulated using the basin of attraction of the local maxima. {W}e employ a metric derived from the {B}hattacharyya coefficient as similarity measure, and use the mean shift procedure to perform the optimization. {I}n the presented tracking examples, the new method successfully coped with camera motion, partial occlusions, clutter, and target scale variations. {I}ntegration with motion filters and data association techniques is also discussed. {W}e describe only a few of the potential applications: exploitation of background information, {K}alman tracking using motion models, and face tracking.},
  Doi                      = {10.1109/TPAMI.2003.1195991},
  File                     = {:sources/01195991.pdf:PDF},
  Keywords                 = {Cameras, Face detection, Filtering, Filters, Kernel, Layout, Nonlinear equations, Performance evaluation, State-space methods, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.10}
}

@InProceedings{Comaniciu2000,
  Title                    = {{R}eal-time tracking of non-rigid objects using mean shift},
  Author                   = {Dorin Comaniciu and Visvanathan Ramesh and Peter Meer},
  Booktitle                = {Proceedings. IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2000},

  Address                  = {USA, SC, Hilton Head Island},
  Month                    = {June},
  Pages                    = {142 - 149},
  Volume                   = {2},

  Abstract                 = {{A} new method for real time tracking of non-rigid objects seen from a moving camera is proposed. {T}he central computational module is based on the mean shift iterations and finds the most probable target position in the current frame. {T}he dissimilarity between the target model (its color distribution) and the target candidates is expressed by a metric derived from the {B}hattacharyya coefficient. {T}he theoretical analysis of the approach shows that it relates to the {B}ayesian framework while providing a practical, fast and efficient solution. {T}he capability of the tracker to handle in real time partial occlusions, significant clutter, and target scale variations, is demonstrated for several image sequences},
  Doi                      = {10.1109/CVPR.2000.854761},
  File                     = {:sources/00854761.pdf:PDF},
  Keywords                 = {Bayesian methods, Computational complexity, Educational institutions, Image sequences, Kernel, Monitoring, Surveillance, Target tracking, Visualization, Yield estimation},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.09}
}

@InProceedings{Deng2010,
  Title                    = {{A} motion controller for a pan-tilt camera on an autonomous helicopter},
  Author                   = {Deng, Haibo and Zhao, Xiaoguang and Hou, Zengguang},
  Booktitle                = {11th International Conference on Control Automation Robotics \& Vision (ICARCV)},
  Year                     = {2010},

  Address                  = {China, Singapore},
  Month                    = {December},
  Pages                    = {585 - 590},
  Publisher                = {IEEE},

  Abstract                 = {{I}n this paper, a motion controller for a pan-tilt camera mounted on an autonomous helicopter is presented. {T}he motion planner is designed according to the kinematics of the pan-tilt camera. {H}owever, the solution of the inverse kinematics of the pan-tilt camera is not unique. {T}o deal with this situation, a decision maker is designed. {T}he decision maker makes use of a cost function to decide which solution should be chose. {A}nd the error signal is defined as the difference between pan-tilt joint angles and the desired joint angles. {T}he dynamics of the error system is derived, and proportional controllers are designed according to the error dynamics to control the pan and tilt joints respectively. {T}his system is validated in a virtual reality environment, and the simulation results show that this system can track the target successfully.},
  Doi                      = {10.1109/ICARCV.2010.5707378},
  File                     = {:sources/05707378.pdf:PDF},
  Keywords                 = {Angular velocity, Cameras, Helicopters, Joints, Target tracking, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.06.13}
}

@InProceedings{Deng2010a,
  Title                    = {{T}racking ground targets using an autonomous helicopter with a vision system},
  Author                   = {Deng, Haibo and Zhao, Xiaoguang and Hou, Zengguang},
  Booktitle                = {3rd International Congress on Image and Signal Processing},
  Year                     = {2010},

  Address                  = {China, Yantai},
  Month                    = {October},
  Pages                    = {1704 - 1708},
  Publisher                = {IEEE},
  Volume                   = {4},

  Abstract                 = {{A}n autonomous helicopter with a vision system onboard to track ground targets can accomplish very important tasks like inspection and surveillance. {T}he autonomous helicopter target tracking system is a interdisciplinary complex system. {A} typical autonomous helicopter target tracking system using vision sensors usually includes five major component: a navigation system, a flight controller, a active vision system, a target state estimator and a helicopter guidance law. {I}n this paper, we focus on target state estimator and helicopter guidance law. {I}t is assumed that the target moves on a flat ground, so the relative position between the target and the helicopter can be measured by a monocular vision system. {K}alman filter is used to estimate the state of the target based on the vision measurements. {A} new guidance law inspired by velocity pursuit guidance and car-following control is proposed to guide the helicopter to follow the moving target. {T}he tracking system is tested in a virtual reality environment and the results shows that the performance of the system is satisfied.},
  Doi                      = {10.1109/CISP.2010.5647759},
  File                     = {:sources/05647759.pdf:PDF},
  Keywords                 = {Aircraft navigation, Estimation, Helicopters, Machine vision, Pixel, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Dinh2009,
  Title                    = {{R}eal time tracking using an active pan-tilt-zoom network camera},
  Author                   = {Dinh, T. and Qian Yu and Medioni, G.},
  Booktitle                = {IEEE/RSJ International Conference on Robots and Systems},
  Year                     = {2009},

  Address                  = {USA, MO, St. Louis},
  Month                    = {October},
  Pages                    = {3786 - 3793},
  Publisher                = {IEEE},

  Abstract                 = {{W}e present here a real time active vision system on a {PTZ} network camera to track an object of interest. {W}e address two critical issues in this paper. {O}ne is the control of the camera through network communication to follow a selected object. {T}he other is to track an arbitrary type of object in real time under conditions of pose, viewpoint and illumination changes. {W}e analyze the difficulties in the control through the network and propose a practical solution for tracking using a {PTZ} network camera. {M}oreover, we propose a robust real time tracking approach, which enhances the effectiveness by using complementary features under a two-stage particle filtering framework and a multi-scale mechanism. {T}o improve time performance, the tracking algorithm is implemented as a multi-threaded process in {O}pen{MP}. {C}omparative experiments with state-of-the-art methods demonstrate the efficiency and robustness of our system in various applications such as pedestrian tracking, face tracking, and vehicle tracking.},
  Doi                      = {10.1109/IROS.2009.5353915},
  File                     = {:sources/05353915.pdf:PDF},
  Keywords                 = {Automatic control, Cameras, Communication system control, Humans, Intelligent robots, Lighting, Real time systems, Robot vision systems, Robustness, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Book{Dorf2011,
  Title                    = {{M}odern {C}ontrol {S}ystems},
  Author                   = {Dorf, Richard C. and Bishop, Robert H.},
  Publisher                = {Prentice Hall},
  Year                     = {2011},
  Edition                  = {12},

  File                     = {:sources/Modern Control Systems.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.02.25}
}

@Article{Doyle2014,
  Title                    = {{O}ptical flow background estimation for real-time pan/tilt camera object tracking},
  Author                   = {Daniel D. Doyle and Alan L. Jennings and Jonathan T. Black},
  Journal                  = {Measurement},
  Year                     = {2014},
  Pages                    = {195 - 207},
  Volume                   = {48},

  Abstract                 = {{A}s {C}omputer {V}ision ({CV}) techniques develop, pan/tilt camera systems are able to enhance data capture capabilities over static camera systems. {I}n order for these systems to be effective for metrology purposes, they will need to respond to the test article in real-time with a minimum of additional uncertainty. {A} methodology is presented here for obtaining high-resolution, high frame-rate images, of objects traveling at speeds >= 1.2 m/s at 1 m from the camera by tracking the moving texture of an object. {S}trong corners are determined and used as flow points using implementations on a graphic processing unit ({GPU}), resulting in significant speed-up over central processing units ({CPU}). {B}ased on directed pan/tilt motion, a pixel-to-pixel relationship is used to estimate whether optical flow points fit background motion, dynamic motion or noise. {T}o smooth variation, a two-dimensional position and velocity vector is used with a {K}alman filter to predict the next required position of the camera so the object stays centered in the image. {H}igh resolution images can be stored by a parallel process resulting in a high frame rate procession of images for post-processing. {T}he results provide real-time tracking on a portable system using a pan/tilt unit for generic moving targets where no training is required and camera motion is observed from high accuracy encoders opposed to image correlation.},
  Doi                      = {10.1016/j.measurement.2013.10.025},
  File                     = {:sources/1-s2.0-S0263224113005241-main.pdf:PDF},
  ISSN                     = {0263-2241},
  Keywords                 = {Pan/tilt camera, Optical flow, Background subtraction, Object tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0263224113005241}
}

@Article{Fernandez-Caballero2010,
  Title                    = {{O}ptical flow or image subtraction in human detection from infrared camera on mobile robot},
  Author                   = {Fernández-Caballero, Antonio and Castillo, José Carlos and Martínez-Cantos, Javier and Martínez-Tomás, Rafael},
  Journal                  = {Robotics and Autonomous Systems},
  Year                     = {2010},
  Number                   = {12},
  Pages                    = {1273 - 1281},
  Volume                   = {58},

  Abstract                 = {{P}erceiving the environment is crucial in any application related to mobile robotics research. {I}n this paper, a new approach to real-time human detection through processing video captured by a thermal infrared camera mounted on the autonomous mobile platform m{S}ecurit {T} {M} is introduced. {T}he approach starts with a phase of static analysis for the detection of human candidates through some classical image processing techniques such as image normalization and thresholding. {T}hen, the proposal starts a dynamic image analysis phase based in optical flow or image difference. {O}ptical flow is used when the robot is moving, whilst image difference is the preferred method when the mobile platform is still. {T}he results of both phases are compared to enhance the human segmentation by infrared camera. {I}ndeed, optical flow or image difference will emphasize the foreground hot spot areas obtained at the initial human candidates’ detection.},
  Doi                      = {10.1016/j.robot.2010.06.002},
  File                     = {:sources/1-s2.0-S0921889010001168-main.pdf:PDF},
  ISSN                     = {0921-8890},
  Keywords                 = {Surveillance system, Mobile robot, Infrared camera, Optical flow, Image subtraction},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0921889010001168}
}

@Book{Forsyth2012,
  Title                    = {{C}omputer {V}ision - {A} modern approach},
  Author                   = {David A. Forsyth and Jean Ponce},
  Publisher                = {Prentice Hall},
  Year                     = {2012},
  Edition                  = {2},

  File                     = {:sources/Computer Vision A Modern Approach 2nd Edition.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.15}
}

@Article{Fukunaga1975,
  Title                    = {{T}he estimation of the gradient of a density function, with applications in pattern recognition},
  Author                   = {Keinosuke Fukunaga and Larry D. Hostetler},
  Journal                  = {IEEE Transactions on Information Theory},
  Year                     = {1975},

  Month                    = {January},
  Number                   = {1},
  Pages                    = {32 - 40},
  Volume                   = {21},

  Abstract                 = {{N}onparametric density gradient estimation using a generalized kernel approach is investigated. {C}onditions on the kernel functions are derived to guarantee asymptotic unbiasedness, consistency, and uniform consistency of the estimates. {T}he results are generalized to obtain a simple mean-shift estimate that can be extended in a k-nearest-neighbor approach. {A}pplications of gradient estimation to pattern recognition are presented using clustering and intrinsic dimensionality problems, with the ultimate goal of providing further understanding of these problems in terms of density gradients.},
  Doi                      = {10.1109/TIT.1975.1055330},
  File                     = {:sources/01055330.pdf:PDF},
  Keywords                 = {Clustering algorithms, Density functional theory, Filtering, Kernel, Laboratories, Pattern recognition, Probability density function},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.10}
}

@InProceedings{Galego2011,
  Title                    = {{V}ignetting {C}orrection {F}or {P}an-{T}ilt {S}urveillance {C}ameras},
  Author                   = {Galego, Ricardo and Bernardino, Alexandre and Gaspar, José},
  Booktitle                = {6th International Conference on Computer Vision Theory and Applications},
  Year                     = {2011},

  Address                  = {Algarve, Portugal},
  Month                    = {March},

  Abstract                 = {{I}t is a well know result that the geometry of pan and tilt (perspective) cameras auto-calibrate using just the image information. {H}owever, applications based on panoramic background representations must also com- pensate for radiometric effects due to camera motion. {I}n this paper we propose a methodology for calibrating the radiometric effects inherent in the operation of pan-tilt cameras, with applications to visual surveillance in a cube (mosaicked) visual field representation. {T}he radiometric calibration is based on the estimation of vignetting image distortion using the pan and tilt degrees of freedom instead of color calibrating patterns. {E}xperiments with real images show that radiometric calibration reduce the variance in the background repre- sentation allowing for more effective event detection in background-subtraction-based algorithms.},
  File                     = {:sources/11-VISAPP-RGalego.pdf:PDF},
  Keywords                 = {Image formation, Vignetting correction, Pan-Tilt cameras, Visual event detection, Surveillance},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.09.09}
}

@Article{Gaspar2011,
  Title                    = {{S}ingle {P}an and {T}ilt {C}amera {I}ndoor {P}ositioning and {T}racking {S}ystem},
  Author                   = {Gaspar, Tiago and Oliveira, Paulo},
  Journal                  = {European Journal of Control},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {414 - 428},
  Volume                   = {17},

  Abstract                 = {{A}n inexpensive single pan and tilt camera based indoor positioning and tracking system is proposed, supported on a functional architecture where three main modules can be identified: one related to the interface with the camera, tackled with parameter estimation techniques; other, responsible for isolating and identifying the target, based on advanced image processing techniques, and a third, that resorting to nonlinear dynamic system suboptimal state estimation techniques, performs the tracking of the target and estimates its position, and linear and angular velocities. {T}he contributions of this work are fourfold: i) a new indoor positioning and tracking system architecture; ii) a new lens distortion calibration method, that preserves generic straight lines in images; iii) suboptimal nonlinear multiple-model adaptive estimation techniques, for the adopted target model, to tackle the positioning and trackingtasks, andiv) theimplementationandvalidationin real time of a complex tracking system, based on a low cost single camera. {T}o assess the performance of the proposed system, a series of indoor experimental tests for a range of operation of up to ten meter were carried out. {A}centimetric accuracy was obtained under realistic conditions. },
  Doi                      = {10.3166/ejc.17.414-428},
  File                     = {:sources/1-s2.0-S0947358011706082-main.pdf:PDF},
  ISSN                     = {0947-3580},
  Keywords                 = {Indoor positioning and tracking, nonlinear filtering, multiple-model adaptive estimation, single camera vision systems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0947358011706082}
}

@Article{Germa2010,
  Title                    = {{V}ision and {RFID} data fusion for tracking people in crowds by a mobile robot},
  Author                   = {Germa, T. and Lerasle, F. and Ouadah, N. and Cadenat, V.},
  Journal                  = {Computer Vision and Image Understanding},
  Year                     = {2010},
  Number                   = {6},
  Pages                    = {641–651},
  Volume                   = {114},

  Abstract                 = {{I}n this paper, we address the problem of realizing a human following task in a crowded environment. {W}e consider an active perception system, consisting of a camera mounted on a pan-tilt unit and a 360°360° {RFID} detection system, both embedded on a mobile robot. {T}o perform such a task, it is necessary to efficiently track humans in crowds. {I}n a first step, we have dealt with this problem using the particle filtering framework because it enables the fusion of heterogeneous data, which improves the tracking robustness. {I}n a second step, we have considered the problem of controlling the robot motion to make the robot follow the person of interest. {T}o this aim, we have designed a multi-sensor-based control strategy based on the tracker outputs and on the {RFID} data. {F}inally, we have implemented the tracker and the control strategy on our robot. {T}he obtained experimental results highlight the relevance of the developed perceptual functions. {P}ossible extensions of this work are discussed at the end of the article.},
  Doi                      = {doi:10.1016/j.cviu.2010.01.008},
  File                     = {:sources/1-s2.0-S1077314210000317-main.pdf:PDF},
  Keywords                 = {Radio frequency ID, Multimodal data fusion, Particle filtering, Person tracking, Person following, Multi-sensor fusion, Human visual servoing},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Book{Golnaraghi2010,
  Title                    = {{A}utomatic {C}ontrol {S}ystems},
  Author                   = {Golnaraghi, Farid and Kuo, Benjamin C.},
  Publisher                = {John Wiley and Sons},
  Year                     = {2010},
  Edition                  = {9},

  File                     = {:sources/Automatic Control Systems, 9th Edition by Farid Golnaraghi, Benjamin C. Kuo.www.eeeuniversity.com.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.02.25}
}

@Book{Gonzalez,
  Title                    = {{D}igital {I}mage {P}rocessing},
  Author                   = {Rafael C. Gonzalez and Richard E. Woods},
  Publisher                = {Pearson Education, Inc.},
  Edition                  = {3},

  File                     = {:sources/Digital Image Processing_3ed_Gonzalez.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.04.07}
}

@InProceedings{Guha2005,
  Title                    = {{D}yna{T}racker: {T}arget tracking in active video surveillance systems},
  Author                   = {Guha, P. and Palai, D. and Goswami, D. and Mukerjee, A.},
  Booktitle                = {12th International Conference on Advanced Robotics (ICAR)},
  Year                     = {2005},

  Address                  = {USA, WA, Seattle},
  Month                    = {July},
  Pages                    = {621 - 627},
  Publisher                = {IEEE},

  Abstract                 = {{A}ctive video surveillance systems provide challenging research issues in the interface of computer vision, pattern recognition and control system analysis. {A} significant part of such systems is devoted toward active camera control for efficient target tracking. {D}yna{T}racker is a pan-tilt device based active camera system for maintaining continuous track of the moving target, while keeping the same at a pre-specified region (typically, the center) of the image. {T}he significant contributions in this work are the use of mean-shift algorithm for visual tracking and the derivation of the error dynamics for a proportional-integral control action. {T}he stability analysis and optimal controller gain selections are performed from the simulation studies of the derived error dynamics. {S}imulation predictions are also validated from the results of practical experimentations. {T}he present implementation of {D}yna{T}racker performs on a standard {P}entium {IV} {PC} at an average speed of 10 frames per second while operating on color images of 320x240 resolution},
  Doi                      = {10.1109/ICAR.2005.1507473},
  File                     = {:sources/01507473.pdf:PDF},
  Keywords                 = {Cameras, Computer vision, Control system analysis, Control systems, Error correction, Pattern recognition, PI control, Stability analysis, Target tracking, Video surveillance},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.09.09}
}

@Article{Gustafsson2010,
  Title                    = {{P}article filter theory and practice with positioning applications},
  Author                   = {Fredrik Gustafsson},
  Journal                  = {IEEE Aerospace and Electronic Systems Magazine},
  Year                     = {2010},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {53 - 82},
  Volume                   = {25},

  Abstract                 = {{T}he particle filter ({PF}) was introduced in 1993 as a numerical approximation to the nonlinear {B}ayesian filtering problem, and there is today a rather mature theory as well as a number of successful applications described in literature. {T}his tutorial serves two purposes: to survey the part of the theory that is most important for applications and to survey a number of illustrative positioning applications from which conclusions relevant for the theory can be drawn. {T}he theory part first surveys the nonlinear filtering problem and then describes the general {PF} algorithm in relation to classical solutions based on the extended {K}alman filter ({EKF}) and the point mass filter ({PMF}). {T}uning options, design alternatives, and user guidelines are described, and potential computational bottlenecks are identified and remedies suggested. {F}inally, the marginalized (or {R}ao-{B}lackwellized) {PF} is overviewed as a general framework for applying the {PF} to complex systems. {T}he application part is more or less a stand-alone tutorial without equations that does not require any background knowledge in statistics or nonlinear filtering. {I}t describes a number of related positioning applications where geographical information systems provide a nonlinear measurement and where it should be obvious that classical approaches based on {K}alman filters ({KF}s) would have poor performance. {A}ll applications are based on real data and several of them come from real-time implementations. {T}his part also provides complete code examples.},
  Doi                      = {10.1109/MAES.2010.5546308},
  File                     = {:sources/05546308.pdf:PDF},
  Keywords                 = {Bayesian methods, Filtering algorithms, Filtering theory, Guidelines, Information systems, Nonlinear equations, Numerical models, Particle filters, Position measurement, Solid modeling, Statistics, Three dimensional displays, Tutorials},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.18}
}

@Article{Han2013,
  Title                    = {{T}he {T}racking of a {M}oving {O}bject by a {M}obile {R}obot {F}ollowing the {O}bject’s {S}ound},
  Author                   = {Han, Jongho and Han, Sunsin and Lee, Jangmyung},
  Journal                  = {Journal of Intelligent \& Robotic Systems},
  Year                     = {2013},

  Month                    = {July},
  Number                   = {1},
  Pages                    = {31-42},
  Volume                   = {71},

  Abstract                 = {{T}he tracking of a moving object with a mobile robot has been implemented based on the detected sound from the moving object using a microphone array. {T}he difference between the travel times of the sound source to each of the three microphones mounted to the robot has been used to calculate the distance and orientation of the sound source. {T}he cross-correlations between the received signals have been used to detect the individual sound signal from the object and to calculate the time difference between two signals. {T}his provides reliable and precise time differences among the sound signals arrived at the microphones compared to the conventional method. {I}n order to determine the tracking direction to the sound source, {F}uzzy rules have been applied; the results are used for real-time control of the mobile robot. {T}he efficiency of the proposed algorithm has been demonstrated through real-world experiments and compared to the conventional approach.},
  Doi                      = {10.1007/s10846-012-9769-3},
  File                     = {:sources/art%3A10.1007%2Fs10846-012-9769-3.pdf:PDF},
  Keywords                 = {Microphone array, Tracking, Estimation, Mobile robot, Sound source},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Book{Hartley2004,
  Title                    = {{M}ultiple {V}iew {G}eometry in {C}omputer {V}ision},
  Author                   = {Hartley, Richard and Zisserman, Andrew},
  Publisher                = {Cambridge University Press},
  Year                     = {2004},
  Edition                  = {2},

  File                     = {:sources/Hartley, Zisserman - Multiple View Geometry in Computer Vision.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.08.05}
}

@InProceedings{Hashimoto2007,
  Title                    = {{M}oving-object tracking with multi-laser range sensors for mobile robot navigation},
  Author                   = {Hashimoto, M. and Takahashi, K. and Matsui, Y.},
  Booktitle                = {IEEE International Conference on Robotics and Biomimetics},
  Year                     = {2007},

  Address                  = {China, Sanya},
  Month                    = {December},
  Pages                    = {399 - 404},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents a method for moving-object tracking with in-vehicle 2{D}-laser range sensors ({LRS}). {S}ince a sensing area of the {LRS} is limited in orientation, mobile robot is equipped with multi-{LRS}'s for omnidirectional sensing. {I}n order for moving-object tracking by multi-{LRS}'s cooperation, the coordinate frames of the multi-{LRS}'s are calibrated based on {K}alman filter and chi-hypothesis testing. {T}he moving-object tracking is achieved by {K}alman filter, {C}ovariance {I}ntersection and the assignment algorithm based data association. {A} rule based track management system is embedded into the tracker in order to improve the tracking performance. {T}he experimental result of three people tracking in indoor environments validates the proposed method.},
  Doi                      = {10.1109/ROBIO.2007.4522195},
  File                     = {:sources/04522195.pdf:PDF},
  Keywords                 = {Biosensors, Computer architecture, Filters, Indoor environments, Laser modes, Mobile robots, Navigation, Robot kinematics, Robot sensing systems, Sensor systems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Book{Haug2012,
  Title                    = {{B}ayesian {E}stimation and {T}racking - {A} {P}ractical {G}uide},
  Author                   = {Haug, Anton J.},
  Publisher                = {John Wiley and Sons},
  Year                     = {2012},

  File                     = {:sources/9obr0.Bayesian.Estimation.and.Tracking.A.Practical.Guide.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.02.25}
}

@Book{Haykin2001,
  Title                    = {{K}alman {F}iltering {A}nd {N}eural {N}etworks},
  Author                   = {Haykin, Simon},
  Publisher                = {John Wiley and Sons},
  Year                     = {2001},

  File                     = {:sources/Kalman Filtering and Neural Networks.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.02.25}
}

@Article{Horn1981,
  Title                    = {{D}etermining {O}ptical {F}low},
  Author                   = {Berthold K. P. Horn and Brian G. Schunck},
  Journal                  = {Artificial Intelligence},
  Year                     = {1981},
  Pages                    = {185 - 203},
  Volume                   = {17},

  Abstract                 = {{O}ptical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. {A} second constraint is needed. {A} method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. {A}n iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. {T}he algorithm is robust in that it can handle image sequences that are quantified rather coarsely in space and time. {I}t is also insensitive to quantization of brightness levels and additive noise. {E}xamples are included where the assumption of smoothness is violated at singular points or along lines in the image.},
  File                     = {:sources/horn81.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.20}
}

@Article{Hu2015,
  Title                    = {{M}oving object detection and tracking from video captured by moving camera },
  Author                   = {Wu-Chih Hu and Chao-Ho Chen and Tsong-Yi Chen and Deng-Yuan Huang and Zong-Che Wu},
  Journal                  = {Journal of Visual Communication and Image Representation},
  Year                     = {2015},
  Pages                    = {164 - 180},
  Volume                   = {30},

  Abstract                 = {{A}bstract {T}his paper presents an effective method for the detection and tracking of multiple moving objects from a video sequence captured by a moving camera without additional sensors. {M}oving object detection is relatively difficult for video captured by a moving camera, since camera motion and object motion are mixed. {I}n the proposed method, the feature points in the frames are found and then classified as belonging to foreground or background features. {N}ext, moving object regions are obtained using an integration scheme based on foreground feature points and foreground regions, which are obtained using an image difference scheme. {T}hen, a compensation scheme based on the motion history of the continuous motion contours obtained from three consecutive frames is applied to increase the regions of moving objects. {M}oving objects are detected using a refinement scheme and a minimum bounding box. {F}inally, moving object tracking is achieved using a {K}alman filter based on the center of gravity of a moving object region in the minimum bounding box. {E}xperimental results show that the proposed method has good performance.},
  Doi                      = {10.1016/j.jvcir.2015.03.003},
  File                     = {:sources/1-s2.0-S104732031500053X-main.pdf:PDF},
  ISSN                     = {1047-3203},
  Keywords                 = {Object detection, Moving camera, Object tracking, Feature classification, Image difference, Object motion, Motion history, Ego-motion compensation },
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S104732031500053X}
}

@InProceedings{Hwang2010,
  Title                    = {{V}ision tracking system for mobile robots using two {K}alman filters and a slip detector},
  Author                   = {Hwang, Wonsang and Park, Jaehong and Kwon, Hyun-il and Anjum, M.L. and Kim, Jong-hyeon and Lee, Changhun and Kim, Kwang-soo and Cho, Dong-il Dan},
  Booktitle                = {2010 International Conference on Control Automation and Systems},
  Year                     = {2010},

  Address                  = {South Corea, Gyeonggi-do},
  Month                    = {October},
  Pages                    = {2041 - 2046},
  Publisher                = {IEEE},

  Abstract                 = {{T}he vision tracking system in this paper estimates the robot position relative to a target and rotates the camera towards the target. {T}o estimate the robot position of mobile robot, the system combines information from an accelerometer, a gyroscope, two encoders, and a vision sensor. {T}he encoders can provide fairly accurate robot position information, but the encoder data are not reliable when robot wheels slip. {A}ccelerometer data can provide the robot position information even when the wheels are slipping, but a long term position estimation is difficult, because of integration of errors arising from bias and noise. {T}o overcome the drawbacks of each method mentioned in the above, the proposed system uses data fusion with two {K}alman filters and a slip detector. {O}ne {K}alman filter is for the slip case, and the other is for the no-slip case. {E}ach {K}alman filter uses a different sensor combination for estimating the robot motion. {T}he slip detector compares the data from the accelerometer with the data from the encoders, and decides if a slip condition has occurred. {A}ccordingly, based on the decision of the slip detector, the system chooses one of the outputs of the two {K}alman filters, which is subsequently used for calculating the camera angle of the vision tracking system. {T}he vision tracking system is implemented on a two-wheeled robot. {T}o evaluate the tracking and recognition performance of the implemented system, experiments are performed for various robot motion scenarios in various environments.},
  File                     = {:sources/05670110.pdf:PDF},
  Keywords                 = {Detectors, Kalman filters, Machine vision, Mobile robots, Robot sensing systems, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Book{Jaehne2005,
  Title                    = {{D}igital {I}mage {P}rocessing},
  Author                   = {Jähne, Bernd},
  Publisher                = {Springer-Verlag Berlin Heidelberg},
  Year                     = {2005},
  Edition                  = {6},

  File                     = {:sources/o54fy.Digital.Image.Processing.Concepts.Algorithms.and.Scientific.Applications.6th.edition.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.16}
}

@InProceedings{Jung2012,
  Title                    = {{C}ontrol algorithms for a mobile robot tracking a human in front},
  Author                   = {Eui-Jung Jung and Byung-Ju Yi and Shin'ichi Yuta},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  Year                     = {2012},

  Address                  = {Portugal, Vilamoura},
  Month                    = {October},
  Pages                    = {2411 - 2416},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents tracking algorithms of a mobile robot that tracks a human in front. {T}he mobile robot, which is a differential-driven wheel type, is equipped with a laser range finder to perform human tracking in front. {I}nitially, we recommend the torso part for the robust tracking of the human body in outdoor environment by a laser range finder. {T}o track a human in front, we define a virtual target determined by the velocity of the target human. {F}or a more efficient movement of the mobile robot, a desired heading direction of the robot considering motion vectors of the robot and the human is proposed.},
  Doi                      = {10.1109/IROS.2012.6386200},
  File                     = {:sources/06386200.pdf:PDF},
  Keywords                 = {Humans, Mobile robots, Noise, Target tracking, Vectors},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.03}
}

@Book{Kaehler2013,
  Title                    = {{L}earning {O}pen{CV} - {C}omputer {V}ision in {C}++ with the {O}pen{CV} {L}ibrary},
  Author                   = {Kaehler, Adrian and Bradski, Gary},
  Publisher                = {O'Reilly Media},
  Year                     = {2013},
  Edition                  = {2},

  File                     = {:sources/oreilly-learning-opencv-second-edition-2013-ver2.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.08.05}
}

@Article{Kanatani1987,
  Title                    = {{C}amera rotation invariance of image characteristics},
  Author                   = {Kanatani, K.},
  Journal                  = {Computer Vision, Graphics, and Image Processing},
  Year                     = {1987},
  Number                   = {3},
  Pages                    = {328-354},
  Volume                   = {39},

  Abstract                 = {{T}he image transformation due to camera rotation relative to a stationary scene is analyzed, and the associated transformation rules of “features” given by weighted averaging of the image are derived by considering infinitesimal generators on the basis of group representation theory. {T}hree-dimensional vectors and tensors are reduced to two-dimensional invariants on the image plane from the viewpoint of projective geometry. {T}hree-dimensional invariants and camera rotation reconstruction are also discussed. {T}he result is applied to the shape recognition problem when camera rotation is involved.},
  Doi                      = {10.1016/S0734-189X(87)80185-8},
  File                     = {:sources/ADA171615.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.09.14}
}

@Book{Karasulu2013,
  Title                    = {{P}erformance {E}valuation {S}oftware - {M}oving {O}bject {D}etection and {T}racking in {V}ideo},
  Author                   = {Karasulu, Bahadir and Korukoglu, Serdar},
  Publisher                = {Springer-Verlag New York},
  Year                     = {2013},
  Edition                  = {1},
  Series                   = {SpringerBriefs in Computer Science},

  Abstract                 = {{P}erformance {E}valuation {S}oftware: {M}oving {O}bject {D}etection and {T}racking in {V}ideos introduces a software approach for the real-time evaluation and performance comparison of the methods specializing in moving object detection and/or tracking ({D}&{T}) in video processing. {D}igital video content analysis is an important item for multimedia content-based indexing ({MCBI}), content-based video retrieval ({CBVR}) and visual surveillance systems. {T}here are some frequently-used generic algorithms for video object {D}&{T} in the literature, such as {B}ackground {S}ubtraction ({BS}), {C}ontinuously {A}daptive {M}ean-shift ({CMS}), {O}ptical {F}low ({OF}), etc. {A}n important problem for performance evaluation is the absence of any stable and flexible software for comparison of different algorithms. {I}n this frame, we have designed and implemented the software for comparing and evaluating the well-known video object {D}&{T} algorithms on the same platform. {T}his software is able to compare them with the same metrics in real-time and on the same platform. {I}t also works as an automatic and/or semi-automatic test environment in real-time, which uses the image and video processing essentials, e.g. morphological operations and filters, and ground-truth ({GT}) {XML} data files, charting/plotting capabilities, etc. {A}long with the comprehensive literature survey of the abovementioned video object {D}&{T} algorithms, this book also covers the technical details of our performance benchmark software as well as a case study on people {D}&{T} for the functionality of the software},
  Doi                      = {10.1007/978-1-4614-6534-8},
  File                     = {:sources/Karasulu B., Korukoglu S. - Moving Object Detection and Tracking in Videos.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Kim2011,
  Title                    = {{V}ision system for mobile robots for tracking moving targets, based on robot motion and stereo vision information},
  Author                   = {Kim, Kwang-soo and Bahn, Wook and Lee, Changhun and Lee, Tae-jae and Shaikh, M.M. and Kim, Kwang-soo},
  Booktitle                = {IEEE/SICE International Symposium on System Integration},
  Year                     = {2011},

  Address                  = {Japan, Kyoto},
  Month                    = {December},
  Pages                    = {634 - 639},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents a vision-tracking for mobile robots, which tracks a moving target based on robot motion and stereo vision information. {T}he proposed system controls pan and tilt actuators attached to a stereo camera, using the data from a gyroscope, robot wheel encoders, pan and tilt actuator encoders, and the stereo camera. {U}sing this proposed system, the stereo camera always faces the moving target. {T}he developed system calculates the angles of the pan and tilt actuators by estimating the relative position of the target with respect to the position of the robot. {T}he developed system estimates the target position using the robot motion information and the stereo vision information. {T}he movement of the robot is modeled as the transformation of the frame, which consists of a rotation and a translation. {T}he developed system calculates the rotation using 3-axis gyroscope data and the translation using robot wheel encoder data. {T}he proposed system measures the position of the target relative to the robot, combining the encoder data of pan and tilt actuators and the disparity map of the stereo vision. {T}he inevitable mismatch of the data, which occurs from the asynchrony of the multiple sensors, is prevented by the proposed system, which compensates for the communication latency and the computation time. {T}he experimental results show that the developed system achieves excellent tracking performance in several motion scenarios, including combinations of straights and curves and climbing of slopes.},
  Doi                      = {10.1109/SII.2011.6147522},
  File                     = {:sources/06147522.pdf:PDF},
  Keywords                 = {Actuators, Cameras, Instruction sets, Mobile robots, Robot vision systems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Kim2012,
  Title                    = {{A} {R}obotic {P}an and {T}ilt 3-{D} {T}arget {T}racking {S}ystem by {D}ata {F}usion of {V}ision, {E}ncoder, {A}ccelerometer, and {G}yroscope {M}easurements},
  Author                   = {Kim, Tae-Il and Bahn, Wook and Lee, Chang-Hun and Lee, Tae-Jae and Jang, Byung-Moon and Lee, Sang-Hoon and Moon, Min-Wug and Cho, Dong-Il},
  Booktitle                = {Intelligent Robotics and Applications - 5th International Conference},
  Year                     = {2012},

  Address                  = {Canada, Montreal},
  Month                    = {October},
  Pages                    = {676-685},
  Publisher                = {Springer Berlin Heidelberg},

  Abstract                 = {{T}his paper presents a vision-tracking system for mobile robots, which travel in a 3-dimentional environment. {T}he developed system controls pan and tilt actuators attached to a camera so that a target is always directly in the line of sight of the camera. {T}his is achieved by using data from robot wheel encoders, a 3-axis accelerometer, a 3-axis gyroscope, pan and tilt motor encoders, and camera. {T}he developed system is a multi-rate sampled data system, where the sampling rate of the camera is different with that of the other sensors. {F}or the accurate estimation of the robot velocity, the developed system detects the slip of robot wheels, by comparing the data from the encoders and the accelerometer. {T}he developed system estimates the target position by using an extended {K}alman filter. {T}he experiments are performed to show the tracking performance of the developed system in several motion scenarios, including climbing slopes and slip cases.},
  Doi                      = {10.1007/978-3-642-33515-0_66},
  File                     = {:sources/chp%3A10.1007%2F978-3-642-33515-0_66.pdf:PDF},
  Keywords                 = {Vision tracking system, Sensor data fusion, Kalman filter, Slip detection},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Kulchandani2015,
  Title                    = {{M}oving object detection: {R}eview of recent research trends},
  Author                   = {Kulchandani, Jaya S. and Dangarwala, Kruti J.},
  Booktitle                = {Pervasive Computing (ICPC), 2015 International Conference on},
  Year                     = {2015},

  Address                  = {India, Pune},
  Month                    = {January},
  Pages                    = {1 - 5},
  Publisher                = {IEEE},

  Abstract                 = {{M}oving object detection is the task of identifying the physical movement of an object in a given region or area. {O}ver last few years, moving object detection has received much of attraction due to its wide range of applications like video surveillance, human motion analysis, robot navigation, event detection, anomaly detection, video conferencing, traffic analysis and security. {I}n addition, moving object detection is very consequential and efficacious research topic in field of computer vision and video processing since it forms a critical step for many complex processes like video object classification and video tracking activity. {C}onsequently, identification of actual shape of moving object from a given sequence of video frames becomes pertinent. {H}owever, task of detecting actual shape of object in motion becomes tricky due to various challenges like dynamic scene changes, illumination variations, presence of shadow, camouflage and bootstrapping problem. {T}o reduce the effect of these problems, researchers have proposed number of new approaches. {T}his paper provides a brief classification of the classical approaches for moving object detection. {F}urther, paper reviews recent research trends to detect moving object for single stationary camera along with discussion of key points and limitations of each approach.},
  Doi                      = {10.1109/PERVASIVE.2015.7087138},
  File                     = {:sources/07087138.pdf:PDF},
  Keywords                 = {Human Motion Analysis, Moving Object Detection, Object Classification, Tracking, Video Survillance},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.05}
}

@InProceedings{Kumar2001,
  Title                    = {{A}erial video surveillance and exploitation},
  Author                   = {Kumar, Rakesh and Sawhney, H. and Samarasekera, Supun and Hsu, S. and Hai Tao and Yanlin Guo and Hanna, K. and Pope, A. and Wildes, R. and Hirvonen, D. and Hansen, M. and Burt, P.},
  Booktitle                = {Proceedings of the IEEE},
  Year                     = {2001},
  Month                    = {October},
  Pages                    = {1518 - 1539},
  Publisher                = {IEEE},
  Volume                   = {89},

  Abstract                 = {{T}here is growing interest in performing aerial surveillance using video cameras. {C}ompared to traditional framing cameras, video cameras provide the capability to observe ongoing activity within a scene and to automatically control the camera to track the activity. {H}owever, the high data rates and relatively small field of view of video cameras present new technical challenges that must be overcome before such cameras can be widely used. {I}n this paper, we present a framework and details of the key components for real-time, automatic exploitation of aerial video for surveillance applications. {T}he framework involves separating an aerial video into the natural components corresponding to the scene. {T}hree major components of the scene are the static background geometry, moving objects, and appearance of the static and dynamic components of the scene. {I}n order to delineate videos into these scene components, we have developed real time, image-processing techniques for 2-{D}/3-{D} frame-to-frame alignment, change detection, camera control, and tracking of independently moving objects in cluttered scenes. {T}he geo-location of video and tracked objects is estimated by registration of the video to controlled reference imagery, elevation maps, and site models. {F}inally static, dynamic and reprojected mosaics may be constructed for compression, enhanced visualization, and mapping applications},
  Doi                      = {10.1109/5.959344},
  File                     = {:sources/00959344.pdf:PDF},
  Keywords                 = {Cameras, Layout, Machine intelligence, Monitoring, Object detection, Vehicle detection, Vehicle dynamics, Video compression, Video surveillance, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Lang2010,
  Title                    = {{V}ision based object identification and tracking for mobile robot visual servo control},
  Author                   = {Haoxiang Lang and Ying Wang and W. de Silva Clarence},
  Booktitle                = {8th IEEE International Conference on Control and Automation (ICCA)},
  Year                     = {2010},

  Address                  = {China, Xiamen},
  Month                    = {June},
  Pages                    = {92 - 96},
  Publisher                = {IEEE},

  Abstract                 = {{A} key problem of an {I}mage {B}ased {V}isual {S}ervo ({IBVS}) system is how to identify and track objects in a series of images. {I}n this paper, a scale-invariant image feature detector and descriptor, which is called the {S}cale-{I}nvariant {F}eature {T}ransform ({SIFT}), is utilized to achieve robust object tracking in terms of rotation, scaling and changes of illumination. {T}o the best of our knowledge, this paper represents the first work to apply the {SIFT} algorithm to visual servoing for robust mobile robot tracking. {F}irst, a {SIFT} method is used to generate the feature points of an object template and a series of images are acquired while the robot is moving. {S}econd, a feature matching method is applied to match the features between the template and the images. {F}inally, based on the locations of the matched feature points, the location of the object is approximated in the images of camera views. {T}his algorithm of object identification and tracking is applied in an {I}mage-{B}ased {V}isual {S}ervo ({IBVS}) system for providing the location of the object in the feedback loop. {I}n particular, the {IBVS} controller determines the desired wheel speeds ω_1 and ω_2 of a wheeled mobile robot, and accordingly commands the low-level controller of the robot. {T}hen the {IBVS} controller drives the robot toward a target object until the location of the object reaches the desired location in the image. {T}he {IBVS} system is implemented and tested in a mobile robot with an on-board camera, in our laboratory. {T}he results are used to demonstrate satisfactory performance of the object identification and tracking algorithm. {F}urthermore, a {MATLAB} simulation is used to confirm the stability and convergence of the {IBVS} controller.},
  Doi                      = {10.1109/ICCA.2010.5524180},
  File                     = {:sources/05524180.pdf:PDF},
  Keywords                 = {Cameras, Computer vision, Detectors, Mobile robots, Object detection, Robot control, Robot vision systems, Robustness, Servomechanisms, Servosystems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.03}
}

@InProceedings{Lee2011,
  Title                    = {{V}ision tracking of a moving robot from a second moving robot using both relative and absolute position referencing methods},
  Author                   = {Lee, Changhun and Park, Jaehong and Bahn, Wook and Kim, Tae-Il and Lee, Tae-jae and Shaikh, Muneeb and Kim, Kwang-soo and Cho, Dong-Il},
  Booktitle                = {37th Annual Conference on IEEE Industrial Electronics Society},
  Year                     = {2011},

  Address                  = {Australia, Melbourne},
  Month                    = {November},
  Pages                    = {325 - 330},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents a vision tracking system for a moving robot that provides continuous tracking of another moving robot. {T}he purpose of the proposed system is to control the line of sight of the camera, which is on the targetting robot, for tracking the targetted robot by using the position information of both the targetting robot and the targetted robot. {T}o estimate the positions of both robots, the proposed system uses two encoders and a gyroscope for the targetting robot and active beacons for the targetted robot. {T}he proposed system computes the relative position between the two robots and aligns the camera for tracking the targetted robot. {T}he proposed system is experimentally evaluated for various scenarios, including the occultation of the targetted robot and temporary illumination changes. {T}he experimental results show that the proposed system is able to track the moving robot well without visual information, in normal situations.},
  Doi                      = {10.1109/IECON.2011.6119273},
  File                     = {:sources/06119273.pdf:PDF},
  Keywords                 = {Cameras, Mobile robots, Robot kinematics, Robot vision systems, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Li2007,
  Title                    = {{G}round {M}oving {T}arget {T}racking {C}ontrol {S}ystem {D}esign for {UAV} {S}urveillance},
  Author                   = {Li, Zhongjian and Ding, Jiarui},
  Booktitle                = {Proceedings of IEEE International Conference on Automation and Logistics},
  Year                     = {2007},

  Address                  = {China, Jinan},
  Month                    = {August},
  Pages                    = {1458 - 1463},
  Publisher                = {IEEE},

  Abstract                 = {{G}round target detection, recognition and tracking play critical roles in aerial video surveillance applications. {A} ground moving target detection and tracking system based on active vision is studied firstly. {A}fter compensating the global motion, the independent moving targets are detected by the salience of residual images. {T}hen object trackers are initialized to track these detected moving targets, at the same time, by controlling the payload equipment to achieve the given surveillance tasks. {T}he influence of the movement of {UAV} to swivel table is studied and the apparent motion function is developed. {I}n order to achieve good tracking performance, a mixed {H}2/{H}-infinite robust flight controller is designed for the {UAV} and a controller containing the disturbance observer ({DOB}) is designed for payload platform. {T}his video servo control system successfully compensates the influence of {UAV} movement to swivel table control. {R}eal-time simulation results show the effectiveness of the designed system.},
  Doi                      = {10.1109/ICAL.2007.4338800},
  File                     = {:sources/04338800.pdf:PDF},
  Keywords                 = {Control systems, Hydrogen, Motion detection, Object detection, Payloads, Robust control, Target recognition, Target tracking, Unmanned aerial vehicles, Video surveillance},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Li2009,
  Title                    = {{C}olor-{B}ased {V}isual {O}bject {T}racking with {P}rediction and {E}rror {J}udgment},
  Author                   = {Zhicheng Li and Bing Qiao and Shaobin Deng},
  Booktitle                = {2nd International Congress on Image and Signal Processing (CISP)},
  Year                     = {2009},

  Address                  = {China, Tianjin},
  Month                    = {October},
  Pages                    = {1 - 4},
  Publisher                = {IEEE},

  Abstract                 = {{V}isual tracking algorithm plays an important role in the fields such as guidance and surveillance of mobile robot. {T}hese applications require the vision algorithm with strong robustness and fast processing speed. {C}amshift uses color histogram as a characteristic and mean shift as the search algorithm. {T}he direction of grads ascension is used to reduce the characteristic match time, so that the object orientation could be faster. {B}ut, {C}amshift algorithm will fail when the background has the similar color with the target, it also cannot work correctly when occlusion occurs. {T}his paper describes a new color based tracking algorithm in order to improve the theoretic limitation of {C}amshift. {F}irstly, the improved algorithm combines morphologic to solve the problems of divergence. {S}econdly, {K}alman filter is used to predict the mass center point when the target has occlusion. {T}hirdly, it can receive error information from the error judgment analysis system to adjust the tracking. {F}inally, the result of this experiments shows that the proposed algorithm can track fast moving object successfully and have better robustness for occlusion.},
  Doi                      = {10.1109/CISP.2009.5301475},
  File                     = {:sources/05301475.pdf:PDF},
  Keywords                 = {Cameras, Image motion analysis, Mobile robots, Optical filters, Optical noise, Optical sensors, Robot vision systems, Robustness, Surveillance, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.11}
}

@InProceedings{Liem2008,
  Title                    = {{A} hybrid algorithm for tracking and following people using a robotic dog},
  Author                   = {Liem, Martijn and Visser, Arnoud and Groen, Frans},
  Booktitle                = {Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction},
  Year                     = {2008},

  Address                  = {Netherlands, Amsterdam},
  Pages                    = {185-192},
  Publisher                = {ACM},
  Series                   = {HRI '08},

  Abstract                 = {{T}he capability to follow a person in a domestic environment is an important prerequisite for a robot companion. {I}n this paper, a tracking algorithm is presented that makes it possible to follow a person using a small robot. {T}his algorithm can track a person while moving around, regardless of the sometimes erratic movements of the legged robot. {R}obust performance is obtained by fusion of two algorithms, one based on salient features and one on color histograms. {R}e-initializing object histograms enables the system to track a person even when the illumination in the environment changes. {B}y being able to re-initialize the system on run time using background subtraction, the system gains an extra level of robustness.},
  Doi                      = {10.1145/1349822.1349847},
  File                     = {:sources/p185-liem.pdf:PDF},
  Keywords                 = {awareness and monitoring of humans, robot companion},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Liu2014,
  Title                    = {{D}ynamic objects tracking with a mobile robot using passive {UHF} {RFID} tags},
  Author                   = {Liu, Ran and Huskic, G. and Zell, A.},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
  Year                     = {2014},

  Address                  = {USA, Chicago},
  Month                    = {September},
  Pages                    = {4247 - 4252},
  Publisher                = {IEEE},

  Abstract                 = {{R}ecent research deals more and more with the application of ultra high frequency ({UHF}) radio-frequency identification ({RFID}) on mobile robots. {H}owever, the sensing characteristics between the reader and the tag (i.e. detections and signal strength) are challenging to model due to the influence of environmental effects (e.g. tag density, reflection, diffraction, or absorption). {I}n this paper, we address the problem of dynamic objects tracking with a mobile agent using the signal strength from {UHF} {RFID} tags attached to objects. {O}ur solution estimates the positions of {RFID} tags under a {B}ayesian framework. {M}ore precisely, we combine a two stage dynamic motion model with the dual particle filter, to capture the dynamic motion of the object and to quickly recover from failures in tracking. {T}his approach is then tested on a {S}citos {G}5 mobile robot through various experiments.},
  Doi                      = {10.1109/IROS.2014.6943161},
  File                     = {:sources/06943161.pdf:PDF},
  Keywords                 = {Estimation, Navigation, RFID tags, Robot sensing systems, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Lowe2004,
  Title                    = {{D}istinctive {I}mage {F}eatures from {S}cale-{I}nvariant {K}eypoints},
  Author                   = {Lowe, David G.},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2004},

  Month                    = {November},
  Number                   = {2},
  Pages                    = {91 - 110},
  Volume                   = {60},

  Abstract                 = {{T}his paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. {T}he features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3{D} viewpoint, addition of noise, and change in illumination. {T}he features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. {T}his paper also describes an approach to using these features for object recognition. {T}he recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a {H}ough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. {T}his approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  Doi                      = {10.1023/B:VISI.0000029664.99615.94},
  File                     = {:sources/art%3A10.1023%2FB%3AVISI.0000029664.99615.94.pdf:PDF},
  Keywords                 = {invariant features, object recognition, scale invariance, image matching},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.13}
}

@InProceedings{Lucas1981,
  Title                    = {{A}n iterative image registration technique with an application to stereo vision},
  Author                   = {Lucas, Bruce D. and Kanade, Takeo},
  Booktitle                = {7th International Joint Conference on Artificial intelligence (IJCAI)},
  Year                     = {1981},
  Month                    = {August},
  Pages                    = {674 - 679},
  Publisher                = {Morgan Kaufmann Publishers},
  Volume                   = {2},

  Abstract                 = {{I}mage registration finds a variety of applications in computer vision. {U}nfortunately, traditional image registration techniques tend to be costly. {W}e present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of {N}ewton-{R}aphson iteration. {O}ur technique is taster because it examines far fewer potential matches between the images than existing techniques {F}urthermore, this registration technique can be generalized to handle rotation, scaling and shearing. {W}e show how our technique can be adapted tor use in a stereo vision system.},
  File                     = {:sources/lucaskanade81.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.26}
}

@Book{Maggio2011,
  Title                    = {{V}ideo {T}racking - {T}heory and {P}ractice},
  Author                   = {Maggio, Emilio and Cavallaro, Andrea},
  Publisher                = {John Wiley and Sons},
  Year                     = {2011},
  Edition                  = {1},

  File                     = {:sources/Video tracking - theory and practice.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.02}
}

@InProceedings{Markovic2014,
  Title                    = {{M}oving object detection, tracking and following using an omnidirectional camera on a mobile robot},
  Author                   = {Marković, Ivan and Chaumette, François and Petrović, Ivan},
  Booktitle                = {IEEE International Conference on Robotics and Automation},
  Year                     = {2014},

  Address                  = {Hong Kong},
  Month                    = {June},
  Pages                    = {5630 - 5635},
  Publisher                = {IEEE},

  Abstract                 = {{E}quipping mobile robots with an omnidirectional camera is very advantageous in numerous applications as all information about the surrounding scene is stored in a single image frame. {I}n the given context, the present paper is concerned with detection, tracking and following of a moving object with an omnidirectional camera. {T}he camera calibration and image formation is based on the spherical unified projection model thus yielding a representation of the omnidirectional image on the unit sphere. {D}etection of moving objects is performed by calculating a sparse optical flow in the image and then lifting the flow vectors on the unit sphere where they are discriminated as dynamic or static by analytically calculating the distance of the terminal vector point to a great circle arc. {T}he flow vectors are then clustered and the center of gravity is calculated to form the sensor measurement. {F}urthermore, the tracking is posed as a {B}ayesian estimation problem on the unit sphere and the solution based on the von {M}ises-{F}isher distribution is utilized. {V}isual servoing is performed for the object following task where the control law calculation is based on the projection of a point on the unit sphere. {E}xperimental results obtained by a camera with a fish-eye lens mounted on a differential drive mobile robot are presented and discussed.},
  Doi                      = {10.1109/ICRA.2014.6907687},
  File                     = {:sources/06907687.pdf:PDF},
  Keywords                 = {Cameras, Mobile robots, Optical imaging, Robot kinematics, Robot vision systems, Vectors},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Mohamed2014a,
  Title                    = {{I}llumination-{R}obust {O}ptical {F}low {U}sing a {L}ocal {D}irectional {P}attern},
  Author                   = {Mahmoud A. Mohamed and Hatem A. Rashwan and Barbel Mertsching and Miguel Angel Garcia and Domenec Puig},
  Journal                  = {IEEE Transactions on Circuits and Systems for Video Technology},
  Year                     = {2014},

  Month                    = {February},
  Number                   = {9},
  Pages                    = {1499 - 1508},
  Volume                   = {24},

  Abstract                 = {{M}ost of the variational optical flow methods are based on the well-known brightness constancy assumption or high-order constancy assumptions to implement the data term in the optimization energy function. {U}nfortunately, any variation in the lighting within the scene violates the brightness constancy constraint; in turn, the gradient constancy assumption does not work properly with large illumination changes. {T}his paper proposes an illumination-robust constancy based on a robust texture descriptor rather than the brightness constancy. {T}hus, the similarity function used as a data term was obtained from extracting texture features through the local directional pattern descriptor for two consecutive frames within the duality total variational optical flow algorithm. {I}n addition, a weighted nonlocal term that depends on both the color similarity and the occlusion state of pixels is integrated during the optimization process to increase the accuracy of the resulting flow field. {T}he experimental results show a qualitative comparison with the proposed approach and yield state-of-the-art results on the {KITTI}, {M}idleburry, and {MPI}-sintel data sets.},
  Doi                      = {10.1109/TCSVT.2014.2308628},
  File                     = {:sources/06748891.pdf:PDF},
  Keywords                 = {Brightness, Feature extraction, Lighting, Optical imaging,Optical sensors, Robustness, Transforms},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.03}
}

@InProceedings{Mohamed2014,
  Title                    = {{R}eal-time moving objects tracking for mobile-robots using motion information},
  Author                   = {Mohamed, M.A. and Boddeker, C. and Mertsching, B.},
  Booktitle                = {IEEE International Symposium on Safety, Security, and Rescue Robotics},
  Year                     = {2014},

  Address                  = {Japan, Hokkaido},
  Month                    = {October},
  Pages                    = {1 - 6},
  Publisher                = {IEEE},

  Abstract                 = {{T}he perceptional of the motion of objects is a key problem for a mobile robot to perform tasks in a dynamic environment. {T}hus, we present a real-time approach for tracking multiple moving objects. {T}he proposed algorithm initially detects moving regions and a dense optical flow technique is exclusively applied to those regions between two consecutive frames. {A}fterwards, the moving objects in each region are determined based on the planar parallax motion by assuming that independently moving objects undergo pure translation. {F}or subsequent frames, the detected moving objects are tracked based on the orientation of the flow fields, while the new position is updated. {I}n turn, the new detected objects are modeled during a tracking period. {T}he proposed algorithm has been tested with various scenarios and the experimental results demonstrate that the proposed algorithm works properly. {I}t can be shown that there is a significant reduction in the overall processing time for detecting and tracking multiple moving objects in a scene.},
  Doi                      = {10.1109/SSRR.2014.7017674},
  File                     = {:sources/07017674.pdf:PDF},
  Keywords                 = {Cameras, Heuristic algorithms, Motion segmentation, Optical imaging, Real-time systems, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Motokucho2014,
  Title                    = {{V}ision-based human-following control using optical flow field for power assisted wheelchair},
  Author                   = {Motokucho, T. and Oda, N.},
  Booktitle                = {13th International Workshop on Advanced Motion Control (AMC)},
  Year                     = {2014},

  Address                  = {Japan, Yokohama},
  Month                    = {March},
  Pages                    = {266 - 271},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper describes a power assisting control with human following function for robotic wheelchair. {I}n our past research, visual feedback control of the wheelchair has been proposed for power assisting according to environmental changes, and it is useful for power assisted wheelchair to integrate several driving assistance, such as obstacle avoidance, following motion only from unified motion controller. {I}n this paper, we propose human following function on the vision-based power assist controller. {T}he proposed method employs optical flow vectors in the field of camera view, and it can be realized only from human leg view. {I}n case that the camera system is mounted at lower position, then the camera image only includes human lower limb. {T}he proposed method allows for stably tracking the walking human even in such a case. {T}he validity of our approach is evaluated by several experimental results.},
  Doi                      = {10.1109/AMC.2014.6823293},
  File                     = {:sources/06823293.pdf:PDF},
  Keywords                 = {Cameras, Force, Observers, Optical imaging, Vectors, Wheelchairs},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.28}
}

@InProceedings{Murakami2012,
  Title                    = {{P}osition tracking and recognition of everyday objects by using sensors embedded in an environment and mounted on mobile robots},
  Author                   = {Murakami, K. and Matsuo, K. and Hasegawa, T. and Kurazume, R.},
  Booktitle                = {IEEE International Conference on Robotics and Automation},
  Year                     = {2012},

  Address                  = {USA, Saint Paul},
  Month                    = {May},
  Pages                    = {2210 - 2216},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper describes an object tracking system for a robot working in an everyday environment, which tracks and recognizes everyday objects. {P}assive {RFID} ({R}adio {F}requency {ID}entification) tags are attached to the objects for object recognition. {T}he system consists of static sensors embedded in the environment and mobile sensors mounted on mobile robots. {B}y utilizing the different characteristics and advantages of these sensors, the system achieves good performance in an everyday environment. {A}lthough the tag {ID} and the position of an object carried by a person is not measurable by static sensors or by mobile sensors, the system can estimate them by using an {SIR} ({S}equential importance resampling) particle filter that integrates the data obtained by the static sensors and the mobile sensors. {I}n the experiment, the system successfully tracked 20 objects, some of which were held by a person.},
  Doi                      = {10.1109/ICRA.2012.6225329},
  File                     = {:sources/06225329.pdf:PDF},
  Keywords                 = {Mobile robots, Position measurement, Radiofrequency identification, Robot sensing systems, Sensor systems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Murray1994,
  Title                    = {{M}otion tracking with an active camera},
  Author                   = {Murray, Don and Basu, Anup},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {1994},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {449 - 459},
  Volume                   = {16},

  Abstract                 = {{T}his paper describes a method for real-time motion detection using an active camera mounted on a pan/tilt platform. {I}mage mapping is used to align images of different viewpoints so that static camera motion detection can be applied. {I}n the presence of camera position noise, the image mapping is inexact and compensation techniques fail. {T}he use of morphological filtering of motion images is explored to desensitize the detection algorithm to inaccuracies in background compensation, {T}wo motion detection techniques are examined, and experiments to verify the methods are presented. {T}he system successfully extracts moving edges from dynamic images even when the pan/tilt angles between successive frames are as large as 3},
  Doi                      = {10.1109/34.291452},
  File                     = {:sources/background_compensation_and_an_active_camera_motion_tracking_algorithm.pdf:PDF},
  Keywords                 = {Animals, Application software, Cameras, Computer vision ,Filtering, Motion detection, Object recognition, Robot vision systems, Service robots, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.06.07}
}

@InProceedings{Nguyen2014,
  Title                    = {{V}ision-based qualitative path-following control of quadrotor aerial vehicle},
  Author                   = {Nguyen, T. and Mann, G.K.I. and Gosine, R.G.},
  Booktitle                = {International Conference on Unmanned Aircraft Systems (ICUAS)},
  Year                     = {2014},

  Address                  = {USA, Florida, Orlando},
  Month                    = {May},
  Pages                    = {412 - 417},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents a vision-based qualitative 3{D} navigation technique as well as first results of adapting {F}unnel {L}ane theory into path-following control of quadrotor aerial vehicle. {T}he image's {K}anade-{L}ucas-{T}omasi ({KLT}) corner features are detected along the reference path in order to build a funnel lane for navigation. {T}hen a funnel-lane navigation calculation is developed to estimate the desired yaw angle and height for the next movement. {T}he proposed algorithm uses the front camera, heading measurement and altimeter of the {A}r.{D}rone quadrotor for navigation. {T}he remarkable advantage of the proposed technique is independently working in {GPS}-denied environments without the support of the external tracking system as well as computationally efficient. {A}s compared to other available approaches, at-least one matched feature is required during path following. {T}he proposed navigation technique can be implemented for visual-homing, visual-servoing and visual-teach-and-repeat ({VT}&{R}) applications. {T}he proposed method is simulated in {ROS} and {G}azebo simulator followed by a realtime experiment with the {A}r.{D}rone quadrotor.},
  Doi                      = {10.1109/ICUAS.2014.6842281},
  File                     = {:sources/06842281.pdf:PDF},
  Keywords                 = {Cameras, Image segmentation, Navigation, Phase measurement, Three-dimensional displays, Vehicles, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.28}
}

@InProceedings{Olivares-Mendez2009,
  Title                    = {{A} pan-tilt camera {F}uzzy vision controller on an unmanned aerial vehicle},
  Author                   = {Olivares-Méndez, Miguel A. and Campoy, Pascual and Martínez, Carol and Mondragón, Iván},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
  Year                     = {2009},

  Address                  = {USA, MO, St. Louis},
  Month                    = {October},
  Pages                    = {2879 - 2884},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents an implementation of two {F}uzzy {L}ogic controllers working in parallel for a pan-tilt camera platform on an {UAV}. {T}his implementation uses a basic {L}ucas-{K}anade tracker algorithm, which sends information about the error between the center of the object to track and the center of the image, to the {F}uzzy controller. {T}his information is enough for the controller to follow the object by moving a two axis servo-platform, regardless the {UAV} vibrations and movements. {T}he two {F}uzzy controllers for each axis, work with a rules-base of 49 rules, two inputs and one output with a more significant sector defined to improve the behavior of those controllers. {T}he controllers have shown very good performances in real flights for statics objects, tested on the {C}olibri prototypes.},
  Doi                      = {10.1109/IROS.2009.5354576},
  File                     = {:sources/05354576.pdf:PDF},
  Keywords                 = {Cameras, Computer vision, Control systems, Fuzzy control, Fuzzy logic, Helicopters, State estimation, Target tracking, Testing, Unmanned aerial vehicles},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Park2011,
  Title                    = {{P}an/{T}ilt {C}amera {C}ontrol for {V}ision {T}racking {S}ystem {B}ased on the {R}obot {M}otion and {V}ision {I}nformation},
  Author                   = {Park, Jaehong and Hwang, Wonsang and Bahn, Wook and Lee, Changhun and Tae-Il, Kim and Shaikh, Muhammad Muneeb and Kim, Kwangsoo and Cho, Dong-il Dan},
  Booktitle                = {18th IFAC World Congress},
  Year                     = {2011},

  Address                  = {Italy, Milano},
  Month                    = {August},
  Pages                    = {3165-3170},
  Publisher                = {IFAC},

  Abstract                 = {{T}his paper presents a vision tracking system, based on the robot motion and vision information. {F}or mobile robots, it is difficult to collect continuous vision information while they are in motion due to the unstable vision information. {T}o solve this problem, the proposed vision tracking system estimates the robot position relative to a target and rotates a camera towards the target based on the estimated position information. {T}his concept is derived from the human eye reflex mechanisms, known as the {V}estibulo-{O}cular {R}eflex ({VOR}) and the {O}pto-{K}inetic {R}eflex ({OKR}). {I}n the proposed vision tracking system, the {VOR} concept for compensating the head motion is realized by the feedforward control using the robot motion information from a 3-axis gyroscope and wheel encoders. {T}o realize the {OKR} concept, targeting errors are periodically compensated by using the vision feedback information. {A}n actuation module consists of pan/tilt motion motors and the camera which performs the pan and tilt functions to locate a target at the center of an image plane. {T}he proposed vision tracking system is integrated with a two-wheeled robot. {T}he performance of the proposed system is evaluated by extensive experiments. {T}he proposed system shows a significant improvement in tracking performance with small targeting error. {A}lso, the necessities of the position feedforward and vision feedback controls are verified by the experiments for the illumination change condition and the static tilting motion.},
  Doi                      = {10.3182/20110828-6-IT-1002.01781},
  File                     = {:sources/1781.pdf:PDF},
  Keywords                 = {Perception and sensing, Information and sensor fusion. Intelligent robotics},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Park2010,
  Title                    = {{H}igh performance vision tracking system for mobile robot using sensor data fusion with {K}alman filter},
  Author                   = {Park, Jaehong and Hwang, Wonsang and Kwon, Hyun-il and Kim, Jong-hyeon and Lee, Chang-hun and Anjum, M.L. and Kim, Kwang-soo and Cho, Dong-il},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
  Year                     = {2010},

  Address                  = {Taiwan, Taipei},
  Month                    = {October},
  Pages                    = {3778 - 3783},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper introduces a high performance vision tracking system for mobile robot using sensor data fusion. {F}or mobile robots, it is difficult to collect continuous vision information due to robot's motion. {T}o solve this problem, the proposed vision tracking system estimates the robot's position relative to a target and rotates the camera towards the target. {T}his concept is derived from the human eye reflex mechanism, known as the {V}estibulo-{O}cular {R}eflex ({VOR}), for compensating the head motion. {T}his concept for tracking the target results in much higher performance levels, when compared with the conventional method that rotates the camera using only vision information. {T}he proposed system do not require heavy computing loads to process image data and can track the target continuously even during vision occlusion. {T}he robot motion information is estimated using data from accelerometer, gyroscope, and encoders. {T}his multi-sensor data fusion is achieved using {K}alman filter. {T}he proposed vision tracking system is implemented on a two-wheeled robot. {T}he experimental results show that the proposed system achieves excellent tracking and recognition performance in various motion scenarios, including scenarios where camera is temporarily blocked from the target.},
  Doi                      = {10.1109/IROS.2010.5650367},
  File                     = {:sources/05650367.pdf:PDF},
  Keywords                 = {Kalman filters, accelerometers, cameras, gyroscopes, mobile robots, motion compensation, object tracking, robot vision, sensor fusion, wheels},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Book{Pratt2007,
  Title                    = {{D}igital {I}mage {P}rocessing},
  Author                   = {Pratt, William K.},
  Publisher                = {John Wiley and Sons},
  Year                     = {2007},
  Edition                  = {4},

  File                     = {:sources/Pratt - Digital Image Processing 4th Ed.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.15}
}

@InProceedings{Rui2009,
  Title                    = {{M}oving object tracking based on mobile robot vision},
  Author                   = {Rui, Lin and Zhijiang, Du and Lining, Sun},
  Booktitle                = {International Conference on Mechatronics and Automation},
  Year                     = {2009},

  Address                  = {China, Changchun},
  Month                    = {August},
  Pages                    = {3625 - 3630},
  Publisher                = {IEEE},

  Abstract                 = {{T}he paper describes a robotic application that tracks a moving object by utilizing a mobile robot with capacity of avoiding obstacles. {R}eal-time tracking algorithm is based on mobile robot vision using adaptive color matching and {K}alman filter. {A}daptive color matching is used to obtain motion vectors of moving object in the robot coordinate system. {I}t can adjust color matching threshold adaptively to reduce the influence of lighting variations in the scene. {K}alman filter is applied to design a view window. {V}iew window, which contains the next position of the moving object estimated by {K}alman {A}lter, is determined on image plane to reduce the image processing area. {C}olor matching threshold can adjust itself adaptively in view window. {A} virtual targets based algorithm is also presented for real-time obstacle avoidance in this paper. {E}xperimental results show that the tracking algorithm can adapt to lighting variations and has good tracking precision. {I}t can also avoid obstacles smoothly while tracking moving object. {F}urthermore, it can be implemented in real time.},
  Doi                      = {10.1109/ICMA.2009.5246022},
  File                     = {:sources/05246022.pdf:PDF},
  Keywords                 = {Force control, Fuzzy control, Image sequences, Laboratories, Mobile robots, Robot kinematics, Robot vision systems, Robotics and automation, Sun, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Sadeghi-Tehran2014,
  Title                    = {{A} real-time approach for autonomous detection and tracking of moving objects from {UAV}},
  Author                   = {Sadeghi-Tehran, Pouria and Clarke, Christopher and Angelov, Plamen},
  Booktitle                = {Symposium on Evolving and Autonomous Learning Systems},
  Year                     = {2014},

  Address                  = {USA, FL, Orlando},
  Pages                    = {43 - 49},
  Publisher                = {IEEE},

  Abstract                 = {{A} new approach to autonomously detect and track moving objects in a video captured by a moving camera from a {UAV} in real-time is proposed in this paper. {T}he introduced approach replaces the need for a human operator to perform video analytics by autonomously detecting moving objects and clustering them for tracking purposes. {T}he effectiveness of the introduced approach is tested on the footage taken from a real {UAV} and the evaluation results are demonstrated in this paper.},
  Doi                      = {10.1109/EALS.2014.7009502},
  File                     = {:sources/07009502.pdf:PDF},
  Keywords                 = {Cameras, Clustering algorithms, Detectors, Feature extraction, Optical imaging, Robustness, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Shi1994,
  Title                    = {{G}ood features to track},
  Author                   = {Shi, Jianbo and Tomasi, Carlo},
  Booktitle                = {Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {1994},

  Address                  = {USA, Washington, Seattle},
  Month                    = {June},
  Pages                    = {593 - 600},
  Publisher                = {IEEE},

  Abstract                 = {{N}o feature-based vision system can work unless good features can be identified and tracked from frame to frame. {A}lthough tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. {W}e propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. {T}hese methods are based on a new tracking algorithm that extends previous {N}ewton-{R}aphson style search methods to work under affine image transformations. {W}e test performance with several simulations and experiments},
  Doi                      = {10.1109/CVPR.1994.323794},
  File                     = {:sources/00323794.pdf:PDF},
  Keywords                 = {Feature extraction, Machine vision, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.28}
}

@InProceedings{Shibata2010,
  Title                    = {{I}mage-based visual tracking to fast moving target for active binocular robot},
  Author                   = {Masaaki Shibata and Hideki Eto and Masahide Ito},
  Booktitle                = {36th Annual Conference on IEEE Industrial Electronics Society (IECON)},
  Year                     = {2010},

  Address                  = {United States, Arizona, Glendale},
  Month                    = {November},
  Pages                    = {2727 - 2732},
  Publisher                = {IEEE},

  Abstract                 = {{T}he paper reports the control way of visual tracking to a fast moving target with using an active binocular robot. {T}he robot consists of a pair of active cameras and performs as a stereo vision system. {T}he motions of those cameras are controlled independently and achieve rigid visual tracking respectively. {T}he difficulty of visual tracking to a fast target is to suppress the tracking delay, affected by the high-speed of the target and the rapid changes of its motions. {A}lthough we have established an explicit theory for non-delayed visual tracking in our previous works, it is fact that the good tuning of the control parameters should realize its own high performance. {I}n this paper, we have examined the effective tunings of three types of the control parameters; the feedback gains for motion control and for visual servoing and the image capturing frequency. {I}n several conditions, the higher performance on the visual tracking to a fast moving target has been achieved. {T}he validity of the proposed method is confirmed in the many physical experimental results.},
  Doi                      = {10.1109/IECON.2010.5675119},
  File                     = {:sources/05675119.pdf:PDF},
  Keywords                 = {Cameras, Robot kinematics, Robot vision systems, Target tracking, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.03}
}

@Article{Smeulders2010,
  Title                    = {{V}isual {T}racking: {A}n {E}xperimental {S}urvey},
  Author                   = {Smeulders, Arnold W. M. and Chu, Dung M. and Cucchiara, Rita and Calderara, Simone and Dehghan, Afshin and Shah, Mubarak},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2010},

  Month                    = {November},
  Pages                    = {1442 - 1468},
  Volume                   = {36},

  Abstract                 = {{T}here is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. {O}bject tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. {A} good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. {H}owever, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. {I}n this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. {W}e selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. {W}e demonstrate that trackers can be evaluated objectively by survival curves, {K}aplan {M}eier statistics, and {G}rubs testing. {W}e find that in the evaluation practice the {F}-score is as effective as the object tracking accuracy ({OTA}) score. {T}he analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers.},
  Booktitle                = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Doi                      = {10.1109/TPAMI.2013.230},
  File                     = {:sources/06671560.pdf:PDF},
  Keywords                 = {Educational institutions, Object tracking, Radar tracking, Robustness, Target tracking, Videos},
  Owner                    = {Stanisław Maciąg},
  Publisher                = {IEEE},
  Timestamp                = {2015.05.20}
}

@Book{Szeliski2011,
  Title                    = {{C}omputer {V}ision - {A}lgorithms and {A}pplications},
  Author                   = {Szeliski, Richard},
  Publisher                = {Springer-Verlag London},
  Year                     = {2011},

  File                     = {:sources/Szeliski - Computer Vision Algorithms and Applications.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.08.04}
}

@InProceedings{Tinh2014,
  Title                    = {{V}isual control of integrated mobile robot-pan tilt-camera system for tracking a moving target},
  Author                   = {Tinh, N.V. and Cat, P.T. and Tuan, P.M. and Bui, T.T.Q.},
  Booktitle                = {IEEE International Conference on Robotics and Biomimetics},
  Year                     = {2014},

  Address                  = {Indonesia, Bali},
  Month                    = {December},
  Pages                    = {1566 - 1571},
  Publisher                = {IEEE},

  Abstract                 = {{I}n this paper, in order to track a moving target, we propose a new control law for an integrated mobile robot- pan tilt-camera system. {O}ur controller consists of two control loops, i.e., a kinematic and dynamic control loop, respectively. {T}he kinematic control loop performs three tasks, i.e., allowing an image feature of the target to converge to the center of the image plane asymptotically, designing a trajectory for the mobile robot, and allowing the mobile robot to track the desired position and direction. {I}n the dynamic control loop, the torques are determined; the actual angular velocities of the system, i.e., angular velocities of pan and tilt axis, angular velocities of right and left wheels, track the desired angular velocities which are the outputs of the kinematic controller of the kinematic control loop. {A}ccording to the {L}yapunov theory and {B}arbalat's theorem, the asymptotic stability of the whole system is proven. {S}imulation results achieved by using {M}atlab - {S}imulink are introduced.},
  Doi                      = {10.1109/ROBIO.2014.7090557},
  File                     = {:sources/07090557.pdf:PDF},
  Keywords                 = {Angular velocity, Cameras, Kinematics, Mobile robots, Robot vision systems, Target tracking, Wheels},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@TechReport{Tomasi1991,
  Title                    = {{D}etection and {T}racking of {P}oint {F}eatures},
  Author                   = {Tomasi, Carlo and Kanade, Takeo},
  Institution              = {School of Computer Science Carnegie Mellon University},
  Year                     = {1991},

  Address                  = {USA, Pennsylvania, Pittsburgh},
  Month                    = {April},
  Number                   = {CMU-CS-91-132},

  Abstract                 = {{T}he factorization method described in this series of reports requires an algorithm to track the motion of features in an image stream. {G}iven the small inter-frame displacement made possible by the factorization approach, the best tracking method turns out to be the one proposed by {L}ucas and {K}anade in 1981. {T}he method defines the measure of match between fixed-size feature windows in the past and current frame as the sum of squared intensity differences over the windows. {T}he displacement is then defined as the one that minimizes this sum. {F}or small motions, a linearization of the image intensities leads to a {N}ewton-{R}aphson style minimization. {I}n this report, after rederiving the method in a physically intuitive way, we answer the crucial question of how to choose the feature windows that are best suited for tracking. {O}ur selection criterion is based directly on the definition of the tracking algorithm, and expresses how well a feature can be tracked. {A}s a result, the criterion is optimal by construction. {W}e show by experiment that the performance of both the selection and the tracking algorithm are adequate for our factorization method, and we address the issue of how to detect occlusions. {I}n the conclusion, we point out specific open questions for future research.},
  File                     = {:sources/10.1.1.45.5770.pdf:PDF},
  Keywords                 = {Computer vision, Motion, Shape, Time-varying Imagery},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.02.04}
}

@Article{Torkaman2012,
  Title                    = {{A} {K}alman-{F}ilter-{B}ased {M}ethod for {R}eal-{T}ime {V}isual {T}racking of a {M}oving {O}bject {U}sing {P}an and {T}ilt {P}latform},
  Author                   = {Torkaman, B. and Farrokhi, M.},
  Journal                  = {International Journal of Scientific and Engineering Research},
  Year                     = {2012},

  Month                    = {August},
  Number                   = {8},
  Pages                    = {1-6},
  Volume                   = {3},

  Abstract                 = {{T}he problem of real time estimating position and orientation of a moving object is an important issue for vision-based control of pan and tilt. {T}his paper presents a vision-based navigation strategy for a pan and tilt platform and a mounted video camera as a visual sensor. {F}or detection of objects, a suitable image processing algorithm is used. {M}oreover, estimation of the object position is performed using the {K}alman filter as an estimator. {T}he proposed method is implemented experimentally to a laboratory-size pan and tilt platform. {E}xperimental results show good target tracking by the proposed method in real-time.},
  File                     = {:sources/researchpaper_A-KalmanFilterBased-Method-for-RealTime-Visual-Tracking-of-a-Moving-Object-Using-Pan-and-Tilt-Platform.pdf:PDF},
  Keywords                 = {Visual servoing, Vision-based navigation, Target tracking, Estimation, Pan and tilt platform, Kalman filter, Image processing},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.08.05}
}

@Book{Treiber2010,
  Title                    = {{A}n {I}ntroduction to {O}bject {R}ecognition - {S}elected {A}lgorithms for a {W}ide {V}ariety of {A}pplications},
  Author                   = {Treiber, Marco Alexander},
  Publisher                = {Springer-Verlag London},
  Year                     = {2010},
  Edition                  = {1},

  File                     = {:sources/An Introduction to Object Recognition.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.16}
}

@Book{Trucco1998,
  Title                    = {{I}ntroductory {T}echniques for 3-{D} {C}omputer {V}ision},
  Author                   = {Trucco, Emanuele and Verri, Alessandro},
  Publisher                = {Prentice Hall PTR},
  Year                     = {1998},

  Address                  = {Upper Saddle River, New Jersey, USA},

  File                     = {:sources/Introductory Techniques for 3-D Computer Vision - Emanuele Trucco, Alessandro Verri.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Wang2007,
  Title                    = {{C}amshift {G}uided {P}article {F}ilter for {V}isual {T}racking},
  Author                   = {Zhaowen Wang and Xiaokang Yang and Yi Xu and Songyu Yu},
  Booktitle                = {IEEE Workshop on Signal Processing Systems},
  Year                     = {2007},

  Address                  = {China, Shanghai},
  Month                    = {October},
  Pages                    = {301 - 306},
  Publisher                = {IEEE},

  Abstract                 = {{P}article filter and mean shift are two important methods for tracking object in video sequence, and they are extensively studied by researchers. {A}s their strength complements each other, some effort has been initiated in [1] to combine these two algorithms, on which the advantage of computational efficiency is focused. {I}n this paper, we extend this idea by exploring even more intrinsic relationship between mean shift and particle filter, and propose a new algorithm, {C}am{S}hift guided particle filter ({CAMSGPF}). {I}n {CAMSGPF}, two basic algorithms - {C}am{S}hift and particle filter - can work cooperatively and benefit from each other, so that the overall performance is improved and some redundancy in algorithms can be removed. {E}xperimental results show that the proposed method can track objects robustly in various environments, and is much faster than the existing methods.},
  Doi                      = {10.1109/SIPS.2007.4387562},
  File                     = {:sources/04387562.pdf:PDF},
  Keywords                 = {Computational efficiency, Electronic mail, Image communication, Information processing, Partial response channels, Particle filters, Particle tracking, Probability distribution, Robustness, Video sequences},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.11}
}

@TechReport{Welch1995,
  Title                    = {{A}n {I}ntroduction to the {K}alman {F}ilter},
  Author                   = {Welch, Greg and Bishop, Gary},
  Institution              = {University of North Carolina at Chapel Hill},
  Year                     = {1995},

  File                     = {:sources/kalman_intro.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.02.25}
}

@InProceedings{Xiang2009,
  Title                    = {{R}eal-{T}ime {F}ollow-{U}p {T}racking {F}ast {M}oving {O}bject with an {A}ctive {C}amera},
  Author                   = {Xiang, Guishan},
  Booktitle                = {2nd International Congress on Image and Signal Processing},
  Year                     = {2009},

  Address                  = {China, Tianjin},
  Month                    = {October},
  Pages                    = {1 - 4},
  Publisher                = {IEEE},

  Abstract                 = {{A} method for real-time follow-up tracking fast moving object with an active camera is proposed. {T}he mean shift algorithm shows an excellent performance on robust and fast object tracking; however, it is prone to fail when tracking a very fast moving object. {T}his paper addresses to solve the problem. {A}t first mean shift based on the color feature of the object is introduced. {T}he object maybe can't be tracked when the motion of the object or the motion of the active camera is very fast. {I}n order to overcome this difficulty, {K}alman filter is introduced to predict the location of the initial search window of mean shift. {I}n order to reduce the disturbance from the background and to reduce the computational consumption, a region of interest ({ROI}) is introduced. {T}o follow-up track the moving object smoothly, a closed loop control model based on speed regulation is applied to drive a {PTZ} camera to center the target. {T}he results of our experiments show that the active camera can follow-up track a fast moving object smoothly. {T}he system is computationally efficient and can run in real-time speed.},
  Doi                      = {10.1109/CISP.2009.5303457},
  File                     = {:sources/05303457.pdf:PDF},
  Keywords                 = {Cameras, Colored noise, Histograms, Mathematical model, Probability distribution, Real time systems, Robustness, Target tracking, Tracking loops, Video sequences},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Xiang2009a,
  Title                    = {{T}racking object under intense disturbance based on adaptive histogram with an active camera},
  Author                   = {Xiang, Guishan and Lin, Zhijie},
  Booktitle                = {International Conference on Mechatronics and Automation},
  Year                     = {2009},

  Address                  = {August},
  Pages                    = {1162 - 1166},
  Publisher                = {IEEE},

  Abstract                 = {{A} novel method based on adaptive histogram is proposed for follow-up tracking moving object under intense disturbance with an active camera. {T}he {M}ean {S}hift algorithm shows an excellent performance on robust and fast object tracking, however, it is prone to fail when facing intense disturbance from background. {T}his paper addresses to solve these problems. {A}t first how the number of dimensions and the number of bins of histogram affects the target representation is analyzed, then an adaptive selection strategy for dimensions number and bins number is proposed depending on the intensity of disturbance from background. {T}o follow-up tracking moving object with an active camera, a closed loop control model based on speed regulation is proposed to drive a {PTZ} camera to center the target. {T}he results of experiments show that the active camera can follow-up track moving target stably, even when encountering large area intense disturbance from background. {T}he algorithm is computationally efficient and can run in realtime speed.},
  Doi                      = {10.1109/ICMA.2009.5246399},
  File                     = {:sources/05246399.pdf:PDF},
  Keywords                 = {Image processing, Target tracking, Video signal processing, Adaptive histogram, Intense disturbance, Mean Shift, Speed regulation},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Article{Yilmaz2006,
  Title                    = {{O}bject {T}racking: {A} {S}urvey},
  Author                   = {Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
  Journal                  = {ACM Computing Surveys},
  Year                     = {2006},

  Month                    = {December},
  Number                   = {4},
  Pages                    = {1-45},
  Volume                   = {38},

  Abstract                 = {{T}he goal of this article is to review the state-of-the-art tracking methods, classify them into different categories, and identify new trends. {O}bject tracking, in general, is a challenging problem. {D}ifficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. {T}racking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. {T}ypically, assumptions are made to constrain the tracking problem in the context of a particular application. {I}n this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. {M}oreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
  Doi                      = {10.1145/1177352.1177355},
  File                     = {:sources/Yilmaz.pdf:PDF},
  Keywords                 = {Appearance models, contour evolution, feature selection, object detection, object representation, point tracking, shape tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.11}
}

@InProceedings{Yu2008,
  Title                    = {{D}etection and tracking of moving object with a mobile robot using laser scanner},
  Author                   = {Yu, Jin-Xia and Cai, Zi-xing and Duan, Zhuo-Hua},
  Booktitle                = {International Conference on Machine Learning and Cybernetics},
  Year                     = {2008},

  Address                  = {China, Kunming},
  Month                    = {July},
  Pages                    = {1947 - 1952},
  Publisher                = {IEEE},
  Volume                   = {4},

  Abstract                 = {{A}n autonomous approach for detection and tracking of moving object with a mobile robot using laser scanner is presented in this paper. {F}irstly, ranging data of environmental objects from laser scanner are clustered according to the dynamic clustering method of k-nearest neighbors. {T}hen, the movement parameter of clustering objects is computed by local grid map matching. {A}t the same time, the movement compensation of mobile robot is estimated. {A}fter obtaining the moving object, particle filter ({PF}) with the improved proposal distribution is adopted to track moving object so as to get the movement condition of the object. {A}t last, experiments with the mobile robot designed by us are implemented and the validity of this approach is verified.},
  Doi                      = {10.1109/ICMLC.2008.4620725},
  File                     = {:sources/04620725.pdf:PDF},
  Keywords                 = {Change detection algorithms, Cybernetics, Laser theory, Machine learning, Mobile robots, Motion detection, Object detection, Particle filters, Particle tracking, Proposals},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Zhang2014,
  Title                    = {{UAV} tracking moving target scene using on-board {ISAR} sensor},
  Author                   = {Zhang, Liren and Karam, Ahmed},
  Booktitle                = {10th International Conference on Innovations in Information Technology},
  Year                     = {2014},

  Address                  = {United Arab Emirates, Al Ain},
  Month                    = {November},
  Pages                    = {88 - 92},
  Publisher                = {IEEE},

  Abstract                 = {{C}ompressive sensing ({CS}) based {I}nverse {S}ynthetic {A}perture {R}adar ({ISAR}) imaging exploits the sparsity of the target scene to achieve high resolution and effective denoising with limited measurements. {T}his paper extends the {CS} based {ISAR} imaging to further include the continuity structure of the target scene within a {B}ayesian framework. {A} correlated prior is imposed to statistically encourage the continuity structures in both the cross-range and range domains of the target region and the {G}ibbs sampling strategy is used for {B}ayesian inference. {B}ecause the resulted method requires to recover the whole target scene at a time with heavy computational complexity, an approximate strategy is proposed to alleviate the computational burden. {E}xperimental results demonstrate that the proposed algorithm can achieve substantial improvements in terms of preserving the weak scatterers and removing noise over other reported {CS} based {ISAR} imaging algorithms.},
  Doi                      = {10.1109/INNOVATIONS.2014.6987568},
  File                     = {:sources/06987568.pdf:PDF},
  Keywords                 = {Approximation algorithms, Bayes methods, Compressed sensing, Imaging, Noise, Radar imaging, Signal processing algorithms},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Zhang2010,
  Title                    = {{A}n effective approach for active tracking with a {PTZ} camera},
  Author                   = {Zhang, Lei and Xu, Ke and Yu, Shiqi and Fu, Ruiqing Fu and Xu, Yangsheng},
  Booktitle                = {IEEE International Conference on Robotics and Biomimetics},
  Year                     = {2010},

  Address                  = {China, Tianjin},
  Month                    = {December},
  Pages                    = {1768 - 1773},
  Publisher                = {IEEE},

  Abstract                 = {{T}he concept of active tracking is presented to simulate the characteristics of human vision in intelligent visual surveillance. {T}he {P}an/{T}ilt/{Z}oom ({PTZ}) camera is generally used for active tracking. {I}n this paper, we present a novel and effective approach for active object tracking with a {PTZ} camera, and construct a near real-time system for indoor and outdoor scenes. {T}he tracking algorithm of our system is based on the feature matching, with the {PID} control to drive the camera. {T}he feature extracted from moving people is described as a region covariance matrix which combines the spatial and statistical properties of the targets (e.g. coordinates, color, and gradient). {R}esults from indoor and outdoor experiments demonstrate the effectiveness and accuracy of our approach.},
  Doi                      = {10.1109/ROBIO.2010.5723599},
  File                     = {:sources/05723599.pdf:PDF},
  Keywords                 = {Cameras, Covariance matrix, Image color analysis, Image sequences, Pixel, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Zhang2011,
  Title                    = {{A} real time object tracking approach for mobile robot visual servo control},
  Author                   = {Zhaoxiang Zhang and Ruhan Sa and Yunhong Wang},
  Booktitle                = {First Asian Conference on Pattern Recognition (ACPR)},
  Year                     = {2011},

  Address                  = {China, Beijing},
  Month                    = {November},
  Pages                    = {500 - 504},
  Publisher                = {IEEE},

  Abstract                 = {{A} key problem of {I}mage {B}ased {V}isual {S}ervo ({IBVS}) {S}ystem is to track objects in image sequences. {T}hus, the tracking algorithm plays an important role in improving the efficiency of {IBVS} systems. {I}n this paper, a novel tracking algorithm called {M}odified {C}am{S}hift {G}uided {P}article {F}ilter ({MCAMSGPF}) is proposed, which interpolated {S}peeded-{U}p {R}obust {F}eatures ({SURF}) into the framework of conventional {C}am{S}hift {G}uided {P}article {F}ilter ({CAMSGPF}) tracking method. {T}his new algorithm outperforms conventional {CAMSGPF} and other baseline trackers with respect to tracking robustness in the clutter background of similar colors and occlusions. {W}e also proposed a new system model to implement and test the new algorithm in a real time moving {IBVS} system, which is applied in a mobile robot with an on-board camera.},
  Doi                      = {10.1109/ACPR.2011.6166627},
  File                     = {:sources/06166627.pdf:PDF},
  Keywords                 = {Cameras, Clutter, Image color analysis, Robots, Robustness, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.03}
}

@InProceedings{Zhao2013,
  Title                    = {{V}ision based ground target tracking for rotor {UAV}},
  Author                   = {Zhao, Xuqiang and Fei, Qing and Geng, Qingbo},
  Booktitle                = {10th IEEE International Conference on Control and Automation (ICCA)},
  Year                     = {2013},

  Address                  = {China, Hangzhou},
  Month                    = {June},
  Pages                    = {1907 - 1911},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper studies an efficient ground target tracking algorithm for rotor {U}nmanned {A}erial {V}ehicle ({UAV}) to overcome the contradiction among the target tracking rapidity, precision and robustness for aerial vehicle. {F}irstly, {S}cale {I}nvariant {F}eature {T}ransform ({SIFT}) algorithm, which has a better robust performance during rotation, scaling and changes of illumination, is utilized to extract and match the feature points in order to realize target recognition and positioning. {S}econdly, using top-down tracking method, {K}alman filter is combined to estimate the target position in the next frame and search target in the predicted area, it can avoid blind matching, improve tracking rapidity and reduce the ratio of losing target. {F}inally, an experimental platform of rotor {UAV} visual tracking is set up and the ground target tracking algorithm is tested. {T}he experiment results show that the algorithm can achieve ground target tracking effectively and has good real-time performance and robustness.},
  Doi                      = {10.1109/ICCA.2013.6565085},
  File                     = {:sources/06565085.pdf:PDF},
  Keywords                 = {Equations, Kalman filters, Mathematical model, Rotors, Target recognition, Target tracking, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Zivkovic2004,
  Title                    = {{A}n {EM}-like algorithm for color-histogram-based object tracking},
  Author                   = {Zivkovic, Zoran and Kröse, Ben},
  Booktitle                = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2004},
  Month                    = {June},
  Pages                    = {I-798 - I-803},
  Publisher                = {IEEE},
  Volume                   = {1},

  Abstract                 = {{T}he iterative procedure called 'mean-shift' is a simple robust method for finding the position of a local mode (local maximum) of a kernel-based estimate of a density function. {A} new robust algorithm is given here that presents a natural extension of the 'mean-shift' procedure. {T}he new algorithm simultaneously estimates the position of the local mode and the covariance matrix that describes the approximate shape of the local mode. {W}e apply the new method to develop a new 5-degrees of freedom ({DOF}) color histogram based non-rigid object tracking algorithm.},
  Doi                      = {10.1109/CVPR.2004.1315113},
  File                     = {:sources/01315113.pdf:PDF},
  Keywords                 = {Covariance matrix, Density functional theory, Histograms, Intelligent systems, Iterative algorithms, Iterative methods, Maximum likelihood estimation, Probability density function, Robustness, Shape},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.03.02}
}

@comment{jabref-meta: fileDirectory:.;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Analiza literaturowa\;0\;Alenya2014\;Bahn2011\;Campoy2
009\;Chen2012\;Deng2010\;Deng2010a\;Fernandez-Caballero2010\;Germa2010
\;Han2013\;Hashimoto2007\;Hu2015\;Hwang2010\;Kim2011\;Kim2012\;Lee2011
\;Li2007\;Liem2008\;Liu2014\;Markovic2014\;Mohamed2014\;Murakami2012\;
Olivares-Mendez2009\;Park2010\;Park2011\;Rui2009\;Sadeghi-Tehran2014\;
Tinh2014\;Yu2008\;Zhao2013\;;
2 ExplicitGroup:Systemy sterowania\;0\;Bahn2011\;Campoy2009\;Chen2012\
;Deng2010\;Deng2010a\;Germa2010\;Han2013\;Hashimoto2007\;Hwang2010\;Ki
m2011\;Kim2012\;Lee2011\;Li2007\;Liem2008\;Liu2014\;Markovic2014\;Mura
kami2012\;Olivares-Mendez2009\;Park2010\;Park2011\;Rui2009\;Tinh2014\;
;
3 ExplicitGroup:Monografie\;0\;Challa2011\;Dorf2011\;Golnaraghi2010\;H
aug2012\;Haykin2001\;Maggio2011\;;
3 ExplicitGroup:Filtr cząsteczkowy\;0\;Arulampalam2002\;Gustafsson2010
\;;
2 ExplicitGroup:Przetwarzanie obrazów\;0\;;
3 ExplicitGroup:Monografie\;0\;Forsyth2012\;Gonzalez\;Jaehne2005\;Magg
io2011\;Pratt2007\;Szeliski2011\;Treiber2010\;;
3 ExplicitGroup:Śledzenie obiektów\;0\;Maggio2011\;Smeulders2010\;Yilm
az2006\;;
4 ExplicitGroup:Filtr Kalmana\;0\;Chen2012a\;Haug2012\;Haykin2001\;Wel
ch1995\;;
3 ExplicitGroup:Ekstrakcja i deskrypcja cech\;0\;Lowe2004\;Szeliski201
1\;Treiber2010\;;
3 ExplicitGroup:Przepływ optyczny\;0\;Baker2004\;Baker2011\;Bouguet200
0\;Jaehne2005\;Karasulu2013\;Lucas1981\;Mohamed2014a\;Shi1994\;Szelisk
i2011\;Tomasi1991\;;
3 ExplicitGroup:Mean-Shift\;0\;Bradski1998\;Cheng1995\;Comaniciu1999\;
Comaniciu2000\;Comaniciu2002\;Comaniciu2003\;Fukunaga1975\;Maggio2011\
;Wang2007\;Zivkovic2004\;;
4 ExplicitGroup:CAMSHIFT\;0\;Bradski1998\;Wang2007\;;
2 ExplicitGroup:Istniejące rozwiązania\;0\;Bahn2011\;Campoy2009\;Chen2
012\;Fernandez-Caballero2010\;Kim2011\;Kim2012\;Lang2010\;Li2007\;Liem
2008\;Markovic2014\;Mohamed2014\;Motokucho2014\;Olivares-Mendez2009\;P
ark2011\;Rui2009\;Sadeghi-Tehran2014\;Shibata2010\;Zhang2011\;Zhao2013
\;;
3 ExplicitGroup:Mean-Shift\;0\;Li2007\;Li2009\;Liem2008\;Zhang2011\;;
3 ExplicitGroup:Śledzenie i detekcja\;0\;Bahn2011\;Fernandez-Caballero
2010\;Lang2010\;Park2011\;;
3 ExplicitGroup:Przepływ optyczny\;0\;Campoy2009\;Fernandez-Caballero2
010\;Liem2008\;Markovic2014\;Mohamed2014\;Motokucho2014\;Olivares-Mend
ez2009\;Sadeghi-Tehran2014\;;
4 ExplicitGroup:Algorytm Lucasa-Kanade\;0\;Campoy2009\;Fernandez-Cabal
lero2010\;Liem2008\;Markovic2014\;Olivares-Mendez2009\;Sadeghi-Tehran2
014\;;
5 ExplicitGroup:Klasyczny\;0\;Fernandez-Caballero2010\;Olivares-Mendez
2009\;;
5 ExplicitGroup:ICA\;0\;Campoy2009\;;
5 ExplicitGroup:KLT\;0\;Liem2008\;Sadeghi-Tehran2014\;;
5 ExplicitGroup:Piramidowy\;0\;Markovic2014\;;
4 ExplicitGroup:Inne\;0\;Mohamed2014\;Motokucho2014\;;
3 ExplicitGroup:Filtr Bayesa\;0\;Campoy2009\;Chen2012\;Kim2012\;Rui200
9\;Shibata2010\;Zhang2011\;Zhao2013\;;
4 ExplicitGroup:FIltr Kalmana\;0\;Campoy2009\;Chen2012\;Kim2012\;Li200
9\;Rui2009\;Shibata2010\;Zhao2013\;;
4 ExplicitGroup:Filtr cząsteczkowy\;0\;Chou2012\;Zhang2011\;;
3 ExplicitGroup:Wybrane\;0\;Kim2012\;Lang2010\;Liem2008\;Markovic2014\
;Olivares-Mendez2009\;Zhang2011\;;
1 ExplicitGroup:Pan-tilt\;0\;;
2 ExplicitGroup:Kinematyka\;0\;AlHaj2010\;Deng2010\;Doyle2014\;Kim2012
\;Murray1994\;;
}

