% This file was created with JabRef 2.10.
% Encoding: UTF8


@InProceedings{AlHaj2010,
  Title                    = {{R}eactive {O}bject {T}racking with a {S}ingle {PTZ} {C}amera},
  Author                   = {Al Haj, Murad and Bagdanov, Andrew D. and Gonzalez, Jordi and Roca, F. Xavier},
  Booktitle                = {20th International Conference on Pattern Recognition (ICPR)},
  Year                     = {2010},

  Address                  = {Turkey, Istanbul},
  Month                    = {August},
  Pages                    = {1690 - 1693},
  Publisher                = {IEEE},

  Abstract                 = {{I}n this paper we describe a novel approach to reactive tracking of moving targets with a pan-tilt-zoom camera. {T}he approach uses an extended {K}alman filter to jointly track the object position in the real world, its velocity in 3{D} and the camera intrinsics, in addition to the rate of change of these parameters. {T}he filter outputs are used as inputs to {PID} controllers which continuously adjust the camera motion in order to reactively track the object at a constant image velocity while simultaneously maintaining a desirable target scale in the image plane. {W}e provide experimental results on simulated and real tracking sequences to show how our tracker is able to accurately estimate both 3{D} object position and camera intrinsics with very high precision over a wide range of focal lengths.},
  Doi                      = {10.1109/ICPR.2010.418},
  File                     = {:../sources/05595809.pdf:PDF},
  Keywords                 = {Cameras, Estimation, Kalman filters, Position measurement, Target tracking, Three dimensional displays},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.06.07}
}

@Article{Alenya2014,
  Title                    = {{T}o{F} cameras for active vision in robotics},
  Author                   = {G. Alenya and S. Foix and C. Torras},
  Journal                  = {Sensors and Actuators A: Physical},
  Year                     = {2014},
  Pages                    = {10 - 22},
  Volume                   = {218},

  Abstract                 = {{A}bstract {T}o{F} cameras are now a mature technology that is widely being adopted to provide sensory input to robotic applications. {D}epending on the nature of the objects to be perceived and the viewing distance, we distinguish two groups of applications: those requiring to capture the whole scene and those centered on an object. {I}t will be demonstrated that it is in this last group of applications, in which the robot has to locate and possibly manipulate an object, where the distinctive characteristics of {T}o{F} cameras can be better exploited. {A}fter presenting the physical sensor features and the calibration requirements of such cameras, we review some representative works highlighting for each one which of the distinctive {T}o{F} characteristics have been more essential. {E}ven if at low resolution, the acquisition of 3{D} images at frame-rate is one of the most important features, as it enables quick background/foreground segmentation. {A} common use is in combination with classical color cameras. {W}e present three developed applications, using a mobile robot and a robotic arm, to exemplify with real images some of the stated advantages.},
  Doi                      = {10.1016/j.sna.2014.07.014},
  File                     = {:../sources/1-s2.0-S0924424714003458-main.pdf:PDF},
  ISSN                     = {0924-4247},
  Keywords                 = {Time-of-Flight cameras, 3D perception for manipulation, Depth calibration, Outdoor imaging, Complex-shape objects},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0924424714003458}
}

@Book{Austin2010,
  Title                    = {{U}nmanned {A}ircraft {S}ystems - {UAVS} design, development and deployment},
  Author                   = {Austin, Reg},
  Publisher                = {John Wiley and Sons},
  Year                     = {2010},

  File                     = {:../sources/Unmanned Air Systems UAV Design, Development and Deployment.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Bahn2011,
  Title                    = {{A} motion-information-based vision-tracking system with a stereo camera on mobile robots},
  Author                   = {Bahn, Wook and Park, Jaehong and Lee, Chang-hun and Kim, Kwang-soo and Lee, Teajae and Shaikh, M.M. and Kim, Kwang-soo and Cho, Dong-Il},
  Booktitle                = {IEEE Conference on Robotics, Automation and Mechatronics},
  Year                     = {2011},

  Address                  = {China, Qingdao},
  Month                    = {September},
  Pages                    = {252 - 257},
  Publisher                = {EEE},

  Abstract                 = {{T}his paper presents a vision-tracking system for a mobile robot, using robot motion and stereo vision data. {T}he mobile robot has an actuator module which pans and tilts an integrated stereo camera. {T}he proposed system controls the actuator to maintain the line of sight of the stereo camera towards a stationary target by using the robot motion data. {T}he robot motion data are obtained from a gyroscope and encoders of the mobile robot. {T}he stereo vision data from the camera is used to compensate for errors in the motion measurements, and to prevent a long term error accumulation. {T}his vision-based compensation is used only when the robot stops or moves slowly, because the long vision processing times can cause the loop time to overrun. {T}he proposed system is experimentally evaluated while the robot moves on a trajectory.},
  Doi                      = {10.1109/RAMECH.2011.6070491},
  File                     = {:../sources/06070491.pdf:PDF},
  Keywords                 = {Cameras, Mobile robots, Robot kinematics, Robot vision systems, Stereo vision, Vectors},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Baker2004,
  Title                    = {{L}ucas-{K}anade 20 {Y}ears {O}n: {A} {U}nifying {F}ramework},
  Author                   = {Baker, Simon and Matthews, Iain},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2004},

  Month                    = {Fabruary},
  Number                   = {3},
  Pages                    = {221-255},
  Volume                   = {56},

  Abstract                 = {{S}ince the {L}ucas-{K}anade algorithm was proposed in 1981 image alignment has become one of the most widely used techniques in computer vision. {A}pplications range from optical flow and tracking to layered motion, mosaic construction, and face coding. {N}umerous algorithms have been proposed and a wide variety of extensions have been made to the original formulation. {W}e present an overview of image alignment, describing most of the algorithms and their extensions in a consistent framework. {W}e concentrate on the inverse compositional algorithm, an efficient algorithm that we recently proposed. {W}e examine which of the extensions to {L}ucas-{K}anade can be used with the inverse compositional algorithm without any significant loss of efficiency, and which cannot. {I}n this paper, {P}art 1 in a series of papers, we cover the quantity approximated, the warp update rule, and the gradient descent approximation. {I}n future papers, we will cover the choice of the error function, how to allow linear appearance variation, and how to impose priors on the parameters.},
  Doi                      = {10.1023/B:VISI.0000011205.11775.fd},
  File                     = {:../sources/Baker&Matthews.pdf:PDF},
  Keywords                 = {image alignment, Lucas-Kanade, a unifying framework, additive vs. compositional algorithms, forwards vs. inverse algorithms, the inverse compositional algorithm, efficiency, steepest descent, Gauss-Newton, Newton, Levenberg-Marquardt},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.08}
}

@Article{Baker2011,
  Title                    = {{A} {D}atabase and {E}valuation {M}ethodology for {O}ptical {F}low},
  Author                   = {Baker, Simon and Scharstein, Daniel and Lewis, J. P. and Roth, Stefan and Black, Michael J. and Szeliski, Richard},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {1 - 31},
  Volume                   = {92},

  Abstract                 = {{T}he quantitative evaluation of optical flow algorithms by {B}arron et al. (1994) led to significant advances in performance. {T}he challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. {I}nstead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. {W}e propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. {T}o that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. {I}n addition to the average angular error used by {B}arron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. {I}n {O}ctober 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. {W}e also made the data freely available on the web at http://​vision.​middlebury.​edu/​flow/​. {S}ubsequently a number of researchers have uploaded their results to our website and published papers using the data. {A} significant improvement in performance has already been achieved. {I}n this paper we analyze the results obtained to date and draw a large number of conclusions from them.},
  Doi                      = {10.1007/s11263-010-0390-2},
  File                     = {:../sources/art%3A10.1007%2Fs11263-010-0390-2.pdf:PDF},
  Keywords                 = {Optical flow, Survey, Algorithms, Database, Benchmarks, Evaluation, Metrics},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.14}
}

@InProceedings{Benezeth2008,
  Title                    = {{R}eview and evaluation of commonly-implemented background subtraction algorithms},
  Author                   = {Benezeth, Y. and Jodoin, P.-M. and Emile, B. and Laurent, H. and Rosenberger, C.},
  Booktitle                = {19th International Conference on Pattern Recognition},
  Year                     = {2008},

  Address                  = {USA, FL, Tampa},
  Month                    = {December},
  Pages                    = {1 - 4},
  Publisher                = {IEEE},

  Abstract                 = {{L}ocating moving objects in a video sequence is the first step of many computer vision applications. {A}mong the various motion-detection techniques, background subtraction methods are commonly implemented, especially for applications relying on a fixed camera. {S}ince the basic inter-frame difference with global threshold is often a too simplistic method, more elaborate (and often probabilistic) methods have been proposed. {T}hese methods often aim at making the detection process more robust to noise, background motion and camera jitter. {I}n this paper, we present commonly-implemented background subtraction algorithms and we evaluate them quantitatively. {I}n order to gauge performances of each method, tests are performed on a wide range of real, synthetic and semi-synthetic video sequences representing different challenges.},
  Doi                      = {10.1109/ICPR.2008.4760998},
  File                     = {:../sources/04760998.pdf:PDF},
  Keywords                 = {Application software, Background noise, Cameras, Computer vision, Jitter, Motion detection, Noise robustness, Performance evaluation, Testing, Video sequences},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Buttazzo1996,
  Title                    = {{R}eal-time issues in advanced robotics applications},
  Author                   = {Buttazzo, G.},
  Booktitle                = {Proceedings of the Eighth Euromicro Workshop on Real-Time Systems},
  Year                     = {1996},

  Address                  = {Italy, L'Aquila},
  Pages                    = {133 - 138},
  Publisher                = {IEEE},

  Abstract                 = {{T}he aim of this paper is to discuss some important real-time issues involved in the design of complex robotic applications. {I}n particular the problem of evaluating why and when a robotics application needs real-time computing is discussed first. {T}he second issue concerns the definition of time constraints for each task of the robot. {W}e show how time constraints, such as periods and deadlines, can be derived from the application, even though they are not explicitly specified in the requirements. {A}s a third issue, we describe a general hierarchical control architecture, which can be built on top of a real-time system, to greatly simplify the development of complex robotics applications having real-time requirements},
  Doi                      = {10.1109/EMWRTS.1996.557843},
  File                     = {:../sources/00557843.pdf:PDF},
  Keywords                 = {Actuators, Computer applications, Mechanical sensors, Real time systems, Robot sensing systems, Robot vision systems, Sensor phenomena and characterization, Sensor systems, Thermal conductivity, Time factors},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.10.30}
}

@Article{Campoy2009,
  Title                    = {{C}omputer {V}ision {O}nboard {UAV}s for {C}ivilian {T}asks},
  Author                   = {Campoy, Pascual and Correa, Juan F. and Mondragón, Ivan and Martínez, Carol and Olivares, Miguel and Mejías, Luis and Artieda, Jorge},
  Journal                  = {Journal of Intelligent and Robotic Systems},
  Year                     = {2009},

  Month                    = {March},
  Note                     = {Publisher: Springer Netherlands},
  Pages                    = {105-135},
  Volume                   = {54},

  Abstract                 = {{C}omputer vision is much more than a technique to sense and recover environmental information from an {UAV}. {I}t should play a main role regarding {UAV}s’ functionality because of the big amount of information that can be extracted, its possible uses and applications, and its natural connection to human driven tasks, taking into account that vision is our main interface to world understanding. {O}ur current research’s focus lays on the development of techniques that allow {UAV}s to maneuver in spaces using visual information as their main input source. {T}his task involves the creation of techniques that allow an {UAV} to maneuver towards features of interest whenever a {GPS} signal is not reliable or sufficient, e.g. when signal dropouts occur (which usually happens in urban areas, when flying through terrestrial urban canyons or when operating on remote planetary bodies), or when tracking or inspecting visual targets—including moving ones—without knowing their exact {UMT} coordinates. {T}his paper also investigates visual servoing control techniques that use velocity and position of suitable image features to compute the references for flight control. {T}his paper aims to give a global view of the main aspects related to the research field of computer vision for {UAV}s, clustered in four main active research lines: visual servoing and control, stereo-based visual navigation, image processing algorithms for detection and tracking, and visual {SLAM}. {F}inally, the results of applying these techniques in several applications are presented and discussed: this study will encompass power line inspection, mobile target tracking, stereo distance estimation, mapping and positioning.},
  Doi                      = {10.1007/s10846-008-9256-z},
  File                     = {:../sources/art%3A10.1007%2Fs10846-008-9256-z.pdf:PDF},
  Keywords                 = {UAV, Visual servoing, Image processing, Feature detection, Tracking, SLAM},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Chen2009,
  Title                    = {{A} {S}urvey of {A}utonomous {C}ontrol for {UAV}},
  Author                   = {Chen, Hai and Wang, Xin-Min and Li, Yan},
  Booktitle                = {International Conference on Artificial Intelligence and Computational Intelligence},
  Year                     = {2009},

  Address                  = {China, Shanghai},
  Month                    = {November},
  Pages                    = {267 - 271},
  Publisher                = {IEEE},
  Volume                   = {2},

  Abstract                 = {{T}his paper presents the autonomous control for {UAV}. {T}he autonomous control concept and autonomous control level ({ACL}) metrics that can measure autonomy of {UAV}s are introduced. {C}ompared with manned aircraft control task in battlefield, the functions of autonomous {UAV} system are organized. {A}ccording to the laws of increasing precision with decreasing intelligent ({IPDI}), the architecture of autonomous control for {UAV} is given. {T}he architecture can be divided into three levels: execution level, coordination level and organization level. {T}he constraint conditions and realizations of coordination level and organization level are studied comprehensively. {T}he key hardware and software technologies for multi-tasks are modularized depending on the requirements of mission. {T}he software technologies are distributed to stages of flying particularly.},
  Doi                      = {10.1109/AICI.2009.147},
  File                     = {:../sources/05375937.pdf:PDF},
  Keywords                 = {Artificial intelligence, Automatic control, Automation, Communication system control, Computer architecture, Control systems, Educational institutions, Force control, Telecommunication control, Unmanned aerial vehicles},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Chen2012,
  Title                    = {{A}daptive visual servo control of {UAV} {G}round-{T}arget-{A}utonomous-{T}racking {S}ystem},
  Author                   = {Chen, Longsheng and Jiang, Yang and Wang, Changkug},
  Booktitle                = {10th World Congress on Intelligent Control and Automation},
  Year                     = {2012},

  Address                  = {China, Beijing},
  Month                    = {July},
  Pages                    = {133 - 137},
  Publisher                = {IEEE},

  Abstract                 = {{A} novel adaptive servo control method is proposed for {UAV} {GTATS}({G}round-{T}arget-{A}utonomous-{T}racking {S}ystem),which consists of basic control law for {UAV} and visual tracking controller for {GTATS}. {T}he adaptive servo control method only depends on target information in the image plane and {K}alman filtering technology. {B}ased on this proposed method, a dynamic motion target can be tracked without target's 3{D} velocity. {S}ynchronously, in order to estimate the optimal system state and target image velocity which is used later by the visual tracking controller, a self-tuning {K}alman filter is adopted to estimate interesting parameters on-line in real-time. {F}urther, {B}ecause the visual tracking controller is working entirely in image space, the dynamic characteristics of the image signal are analyzed and a kinematics model is developed for the target in the image plane by the geometrical relations among the {UAV}, the target and the camera. {F}inally, {T}he performance of the controller is demonstrated by theoretical stability analysis.},
  Doi                      = {10.1109/WCICA.2012.6357854},
  File                     = {:../sources/06357854.pdf:PDF},
  Keywords                 = {Kalman filters, Adaptive control, Autonomous aerial vehicles, Geometry, Parameter estimation, Self-adjusting systems, Stability, State estimation, Target tracking, Vehicle dynamics, Velocity control, Visual servoing},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Cheung2004,
  Title                    = {{R}obust techniques for background subtraction in urban traffic video},
  Author                   = {Cheung, Sen-ching S. and Kamath, Chandrika},
  Booktitle                = {Proceedings of the SPIE},
  Year                     = {2004},
  Pages                    = {881-892},
  Volume                   = {5308},

  Abstract                 = {{I}dentifying moving objects from a video sequence is a fundamental and critical task in many computer-vision applications. {A} common approach is to perform background subtraction, which identifies moving objects from the portion of a video frame that differs significantly from a background model. {T}here are many challenges in developing a good background subtraction algorithm. {F}irst, it must be robust against changes in illumination. {S}econd, it should avoid detecting non-stationary background objects such as swinging leaves, rain, snow, and shadow cast by moving objects. {F}inally, its internal background model should react quickly to changes in background such as starting and stopping of vehicles. {I}n this paper, we compare various background subtraction algorithms for detecting moving vehicles and pedestrians in urban traffic video sequences. {W}e consider approaches varying from simple techniques such as frame differencing and adaptive median filtering, to more sophisticated probabilistic modeling techniques. {W}hile complicated techniques often produce superior performance, our experiments show that simple techniques such as adaptive median filtering can produce good results with much lower computational complexity.},
  Doi                      = {10.1117/12.526886},
  File                     = {:../sources/10.1.1.214.6703.pdf:PDF},
  Keywords                 = {Background subtraction, urban traffic video},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Deng2010,
  Title                    = {{A} motion controller for a pan-tilt camera on an autonomous helicopter},
  Author                   = {Deng, Haibo and Zhao, Xiaoguang and Hou, Zengguang},
  Booktitle                = {11th International Conference on Control Automation Robotics \& Vision (ICARCV)},
  Year                     = {2010},

  Address                  = {China, Singapore},
  Month                    = {December},
  Pages                    = {585 - 590},
  Publisher                = {IEEE},

  Abstract                 = {{I}n this paper, a motion controller for a pan-tilt camera mounted on an autonomous helicopter is presented. {T}he motion planner is designed according to the kinematics of the pan-tilt camera. {H}owever, the solution of the inverse kinematics of the pan-tilt camera is not unique. {T}o deal with this situation, a decision maker is designed. {T}he decision maker makes use of a cost function to decide which solution should be chose. {A}nd the error signal is defined as the difference between pan-tilt joint angles and the desired joint angles. {T}he dynamics of the error system is derived, and proportional controllers are designed according to the error dynamics to control the pan and tilt joints respectively. {T}his system is validated in a virtual reality environment, and the simulation results show that this system can track the target successfully.},
  Doi                      = {10.1109/ICARCV.2010.5707378},
  File                     = {:../sources/05707378.pdf:PDF},
  Keywords                 = {Angular velocity, Cameras, Helicopters, Joints, Target tracking, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.06.13}
}

@InProceedings{Deng2010a,
  Title                    = {{T}racking ground targets using an autonomous helicopter with a vision system},
  Author                   = {Deng, Haibo and Zhao, Xiaoguang and Hou, Zengguang},
  Booktitle                = {3rd International Congress on Image and Signal Processing},
  Year                     = {2010},

  Address                  = {China, Yantai},
  Month                    = {October},
  Pages                    = {1704 - 1708},
  Publisher                = {IEEE},
  Volume                   = {4},

  Abstract                 = {{A}n autonomous helicopter with a vision system onboard to track ground targets can accomplish very important tasks like inspection and surveillance. {T}he autonomous helicopter target tracking system is a interdisciplinary complex system. {A} typical autonomous helicopter target tracking system using vision sensors usually includes five major component: a navigation system, a flight controller, a active vision system, a target state estimator and a helicopter guidance law. {I}n this paper, we focus on target state estimator and helicopter guidance law. {I}t is assumed that the target moves on a flat ground, so the relative position between the target and the helicopter can be measured by a monocular vision system. {K}alman filter is used to estimate the state of the target based on the vision measurements. {A} new guidance law inspired by velocity pursuit guidance and car-following control is proposed to guide the helicopter to follow the moving target. {T}he tracking system is tested in a virtual reality environment and the results shows that the performance of the system is satisfied.},
  Doi                      = {10.1109/CISP.2010.5647759},
  File                     = {:../sources/05647759.pdf:PDF},
  Keywords                 = {Aircraft navigation, Estimation, Helicopters, Machine vision, Pixel, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Dinh2009,
  Title                    = {{R}eal time tracking using an active pan-tilt-zoom network camera},
  Author                   = {Dinh, T. and Qian Yu and Medioni, G.},
  Booktitle                = {IEEE/RSJ International Conference on Robots and Systems},
  Year                     = {2009},

  Address                  = {USA, MO, St. Louis},
  Month                    = {October},
  Pages                    = {3786 - 3793},
  Publisher                = {IEEE},

  Abstract                 = {{W}e present here a real time active vision system on a {PTZ} network camera to track an object of interest. {W}e address two critical issues in this paper. {O}ne is the control of the camera through network communication to follow a selected object. {T}he other is to track an arbitrary type of object in real time under conditions of pose, viewpoint and illumination changes. {W}e analyze the difficulties in the control through the network and propose a practical solution for tracking using a {PTZ} network camera. {M}oreover, we propose a robust real time tracking approach, which enhances the effectiveness by using complementary features under a two-stage particle filtering framework and a multi-scale mechanism. {T}o improve time performance, the tracking algorithm is implemented as a multi-threaded process in {O}pen{MP}. {C}omparative experiments with state-of-the-art methods demonstrate the efficiency and robustness of our system in various applications such as pedestrian tracking, face tracking, and vehicle tracking.},
  Doi                      = {10.1109/IROS.2009.5353915},
  File                     = {:../sources/05353915.pdf:PDF},
  Keywords                 = {Automatic control, Cameras, Communication system control, Humans, Intelligent robots, Lighting, Real time systems, Robot vision systems, Robustness, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Article{Doyle2014,
  Title                    = {{O}ptical flow background estimation for real-time pan/tilt camera object tracking},
  Author                   = {Daniel D. Doyle and Alan L. Jennings and Jonathan T. Black},
  Journal                  = {Measurement},
  Year                     = {2014},
  Pages                    = {195 - 207},
  Volume                   = {48},

  Abstract                 = {{A}s {C}omputer {V}ision ({CV}) techniques develop, pan/tilt camera systems are able to enhance data capture capabilities over static camera systems. {I}n order for these systems to be effective for metrology purposes, they will need to respond to the test article in real-time with a minimum of additional uncertainty. {A} methodology is presented here for obtaining high-resolution, high frame-rate images, of objects traveling at speeds >= 1.2 m/s at 1 m from the camera by tracking the moving texture of an object. {S}trong corners are determined and used as flow points using implementations on a graphic processing unit ({GPU}), resulting in significant speed-up over central processing units ({CPU}). {B}ased on directed pan/tilt motion, a pixel-to-pixel relationship is used to estimate whether optical flow points fit background motion, dynamic motion or noise. {T}o smooth variation, a two-dimensional position and velocity vector is used with a {K}alman filter to predict the next required position of the camera so the object stays centered in the image. {H}igh resolution images can be stored by a parallel process resulting in a high frame rate procession of images for post-processing. {T}he results provide real-time tracking on a portable system using a pan/tilt unit for generic moving targets where no training is required and camera motion is observed from high accuracy encoders opposed to image correlation.},
  Doi                      = {10.1016/j.measurement.2013.10.025},
  File                     = {:../sources/1-s2.0-S0263224113005241-main.pdf:PDF},
  ISSN                     = {0263-2241},
  Keywords                 = {Pan/tilt camera, Optical flow, Background subtraction, Object tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0263224113005241}
}

@Article{Fernandez-Caballero2010,
  Title                    = {{O}ptical flow or image subtraction in human detection from infrared camera on mobile robot },
  Author                   = {Fernandez-Caballero, Antonio and Castillo, Jose Carlos and Martinez-Cantos, Javier and Martinez-Tomas, Javier},
  Journal                  = {Robotics and Autonomous Systems},
  Year                     = {2010},
  Number                   = {12},
  Pages                    = {1273 - 1281},
  Volume                   = {58},

  Abstract                 = {{P}erceiving the environment is crucial in any application related to mobile robotics research. {I}n this paper, a new approach to real-time human detection through processing video captured by a thermal infrared camera mounted on the autonomous mobile platform m{S}ecurit {T} {M} is introduced. {T}he approach starts with a phase of static analysis for the detection of human candidates through some classical image processing techniques such as image normalization and thresholding. {T}hen, the proposal starts a dynamic image analysis phase based in optical flow or image difference. {O}ptical flow is used when the robot is moving, whilst image difference is the preferred method when the mobile platform is still. {T}he results of both phases are compared to enhance the human segmentation by infrared camera. {I}ndeed, optical flow or image difference will emphasize the foreground hot spot areas obtained at the initial human candidates’ detection.},
  Doi                      = {10.1016/j.robot.2010.06.002},
  File                     = {:../sources/1-s2.0-S0921889010001168-main.pdf:PDF},
  ISSN                     = {0921-8890},
  Keywords                 = {Surveillance system, Mobile robot, Infrared camera, Optical flow, Image subtraction},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0921889010001168}
}

@InProceedings{Galego2011,
  Title                    = {{V}ignetting {C}orrection {F}or {P}an-{T}ilt {S}urveillance {C}ameras},
  Author                   = {Galego, Ricardo and Bernardino, Alexandre and Gaspar, Jose},
  Booktitle                = {6th International Conference on Computer Vision Theory and Applications},
  Year                     = {2011},

  Address                  = {Algarve, Portugal},
  Month                    = {March},

  Abstract                 = {{I}t is a well know result that the geometry of pan and tilt (perspective) cameras auto-calibrate using just the
image information. {H}owever, applications based on panoramic background representations must also com-
pensate for radiometric effects due to camera motion. {I}n this paper we propose a methodology for calibrating
the radiometric effects inherent in the operation of pan-tilt cameras, with applications to visual surveillance
in a cube (mosaicked) visual field representation. {T}he radiometric calibration is based on the estimation of
vignetting image distortion using the pan and tilt degrees of freedom instead of color calibrating patterns.
{E}xperiments with real images show that radiometric calibration reduce the variance in the background repre-
sentation allowing for more effective event detection in background-subtraction-based algorithms.},
  File                     = {:../sources/11-VISAPP-RGalego.pdf:PDF},
  Keywords                 = {Image formation, Vignetting correction, Pan-Tilt cameras, Visual event detection, Surveillance},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.09.09}
}

@Article{Gaspar2011,
  Title                    = {{S}ingle {P}an and {T}ilt {C}amera {I}ndoor {P}ositioning and {T}racking {S}ystem},
  Author                   = {Gaspar, Tiago and Oliveira, Paulo},
  Journal                  = {European Journal of Control},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {414 - 428},
  Volume                   = {17},

  Abstract                 = {{A}n inexpensive single pan and tilt camera based indoor positioning and tracking system is proposed, supported on a functional architecture where three main modules can be identified: one related to the interface with the camera, tackled with parameter estimation techniques; other, responsible for isolating and identifying the target, based on advanced image processing techniques, and a third, that resorting to nonlinear dynamic system suboptimal state estimation techniques, performs the tracking of the target and estimates its position, and linear and angular velocities. {T}he contributions of this work are fourfold: i) a new indoor positioning and tracking system architecture; ii) a new lens distortion calibration method, that preserves generic straight lines in images; iii) suboptimal nonlinear multiple-model adaptive estimation techniques, for the adopted target model, to tackle the positioning and trackingtasks, andiv) theimplementationandvalidationin real time of a complex tracking system, based on a low cost single camera. {T}o assess the performance of the proposed system, a series of indoor experimental tests for a range of operation of up to ten meter were carried out. {A}centimetric accuracy was obtained under realistic conditions. },
  Doi                      = {10.3166/ejc.17.414-428},
  File                     = {:../sources/1-s2.0-S0947358011706082-main.pdf:PDF},
  ISSN                     = {0947-3580},
  Keywords                 = {Indoor positioning and tracking, nonlinear filtering, multiple-model adaptive estimation, single camera vision systems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0947358011706082}
}

@Article{Germa2010,
  Title                    = {{V}ision and {RFID} data fusion for tracking people in crowds by a mobile robot},
  Author                   = {Germa, T. and Lerasle, F. and Ouadah, N. and Cadenat, V.},
  Journal                  = {Computer Vision and Image Understanding},
  Year                     = {2010},
  Number                   = {6},
  Pages                    = {641–651},
  Volume                   = {114},

  Abstract                 = {{I}n this paper, we address the problem of realizing a human following task in a crowded environment. {W}e consider an active perception system, consisting of a camera mounted on a pan-tilt unit and a 360°360° {RFID} detection system, both embedded on a mobile robot. {T}o perform such a task, it is necessary to efficiently track humans in crowds. {I}n a first step, we have dealt with this problem using the particle filtering framework because it enables the fusion of heterogeneous data, which improves the tracking robustness. {I}n a second step, we have considered the problem of controlling the robot motion to make the robot follow the person of interest. {T}o this aim, we have designed a multi-sensor-based control strategy based on the tracker outputs and on the {RFID} data. {F}inally, we have implemented the tracker and the control strategy on our robot. {T}he obtained experimental results highlight the relevance of the developed perceptual functions. {P}ossible extensions of this work are discussed at the end of the article.},
  Doi                      = {doi:10.1016/j.cviu.2010.01.008},
  File                     = {:../sources/1-s2.0-S1077314210000317-main.pdf:PDF},
  Keywords                 = {Radio frequency ID, Multimodal data fusion, Particle filtering, Person tracking, Person following, Multi-sensor fusion, Human visual servoing},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Guha2005,
  Title                    = {{D}yna{T}racker: {T}arget tracking in active video surveillance systems},
  Author                   = {Guha, P. and Palai, D. and Goswami, D. and Mukerjee, A.},
  Booktitle                = {12th International Conference on Advanced Robotics (ICAR)},
  Year                     = {2005},

  Address                  = {USA, WA, Seattle},
  Month                    = {July},
  Pages                    = {621 - 627},
  Publisher                = {IEEE},

  Abstract                 = {{A}ctive video surveillance systems provide challenging research issues in the interface of computer vision, pattern recognition and control system analysis. {A} significant part of such systems is devoted toward active camera control for efficient target tracking. {D}yna{T}racker is a pan-tilt device based active camera system for maintaining continuous track of the moving target, while keeping the same at a pre-specified region (typically, the center) of the image. {T}he significant contributions in this work are the use of mean-shift algorithm for visual tracking and the derivation of the error dynamics for a proportional-integral control action. {T}he stability analysis and optimal controller gain selections are performed from the simulation studies of the derived error dynamics. {S}imulation predictions are also validated from the results of practical experimentations. {T}he present implementation of {D}yna{T}racker performs on a standard {P}entium {IV} {PC} at an average speed of 10 frames per second while operating on color images of 320x240 resolution},
  Doi                      = {10.1109/ICAR.2005.1507473},
  File                     = {:../sources/01507473.pdf:PDF},
  Keywords                 = {Cameras, Computer vision, Control system analysis, Control systems, Error correction, Pattern recognition, PI control, Stability analysis, Target tracking, Video surveillance},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.09.09}
}

@Article{Han2013,
  Title                    = {{T}he {T}racking of a {M}oving {O}bject by a {M}obile {R}obot {F}ollowing the {O}bject’s {S}ound},
  Author                   = {Han, Jongho and Han, Sunsin and Lee, Jangmyung},
  Journal                  = {Journal of Intelligent \& Robotic Systems},
  Year                     = {2013},

  Month                    = {July},
  Number                   = {1},
  Pages                    = {31-42},
  Volume                   = {71},

  Abstract                 = {{T}he tracking of a moving object with a mobile robot has been implemented based on the detected sound from the moving object using a microphone array. {T}he difference between the travel times of the sound source to each of the three microphones mounted to the robot has been used to calculate the distance and orientation of the sound source. {T}he cross-correlations between the received signals have been used to detect the individual sound signal from the object and to calculate the time difference between two signals. {T}his provides reliable and precise time differences among the sound signals arrived at the microphones compared to the conventional method. {I}n order to determine the tracking direction to the sound source, {F}uzzy rules have been applied; the results are used for real-time control of the mobile robot. {T}he efficiency of the proposed algorithm has been demonstrated through real-world experiments and compared to the conventional approach.},
  Doi                      = {10.1007/s10846-012-9769-3},
  File                     = {:../sources/art%3A10.1007%2Fs10846-012-9769-3.pdf:PDF},
  Keywords                 = {Microphone array, Tracking, Estimation, Mobile robot, Sound source},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Book{Hartley2004,
  Title                    = {{M}ultiple {V}iew {G}eometry in {C}omputer {V}ision},
  Author                   = {Hartley, Richard and Zisserman, Andrew},
  Publisher                = {Cambridge University Press},
  Year                     = {2004},

  File                     = {:../sources/Hartley, Zisserman - Multiple View Geometry in Computer Vision.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.08.05}
}

@InProceedings{Hashimoto2007,
  Title                    = {{M}oving-object tracking with multi-laser range sensors for mobile robot navigation},
  Author                   = {Hashimoto, M. and Takahashi, K. and Matsui, Y.},
  Booktitle                = {IEEE International Conference on Robotics and Biomimetics},
  Year                     = {2007},

  Address                  = {China, Sanya},
  Month                    = {December},
  Pages                    = {399 - 404},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents a method for moving-object tracking with in-vehicle 2{D}-laser range sensors ({LRS}). {S}ince a sensing area of the {LRS} is limited in orientation, mobile robot is equipped with multi-{LRS}'s for omnidirectional sensing. {I}n order for moving-object tracking by multi-{LRS}'s cooperation, the coordinate frames of the multi-{LRS}'s are calibrated based on {K}alman filter and chi-hypothesis testing. {T}he moving-object tracking is achieved by {K}alman filter, {C}ovariance {I}ntersection and the assignment algorithm based data association. {A} rule based track management system is embedded into the tracker in order to improve the tracking performance. {T}he experimental result of three people tracking in indoor environments validates the proposed method.},
  Doi                      = {10.1109/ROBIO.2007.4522195},
  File                     = {:../sources/04522195.pdf:PDF},
  Keywords                 = {Biosensors, Computer architecture, Filters, Indoor environments, Laser modes, Mobile robots, Navigation, Robot kinematics, Robot sensing systems, Sensor systems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Horn1981,
  Title                    = {{D}etermining {O}ptical {F}low},
  Author                   = {Berthold K. P. Horn and Brian G. Schunck},
  Journal                  = {Artificial Intelligence},
  Year                     = {1981},
  Pages                    = {185 - 203},
  Volume                   = {17},

  Abstract                 = {{O}ptical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. {A} second constraint is needed. {A} method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. {A}n iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. {T}he algorithm is robust in that it can handle image sequences that are quantified rather coarsely in space and time. {I}t is also insensitive to quantization of brightness levels and additive noise. {E}xamples are included where the assumption of smoothness is violated at singular points or along lines in the image.},
  File                     = {:../sources/horn81.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.20}
}

@Article{Hu2015,
  Title                    = {{M}oving object detection and tracking from video captured by moving camera },
  Author                   = {Wu-Chih Hu and Chao-Ho Chen and Tsong-Yi Chen and Deng-Yuan Huang and Zong-Che Wu},
  Journal                  = {Journal of Visual Communication and Image Representation},
  Year                     = {2015},
  Pages                    = {164 - 180},
  Volume                   = {30},

  Abstract                 = {{A}bstract {T}his paper presents an effective method for the detection and tracking of multiple moving objects from a video sequence captured by a moving camera without additional sensors. {M}oving object detection is relatively difficult for video captured by a moving camera, since camera motion and object motion are mixed. {I}n the proposed method, the feature points in the frames are found and then classified as belonging to foreground or background features. {N}ext, moving object regions are obtained using an integration scheme based on foreground feature points and foreground regions, which are obtained using an image difference scheme. {T}hen, a compensation scheme based on the motion history of the continuous motion contours obtained from three consecutive frames is applied to increase the regions of moving objects. {M}oving objects are detected using a refinement scheme and a minimum bounding box. {F}inally, moving object tracking is achieved using a {K}alman filter based on the center of gravity of a moving object region in the minimum bounding box. {E}xperimental results show that the proposed method has good performance.},
  Doi                      = {10.1016/j.jvcir.2015.03.003},
  File                     = {:../sources/1-s2.0-S104732031500053X-main.pdf:PDF},
  ISSN                     = {1047-3203},
  Keywords                 = {Object detection, Moving camera, Object tracking, Feature classification, Image difference, Object motion, Motion history, Ego-motion compensation },
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S104732031500053X}
}

@InProceedings{Hwang2010,
  Title                    = {{V}ision tracking system for mobile robots using two {K}alman filters and a slip detector},
  Author                   = {Hwang, Wonsang and Park, Jaehong and Kwon, Hyun-il and Anjum, M.L. and Kim, Jong-hyeon and Lee, Changhun and Kim, Kwang-soo and Cho, Dong-il Dan},
  Booktitle                = {2010 International Conference on Control Automation and Systems},
  Year                     = {2010},

  Address                  = {South Corea, Gyeonggi-do},
  Month                    = {October},
  Pages                    = {2041 - 2046},
  Publisher                = {IEEE},

  Abstract                 = {{T}he vision tracking system in this paper estimates the robot position relative to a target and rotates the camera towards the target. {T}o estimate the robot position of mobile robot, the system combines information from an accelerometer, a gyroscope, two encoders, and a vision sensor. {T}he encoders can provide fairly accurate robot position information, but the encoder data are not reliable when robot wheels slip. {A}ccelerometer data can provide the robot position information even when the wheels are slipping, but a long term position estimation is difficult, because of integration of errors arising from bias and noise. {T}o overcome the drawbacks of each method mentioned in the above, the proposed system uses data fusion with two {K}alman filters and a slip detector. {O}ne {K}alman filter is for the slip case, and the other is for the no-slip case. {E}ach {K}alman filter uses a different sensor combination for estimating the robot motion. {T}he slip detector compares the data from the accelerometer with the data from the encoders, and decides if a slip condition has occurred. {A}ccordingly, based on the decision of the slip detector, the system chooses one of the outputs of the two {K}alman filters, which is subsequently used for calculating the camera angle of the vision tracking system. {T}he vision tracking system is implemented on a two-wheeled robot. {T}o evaluate the tracking and recognition performance of the implemented system, experiments are performed for various robot motion scenarios in various environments.},
  File                     = {:../sources/05670110.pdf:PDF},
  Keywords                 = {Detectors, Kalman filters, Machine vision, Mobile robots, Robot sensing systems, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Book{Jaehne2005,
  Title                    = {{D}igital {I}mage {P}rocessing},
  Author                   = {Jähne, Bernd},
  Publisher                = {Springer-Verlag Berlin Heidelberg},
  Year                     = {2005},
  Edition                  = {6},

  File                     = {:../sources/o54fy.Digital.Image.Processing.Concepts.Algorithms.and.Scientific.Applications.6th.edition.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.16}
}

@Book{Kaehler2013,
  Title                    = {{L}earning {O}pen{CV} - {C}omputer {V}ision in {C}++ with the {O}pen{CV} {L}ibrary},
  Author                   = {Kaehler, Adrian and Bradski, Gary},
  Publisher                = {O'Reilly Media},
  Year                     = {2013},
  Edition                  = {2},

  File                     = {:../sources/oreilly-learning-opencv-second-edition-2013-ver2.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.08.05}
}

@Article{Kanatani1987,
  Title                    = {{C}amera rotation invariance of image characteristics},
  Author                   = {Kanatani, K.},
  Journal                  = {Computer Vision, Graphics, and Image Processing},
  Year                     = {1987},
  Number                   = {3},
  Pages                    = {328-354},
  Volume                   = {39},

  Abstract                 = {{T}he image transformation due to camera rotation relative to a stationary scene is analyzed, and the associated transformation rules of “features” given by weighted averaging of the image are derived by considering infinitesimal generators on the basis of group representation theory. {T}hree-dimensional vectors and tensors are reduced to two-dimensional invariants on the image plane from the viewpoint of projective geometry. {T}hree-dimensional invariants and camera rotation reconstruction are also discussed. {T}he result is applied to the shape recognition problem when camera rotation is involved.},
  Doi                      = {10.1016/S0734-189X(87)80185-8},
  File                     = {:../sources/ADA171615.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.09.14}
}

@Book{Karasulu2013,
  Title                    = {{P}erformance {E}valuation {S}oftware - {M}oving {O}bject {D}etection and {T}racking in {V}ideo},
  Author                   = {Karasulu, Bahadir and Korukoglu, Serdar},
  Publisher                = {Springer-Verlag New York},
  Year                     = {2013},
  Edition                  = {1},
  Series                   = {SpringerBriefs in Computer Science},

  Abstract                 = {{P}erformance {E}valuation {S}oftware: {M}oving {O}bject {D}etection and {T}racking in {V}ideos introduces a software approach for the real-time evaluation and performance comparison of the methods specializing in moving object detection and/or tracking ({D}&{T}) in video processing. {D}igital video content analysis is an important item for multimedia content-based indexing ({MCBI}), content-based video retrieval ({CBVR}) and visual surveillance systems. {T}here are some frequently-used generic algorithms for video object {D}&{T} in the literature, such as {B}ackground {S}ubtraction ({BS}), {C}ontinuously {A}daptive {M}ean-shift ({CMS}), {O}ptical {F}low ({OF}), etc. {A}n important problem for performance evaluation is the absence of any stable and flexible software for comparison of different algorithms. {I}n this frame, we have designed and implemented the software for comparing and evaluating the well-known video object {D}&{T} algorithms on the same platform. {T}his software is able to compare them with the same metrics in real-time and on the same platform. {I}t also works as an automatic and/or semi-automatic test environment in real-time, which uses the image and video processing essentials, e.g. morphological operations and filters, and ground-truth ({GT}) {XML} data files, charting/plotting capabilities, etc. {A}long with the comprehensive literature survey of the abovementioned video object {D}&{T} algorithms, this book also covers the technical details of our performance benchmark software as well as a case study on people {D}&{T} for the functionality of the software},
  Doi                      = {10.1007/978-1-4614-6534-8},
  File                     = {:../sources/Karasulu B., Korukoglu S. - Moving Object Detection and Tracking in Videos.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Kim2011,
  Title                    = {{V}ision system for mobile robots for tracking moving targets, based on robot motion and stereo vision information},
  Author                   = {Kim, Kwang-soo and Bahn, Wook and Lee, Changhun and Lee, Tae-jae and Shaikh, M.M. and Kim, Kwang-soo},
  Booktitle                = {IEEE/SICE International Symposium on System Integration},
  Year                     = {2011},

  Address                  = {Japan, Kyoto},
  Month                    = {December},
  Pages                    = {634 - 639},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents a vision-tracking for mobile robots, which tracks a moving target based on robot motion and stereo vision information. {T}he proposed system controls pan and tilt actuators attached to a stereo camera, using the data from a gyroscope, robot wheel encoders, pan and tilt actuator encoders, and the stereo camera. {U}sing this proposed system, the stereo camera always faces the moving target. {T}he developed system calculates the angles of the pan and tilt actuators by estimating the relative position of the target with respect to the position of the robot. {T}he developed system estimates the target position using the robot motion information and the stereo vision information. {T}he movement of the robot is modeled as the transformation of the frame, which consists of a rotation and a translation. {T}he developed system calculates the rotation using 3-axis gyroscope data and the translation using robot wheel encoder data. {T}he proposed system measures the position of the target relative to the robot, combining the encoder data of pan and tilt actuators and the disparity map of the stereo vision. {T}he inevitable mismatch of the data, which occurs from the asynchrony of the multiple sensors, is prevented by the proposed system, which compensates for the communication latency and the computation time. {T}he experimental results show that the developed system achieves excellent tracking performance in several motion scenarios, including combinations of straights and curves and climbing of slopes.},
  Doi                      = {10.1109/SII.2011.6147522},
  File                     = {:../sources/06147522.pdf:PDF},
  Keywords                 = {Actuators, Cameras, Instruction sets, Mobile robots, Robot vision systems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Kim2012,
  Title                    = {{A} {R}obotic {P}an and {T}ilt 3-{D} {T}arget {T}racking {S}ystem by {D}ata {F}usion of {V}ision, {E}ncoder, {A}ccelerometer, and {G}yroscope {M}easurements},
  Author                   = {Kim, Tae-Il and Bahn, Wook and Lee, Chang-Hun and Lee, Tae-Jae and Jang, Byung-Moon and Lee, Sang-Hoon and Moon, Min-Wug and Cho, Dong-Il},
  Booktitle                = {Intelligent Robotics and Applications - 5th International Conference},
  Year                     = {2012},

  Address                  = {Canada, Montreal},
  Month                    = {October},
  Pages                    = {676-685},
  Publisher                = {Springer Berlin Heidelberg},

  Abstract                 = {{T}his paper presents a vision-tracking system for mobile robots, which travel in a 3-dimentional environment. {T}he developed system controls pan and tilt actuators attached to a camera so that a target is always directly in the line of sight of the camera. {T}his is achieved by using data from robot wheel encoders, a 3-axis accelerometer, a 3-axis gyroscope, pan and tilt motor encoders, and camera. {T}he developed system is a multi-rate sampled data system, where the sampling rate of the camera is different with that of the other sensors. {F}or the accurate estimation of the robot velocity, the developed system detects the slip of robot wheels, by comparing the data from the encoders and the accelerometer. {T}he developed system estimates the target position by using an extended {K}alman filter. {T}he experiments are performed to show the tracking performance of the developed system in several motion scenarios, including climbing slopes and slip cases.},
  Doi                      = {10.1007/978-3-642-33515-0_66},
  File                     = {:../sources/chp%3A10.1007%2F978-3-642-33515-0_66.pdf:PDF},
  Keywords                 = {Vision tracking system, Sensor data fusion, Kalman filter, Slip detection},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Kulchandani2015,
  Title                    = {{M}oving object detection: {R}eview of recent research trends},
  Author                   = {Kulchandani, Jaya S. and Dangarwala, Kruti J.},
  Booktitle                = {Pervasive Computing (ICPC), 2015 International Conference on},
  Year                     = {2015},

  Address                  = {India, Pune},
  Month                    = {January},
  Pages                    = {1 - 5},
  Publisher                = {IEEE},

  Abstract                 = {{M}oving object detection is the task of identifying the physical movement of an object in a given region or area. {O}ver last few years, moving object detection has received much of attraction due to its wide range of applications like video surveillance, human motion analysis, robot navigation, event detection, anomaly detection, video conferencing, traffic analysis and security. {I}n addition, moving object detection is very consequential and efficacious research topic in field of computer vision and video processing since it forms a critical step for many complex processes like video object classification and video tracking activity. {C}onsequently, identification of actual shape of moving object from a given sequence of video frames becomes pertinent. {H}owever, task of detecting actual shape of object in motion becomes tricky due to various challenges like dynamic scene changes, illumination variations, presence of shadow, camouflage and bootstrapping problem. {T}o reduce the effect of these problems, researchers have proposed number of new approaches. {T}his paper provides a brief classification of the classical approaches for moving object detection. {F}urther, paper reviews recent research trends to detect moving object for single stationary camera along with discussion of key points and limitations of each approach.},
  Doi                      = {10.1109/PERVASIVE.2015.7087138},
  File                     = {:../sources/07087138.pdf:PDF},
  Keywords                 = {Human Motion Analysis, Moving Object Detection, Object Classification, Tracking, Video Survillance},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.05}
}

@InProceedings{Kumar2001,
  Title                    = {{A}erial video surveillance and exploitation},
  Author                   = {Kumar, Rakesh and Sawhney, H. and Samarasekera, Supun and Hsu, S. and Hai Tao and Yanlin Guo and Hanna, K. and Pope, A. and Wildes, R. and Hirvonen, D. and Hansen, M. and Burt, P.},
  Booktitle                = {Proceedings of the IEEE},
  Year                     = {2001},
  Month                    = {October},
  Pages                    = {1518 - 1539},
  Publisher                = {IEEE},
  Volume                   = {89},

  Abstract                 = {{T}here is growing interest in performing aerial surveillance using video cameras. {C}ompared to traditional framing cameras, video cameras provide the capability to observe ongoing activity within a scene and to automatically control the camera to track the activity. {H}owever, the high data rates and relatively small field of view of video cameras present new technical challenges that must be overcome before such cameras can be widely used. {I}n this paper, we present a framework and details of the key components for real-time, automatic exploitation of aerial video for surveillance applications. {T}he framework involves separating an aerial video into the natural components corresponding to the scene. {T}hree major components of the scene are the static background geometry, moving objects, and appearance of the static and dynamic components of the scene. {I}n order to delineate videos into these scene components, we have developed real time, image-processing techniques for 2-{D}/3-{D} frame-to-frame alignment, change detection, camera control, and tracking of independently moving objects in cluttered scenes. {T}he geo-location of video and tracked objects is estimated by registration of the video to controlled reference imagery, elevation maps, and site models. {F}inally static, dynamic and reprojected mosaics may be constructed for compression, enhanced visualization, and mapping applications},
  Doi                      = {10.1109/5.959344},
  File                     = {:../sources/00959344.pdf:PDF},
  Keywords                 = {Cameras, Layout, Machine intelligence, Monitoring, Object detection, Vehicle detection, Vehicle dynamics, Video compression, Video surveillance, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Lee2011,
  Title                    = {{V}ision tracking of a moving robot from a second moving robot using both relative and absolute position referencing methods},
  Author                   = {Lee, Changhun and Park, Jaehong and Bahn, Wook and Kim, Tae-Il and Lee, Tae-jae and Shaikh, Muneeb and Kim, Kwang-soo and Cho, Dong-Il},
  Booktitle                = {37th Annual Conference on IEEE Industrial Electronics Society},
  Year                     = {2011},

  Address                  = {Australia, Melbourne},
  Month                    = {November},
  Pages                    = {325 - 330},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents a vision tracking system for a moving robot that provides continuous tracking of another moving robot. {T}he purpose of the proposed system is to control the line of sight of the camera, which is on the targetting robot, for tracking the targetted robot by using the position information of both the targetting robot and the targetted robot. {T}o estimate the positions of both robots, the proposed system uses two encoders and a gyroscope for the targetting robot and active beacons for the targetted robot. {T}he proposed system computes the relative position between the two robots and aligns the camera for tracking the targetted robot. {T}he proposed system is experimentally evaluated for various scenarios, including the occultation of the targetted robot and temporary illumination changes. {T}he experimental results show that the proposed system is able to track the moving robot well without visual information, in normal situations.},
  Doi                      = {10.1109/IECON.2011.6119273},
  File                     = {:../sources/06119273.pdf:PDF},
  Keywords                 = {Cameras, Mobile robots, Robot kinematics, Robot vision systems, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Li2007,
  Title                    = {{G}round {M}oving {T}arget {T}racking {C}ontrol {S}ystem {D}esign for {UAV} {S}urveillance},
  Author                   = {Li, Zhongjian and Ding, Jiarui},
  Booktitle                = {Proceedings of IEEE International Conference on Automation and Logistics},
  Year                     = {2007},

  Address                  = {China, Jinan},
  Month                    = {August},
  Pages                    = {1458 - 1463},
  Publisher                = {IEEE},

  Abstract                 = {{G}round target detection, recognition and tracking play critical roles in aerial video surveillance applications. {A} ground moving target detection and tracking system based on active vision is studied firstly. {A}fter compensating the global motion, the independent moving targets are detected by the salience of residual images. {T}hen object trackers are initialized to track these detected moving targets, at the same time, by controlling the payload equipment to achieve the given surveillance tasks. {T}he influence of the movement of {UAV} to swivel table is studied and the apparent motion function is developed. {I}n order to achieve good tracking performance, a mixed {H}2/{H}-infinite robust flight controller is designed for the {UAV} and a controller containing the disturbance observer ({DOB}) is designed for payload platform. {T}his video servo control system successfully compensates the influence of {UAV} movement to swivel table control. {R}eal-time simulation results show the effectiveness of the designed system.},
  Doi                      = {10.1109/ICAL.2007.4338800},
  File                     = {:../sources/04338800.pdf:PDF},
  Keywords                 = {Control systems, Hydrogen, Motion detection, Object detection, Payloads, Robust control, Target recognition, Target tracking, Unmanned aerial vehicles, Video surveillance},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Liem2008,
  Title                    = {{A} hybrid algorithm for tracking and following people using a robotic dog},
  Author                   = {Liem, Martijn and Visser, Arnoud and Groen, Frans},
  Booktitle                = {Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction},
  Year                     = {2008},

  Address                  = {Netherlands, Amsterdam},
  Pages                    = {185-192},
  Publisher                = {ACM},
  Series                   = {HRI '08},

  Abstract                 = {{T}he capability to follow a person in a domestic environment is an important prerequisite for a robot companion. {I}n this paper, a tracking algorithm is presented that makes it possible to follow a person using a small robot. {T}his algorithm can track a person while moving around, regardless of the sometimes erratic movements of the legged robot. {R}obust performance is obtained by fusion of two algorithms, one based on salient features and one on color histograms. {R}e-initializing object histograms enables the system to track a person even when the illumination in the environment changes. {B}y being able to re-initialize the system on run time using background subtraction, the system gains an extra level of robustness.},
  Doi                      = {10.1145/1349822.1349847},
  File                     = {:../sources/p185-liem.pdf:PDF},
  Keywords                 = {awareness and monitoring of humans, robot companion},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Liu2014,
  Title                    = {{D}ynamic objects tracking with a mobile robot using passive {UHF} {RFID} tags},
  Author                   = {Liu, Ran and Huskic, G. and Zell, A.},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
  Year                     = {2014},

  Address                  = {USA, Chicago},
  Month                    = {September},
  Pages                    = {4247 - 4252},
  Publisher                = {IEEE},

  Abstract                 = {{R}ecent research deals more and more with the application of ultra high frequency ({UHF}) radio-frequency identification ({RFID}) on mobile robots. {H}owever, the sensing characteristics between the reader and the tag (i.e. detections and signal strength) are challenging to model due to the influence of environmental effects (e.g. tag density, reflection, diffraction, or absorption). {I}n this paper, we address the problem of dynamic objects tracking with a mobile agent using the signal strength from {UHF} {RFID} tags attached to objects. {O}ur solution estimates the positions of {RFID} tags under a {B}ayesian framework. {M}ore precisely, we combine a two stage dynamic motion model with the dual particle filter, to capture the dynamic motion of the object and to quickly recover from failures in tracking. {T}his approach is then tested on a {S}citos {G}5 mobile robot through various experiments.},
  Doi                      = {10.1109/IROS.2014.6943161},
  File                     = {:../sources/06943161.pdf:PDF},
  Keywords                 = {Estimation, Navigation, RFID tags, Robot sensing systems, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Lowe2004,
  Title                    = {{D}istinctive {I}mage {F}eatures from {S}cale-{I}nvariant {K}eypoints},
  Author                   = {Lowe, David G.},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2004},

  Month                    = {November},
  Number                   = {2},
  Pages                    = {91 - 110},
  Volume                   = {60},

  Abstract                 = {{T}his paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. {T}he features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3{D} viewpoint, addition of noise, and change in illumination. {T}he features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. {T}his paper also describes an approach to using these features for object recognition. {T}he recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a {H}ough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. {T}his approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
  Doi                      = {10.1023/B:VISI.0000029664.99615.94},
  File                     = {:../sources/art%3A10.1023%2FB%3AVISI.0000029664.99615.94.pdf:PDF},
  Keywords                 = {invariant features, object recognition, scale invariance, image matching},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2016.01.13}
}

@InProceedings{Markovic2014,
  Title                    = {{M}oving object detection, tracking and following using an omnidirectional camera on a mobile robot},
  Author                   = {Markovic, I. and Chaumette, F. and Petrovic, I.},
  Booktitle                = {IEEE International Conference on Robotics and Automation},
  Year                     = {2014},

  Address                  = {Hong Kong},
  Month                    = {June},
  Pages                    = {5630 - 5635},
  Publisher                = {IEEE},

  Abstract                 = {{E}quipping mobile robots with an omnidirectional camera is very advantageous in numerous applications as all information about the surrounding scene is stored in a single image frame. {I}n the given context, the present paper is concerned with detection, tracking and following of a moving object with an omnidirectional camera. {T}he camera calibration and image formation is based on the spherical unified projection model thus yielding a representation of the omnidirectional image on the unit sphere. {D}etection of moving objects is performed by calculating a sparse optical flow in the image and then lifting the flow vectors on the unit sphere where they are discriminated as dynamic or static by analytically calculating the distance of the terminal vector point to a great circle arc. {T}he flow vectors are then clustered and the center of gravity is calculated to form the sensor measurement. {F}urthermore, the tracking is posed as a {B}ayesian estimation problem on the unit sphere and the solution based on the von {M}ises-{F}isher distribution is utilized. {V}isual servoing is performed for the object following task where the control law calculation is based on the projection of a point on the unit sphere. {E}xperimental results obtained by a camera with a fish-eye lens mounted on a differential drive mobile robot are presented and discussed.},
  Doi                      = {10.1109/ICRA.2014.6907687},
  File                     = {:../sources/06907687.pdf:PDF},
  Keywords                 = {Cameras, Mobile robots, Optical imaging, Robot kinematics, Robot vision systems, Vectors},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Mohamed2014,
  Title                    = {{R}eal-time moving objects tracking for mobile-robots using motion information},
  Author                   = {Mohamed, M.A. and Boddeker, C. and Mertsching, B.},
  Booktitle                = {IEEE International Symposium on Safety, Security, and Rescue Robotics},
  Year                     = {2014},

  Address                  = {Japan, Hokkaido},
  Month                    = {October},
  Pages                    = {1 - 6},
  Publisher                = {IEEE},

  Abstract                 = {{T}he perceptional of the motion of objects is a key problem for a mobile robot to perform tasks in a dynamic environment. {T}hus, we present a real-time approach for tracking multiple moving objects. {T}he proposed algorithm initially detects moving regions and a dense optical flow technique is exclusively applied to those regions between two consecutive frames. {A}fterwards, the moving objects in each region are determined based on the planar parallax motion by assuming that independently moving objects undergo pure translation. {F}or subsequent frames, the detected moving objects are tracked based on the orientation of the flow fields, while the new position is updated. {I}n turn, the new detected objects are modeled during a tracking period. {T}he proposed algorithm has been tested with various scenarios and the experimental results demonstrate that the proposed algorithm works properly. {I}t can be shown that there is a significant reduction in the overall processing time for detecting and tracking multiple moving objects in a scene.},
  Doi                      = {10.1109/SSRR.2014.7017674},
  File                     = {:../sources/07017674.pdf:PDF},
  Keywords                 = {Cameras, Heuristic algorithms, Motion segmentation, Optical imaging, Real-time systems, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Murakami2012,
  Title                    = {{P}osition tracking and recognition of everyday objects by using sensors embedded in an environment and mounted on mobile robots},
  Author                   = {Murakami, K. and Matsuo, K. and Hasegawa, T. and Kurazume, R.},
  Booktitle                = {IEEE International Conference on Robotics and Automation},
  Year                     = {2012},

  Address                  = {USA, Saint Paul},
  Month                    = {May},
  Pages                    = {2210 - 2216},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper describes an object tracking system for a robot working in an everyday environment, which tracks and recognizes everyday objects. {P}assive {RFID} ({R}adio {F}requency {ID}entification) tags are attached to the objects for object recognition. {T}he system consists of static sensors embedded in the environment and mobile sensors mounted on mobile robots. {B}y utilizing the different characteristics and advantages of these sensors, the system achieves good performance in an everyday environment. {A}lthough the tag {ID} and the position of an object carried by a person is not measurable by static sensors or by mobile sensors, the system can estimate them by using an {SIR} ({S}equential importance resampling) particle filter that integrates the data obtained by the static sensors and the mobile sensors. {I}n the experiment, the system successfully tracked 20 objects, some of which were held by a person.},
  Doi                      = {10.1109/ICRA.2012.6225329},
  File                     = {:../sources/06225329.pdf:PDF},
  Keywords                 = {Mobile robots, Position measurement, Radiofrequency identification, Robot sensing systems, Sensor systems},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Murray1994,
  Title                    = {{M}otion tracking with an active camera},
  Author                   = {Murray, Don and Basu, Anup},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {1994},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {449 - 459},
  Volume                   = {16},

  Abstract                 = {{T}his paper describes a method for real-time motion detection using an active camera mounted on a pan/tilt platform. {I}mage mapping is used to align images of different viewpoints so that static camera motion detection can be applied. {I}n the presence of camera position noise, the image mapping is inexact and compensation techniques fail. {T}he use of morphological filtering of motion images is explored to desensitize the detection algorithm to inaccuracies in background compensation, {T}wo motion detection techniques are examined, and experiments to verify the methods are presented. {T}he system successfully extracts moving edges from dynamic images even when the pan/tilt angles between successive frames are as large as 3},
  Doi                      = {10.1109/34.291452},
  File                     = {:../sources/background_compensation_and_an_active_camera_motion_tracking_algorithm.pdf:PDF},
  Keywords                 = {Animals, Application software, Cameras, Computer vision ,Filtering, Motion detection, Object recognition, Robot vision systems, Service robots, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.06.07}
}

@InProceedings{Olivares-Mendez2009,
  Title                    = {{A} pan-tilt camera {F}uzzy vision controller on an unmanned aerial vehicle},
  Author                   = {Olivares-Mendez, M.A. and Campoy, P. and Martinez, C. and Mondragon, I.},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
  Year                     = {2009},

  Address                  = {USA, MO, St. Louis},
  Month                    = {October},
  Pages                    = {2879 - 2884},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper presents an implementation of two {F}uzzy {L}ogic controllers working in parallel for a pan-tilt camera platform on an {UAV}. {T}his implementation uses a basic {L}ucas-{K}anade tracker algorithm, which sends information about the error between the center of the object to track and the center of the image, to the {F}uzzy controller. {T}his information is enough for the controller to follow the object by moving a two axis servo-platform, regardless the {UAV} vibrations and movements. {T}he two {F}uzzy controllers for each axis, work with a rules-base of 49 rules, two inputs and one output with a more significant sector defined to improve the behavior of those controllers. {T}he controllers have shown very good performances in real flights for statics objects, tested on the {C}olibri prototypes.},
  Doi                      = {10.1109/IROS.2009.5354576},
  File                     = {:../sources/05354576.pdf:PDF},
  Keywords                 = {Cameras, Computer vision, Control systems, Fuzzy control, Fuzzy logic, Helicopters, State estimation, Target tracking, Testing, Unmanned aerial vehicles},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Park2011,
  Title                    = {{P}an/{T}ilt {C}amera {C}ontrol for {V}ision {T}racking {S}ystem {B}ased on the {R}obot {M}otion and {V}ision {I}nformation},
  Author                   = {Park, Jaehong and Hwang, Wonsang and Bahn, Wook and Lee, Changhun and Tae-Il, Kim and Shaikh, Muhammad Muneeb and Kim, Kwangsoo and Cho, Dong-il Dan},
  Booktitle                = {18th IFAC World Congress},
  Year                     = {2011},

  Address                  = {Italy, Milano},
  Month                    = {August},
  Pages                    = {3165-3170},
  Publisher                = {IFAC},

  Abstract                 = {{T}his paper presents a vision tracking system, based on the robot motion and vision information. {F}or mobile robots, it is difficult to collect continuous vision information while they are in motion due to the unstable vision information. {T}o solve this problem, the proposed vision tracking system estimates the robot position relative to a target and rotates a camera towards the target based on the estimated position information. {T}his concept is derived from the human eye reflex mechanisms, known as the {V}estibulo-{O}cular {R}eflex ({VOR}) and the {O}pto-{K}inetic {R}eflex ({OKR}). {I}n the proposed vision tracking system, the {VOR} concept for compensating the head motion is realized by the feedforward control using the robot motion information from a 3-axis gyroscope and wheel encoders. {T}o realize the {OKR} concept, targeting errors are periodically compensated by using the vision feedback information. {A}n actuation module consists of pan/tilt motion motors and the camera which performs the pan and tilt functions to locate a target at the center of an image plane. {T}he proposed vision tracking system is integrated with a two-wheeled robot. {T}he performance of the proposed system is evaluated by extensive experiments. {T}he proposed system shows a significant improvement in tracking performance with small targeting error. {A}lso, the necessities of the position feedforward and vision feedback controls are verified by the experiments for the illumination change condition and the static tilting motion.},
  Doi                      = {10.3182/20110828-6-IT-1002.01781},
  File                     = {:../sources/1781.pdf:PDF},
  Keywords                 = {Perception and sensing, Information and sensor fusion. Intelligent robotics},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Park2010,
  Title                    = {{H}igh performance vision tracking system for mobile robot using sensor data fusion with {K}alman filter},
  Author                   = {Park, Jaehong and Hwang, Wonsang and Kwon, Hyun-il and Kim, Jong-hyeon and Lee, Chang-hun and Anjum, M.L. and Kim, Kwang-soo and Cho, Dong-il},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
  Year                     = {2010},

  Address                  = {Taiwan, Taipei},
  Month                    = {October},
  Pages                    = {3778 - 3783},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper introduces a high performance vision tracking system for mobile robot using sensor data fusion. {F}or mobile robots, it is difficult to collect continuous vision information due to robot's motion. {T}o solve this problem, the proposed vision tracking system estimates the robot's position relative to a target and rotates the camera towards the target. {T}his concept is derived from the human eye reflex mechanism, known as the {V}estibulo-{O}cular {R}eflex ({VOR}), for compensating the head motion. {T}his concept for tracking the target results in much higher performance levels, when compared with the conventional method that rotates the camera using only vision information. {T}he proposed system do not require heavy computing loads to process image data and can track the target continuously even during vision occlusion. {T}he robot motion information is estimated using data from accelerometer, gyroscope, and encoders. {T}his multi-sensor data fusion is achieved using {K}alman filter. {T}he proposed vision tracking system is implemented on a two-wheeled robot. {T}he experimental results show that the proposed system achieves excellent tracking and recognition performance in various motion scenarios, including scenarios where camera is temporarily blocked from the target.},
  Doi                      = {10.1109/IROS.2010.5650367},
  File                     = {:../sources/05650367.pdf:PDF},
  Keywords                 = {Kalman filters, accelerometers, cameras, gyroscopes, mobile robots, motion compensation, object tracking, robot vision, sensor fusion, wheels},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Book{Pratt2007,
  Title                    = {{D}igital {I}mage {P}rocessing},
  Author                   = {Pratt, William K.},
  Publisher                = {John Wiley and Sons},
  Year                     = {2007},
  Edition                  = {4},

  File                     = {:../sources/Pratt - Digital Image Processing 4th Ed.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.15}
}

@InProceedings{Rui2009,
  Title                    = {{M}oving object tracking based on mobile robot vision},
  Author                   = {Rui, Lin and Zhijiang, Du and Lining, Sun},
  Booktitle                = {International Conference on Mechatronics and Automation},
  Year                     = {2009},

  Address                  = {China, Changchun},
  Month                    = {August},
  Pages                    = {3625 - 3630},
  Publisher                = {IEEE},

  Abstract                 = {{T}he paper describes a robotic application that tracks a moving object by utilizing a mobile robot with capacity of avoiding obstacles. {R}eal-time tracking algorithm is based on mobile robot vision using adaptive color matching and {K}alman filter. {A}daptive color matching is used to obtain motion vectors of moving object in the robot coordinate system. {I}t can adjust color matching threshold adaptively to reduce the influence of lighting variations in the scene. {K}alman filter is applied to design a view window. {V}iew window, which contains the next position of the moving object estimated by {K}alman {A}lter, is determined on image plane to reduce the image processing area. {C}olor matching threshold can adjust itself adaptively in view window. {A} virtual targets based algorithm is also presented for real-time obstacle avoidance in this paper. {E}xperimental results show that the tracking algorithm can adapt to lighting variations and has good tracking precision. {I}t can also avoid obstacles smoothly while tracking moving object. {F}urthermore, it can be implemented in real time.},
  Doi                      = {10.1109/ICMA.2009.5246022},
  File                     = {:../sources/05246022.pdf:PDF},
  Keywords                 = {Force control, Fuzzy control, Image sequences, Laboratories, Mobile robots, Robot kinematics, Robot vision systems, Robotics and automation, Sun, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Sadeghi-Tehran2014,
  Title                    = {{A} real-time approach for autonomous detection and tracking of moving objects from {UAV}},
  Author                   = {Sadeghi-Tehran, P. and Clarke, C. and Angelov, P.},
  Booktitle                = {Symposium on Evolving and Autonomous Learning Systems},
  Year                     = {2014},

  Address                  = {USA, FL, Orlando},
  Pages                    = {43 - 49},
  Publisher                = {IEEE},

  Abstract                 = {{A} new approach to autonomously detect and track moving objects in a video captured by a moving camera from a {UAV} in real-time is proposed in this paper. {T}he introduced approach replaces the need for a human operator to perform video analytics by autonomously detecting moving objects and clustering them for tracking purposes. {T}he effectiveness of the introduced approach is tested on the footage taken from a real {UAV} and the evaluation results are demonstrated in this paper.},
  Doi                      = {10.1109/EALS.2014.7009502},
  File                     = {:../sources/07009502.pdf:PDF},
  Keywords                 = {Cameras, Clustering algorithms, Detectors, Feature extraction, Optical imaging, Robustness, Tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Article{Smeulders2010,
  Title                    = {{V}isual {T}racking: {A}n {E}xperimental {S}urvey},
  Author                   = {Smeulders, A.W.M. and Chu, D.M. and Cucchiara, R. and Calderara, S. and Dehghan, A. and Shah, M.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2010},

  Month                    = {November},
  Pages                    = {1442 - 1468},
  Volume                   = {36},

  Abstract                 = {{T}here is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. {O}bject tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. {A} good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. {H}owever, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. {I}n this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. {W}e selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. {W}e demonstrate that trackers can be evaluated objectively by survival curves, {K}aplan {M}eier statistics, and {G}rubs testing. {W}e find that in the evaluation practice the {F}-score is as effective as the object tracking accuracy ({OTA}) score. {T}he analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers.},
  Booktitle                = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Doi                      = {10.1109/TPAMI.2013.230},
  File                     = {:../sources/06671560.pdf:PDF},
  Keywords                 = {Educational institutions, Object tracking, Radar tracking, Robustness, Target tracking, Videos},
  Owner                    = {Stanisław Maciąg},
  Publisher                = {IEEE},
  Timestamp                = {2015.05.20}
}

@Book{Szeliski2011,
  Title                    = {{C}omputer {V}ision - {A}lgorithms and {A}pplications},
  Author                   = {Szeliski, Richard},
  Publisher                = {Springer-Verlag London},
  Year                     = {2011},

  File                     = {:../sources/Szeliski - Computer Vision Algorithms and Applications.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.08.04}
}

@InProceedings{Tinh2014,
  Title                    = {{V}isual control of integrated mobile robot-pan tilt-camera system for tracking a moving target},
  Author                   = {Tinh, N.V. and Cat, P.T. and Tuan, P.M. and Bui, T.T.Q.},
  Booktitle                = {IEEE International Conference on Robotics and Biomimetics},
  Year                     = {2014},

  Address                  = {Indonesia, Bali},
  Month                    = {December},
  Pages                    = {1566 - 1571},
  Publisher                = {IEEE},

  Abstract                 = {{I}n this paper, in order to track a moving target, we propose a new control law for an integrated mobile robot- pan tilt-camera system. {O}ur controller consists of two control loops, i.e., a kinematic and dynamic control loop, respectively. {T}he kinematic control loop performs three tasks, i.e., allowing an image feature of the target to converge to the center of the image plane asymptotically, designing a trajectory for the mobile robot, and allowing the mobile robot to track the desired position and direction. {I}n the dynamic control loop, the torques are determined; the actual angular velocities of the system, i.e., angular velocities of pan and tilt axis, angular velocities of right and left wheels, track the desired angular velocities which are the outputs of the kinematic controller of the kinematic control loop. {A}ccording to the {L}yapunov theory and {B}arbalat's theorem, the asymptotic stability of the whole system is proven. {S}imulation results achieved by using {M}atlab - {S}imulink are introduced.},
  Doi                      = {10.1109/ROBIO.2014.7090557},
  File                     = {:../sources/07090557.pdf:PDF},
  Keywords                 = {Angular velocity, Cameras, Kinematics, Mobile robots, Robot vision systems, Target tracking, Wheels},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@Article{Torkaman2012,
  Title                    = {{A} {K}alman-{F}ilter-{B}ased {M}ethod for {R}eal-{T}ime {V}isual {T}racking of a {M}oving {O}bject {U}sing {P}an and {T}ilt {P}latform},
  Author                   = {Torkaman, B. and Farrokhi, M.},
  Journal                  = {International Journal of Scientific and Engineering Research},
  Year                     = {2012},

  Month                    = {August},
  Number                   = {8},
  Pages                    = {1-6},
  Volume                   = {3},

  Abstract                 = {{T}he problem of real time estimating position and orientation of a moving object is an important issue for vision-based control of pan and tilt. {T}his paper presents a vision-based navigation strategy for a pan and tilt platform and a mounted video camera as a visual sensor. {F}or detection of objects, a suitable image processing algorithm is used. {M}oreover, estimation of the object position is performed using the {K}alman filter as an estimator. {T}he proposed method is implemented experimentally to a laboratory-size pan and tilt platform. {E}xperimental results show good target tracking by the proposed method in real-time.},
  File                     = {:../sources/researchpaper_A-KalmanFilterBased-Method-for-RealTime-Visual-Tracking-of-a-Moving-Object-Using-Pan-and-Tilt-Platform.pdf:PDF},
  Keywords                 = {Visual servoing, Vision-based navigation, Target tracking, Estimation, Pan and tilt platform, Kalman filter, Image processing},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.08.05}
}

@Book{Treiber2010,
  Title                    = {{A}n {I}ntroduction to {O}bject {R}ecognition - {S}elected {A}lgorithms for a {W}ide {V}ariety of {A}pplications},
  Author                   = {Treiber, Marco Alexander},
  Publisher                = {Springer-Verlag London},
  Year                     = {2010},
  Edition                  = {1},

  File                     = {:../sources/An Introduction to Object Recognition.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.16}
}

@Book{Trucco1998,
  Title                    = {{I}ntroductory {T}echniques for 3-{D} {C}omputer {V}ision},
  Author                   = {Trucco, Emanuele and Verri, Alessandro},
  Publisher                = {Prentice Hall PTR},
  Year                     = {1998},

  Address                  = {Upper Saddle River, New Jersey, USA},

  File                     = {:../sources/Introductory Techniques for 3-D Computer Vision - Emanuele Trucco, Alessandro Verri.pdf:PDF},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.18}
}

@InProceedings{Xiang2009,
  Title                    = {{R}eal-{T}ime {F}ollow-{U}p {T}racking {F}ast {M}oving {O}bject with an {A}ctive {C}amera},
  Author                   = {Xiang, Guishan},
  Booktitle                = {2nd International Congress on Image and Signal Processing},
  Year                     = {2009},

  Address                  = {China, Tianjin},
  Month                    = {October},
  Pages                    = {1 - 4},
  Publisher                = {IEEE},

  Abstract                 = {{A} method for real-time follow-up tracking fast moving object with an active camera is proposed. {T}he mean shift algorithm shows an excellent performance on robust and fast object tracking; however, it is prone to fail when tracking a very fast moving object. {T}his paper addresses to solve the problem. {A}t first mean shift based on the color feature of the object is introduced. {T}he object maybe can't be tracked when the motion of the object or the motion of the active camera is very fast. {I}n order to overcome this difficulty, {K}alman filter is introduced to predict the location of the initial search window of mean shift. {I}n order to reduce the disturbance from the background and to reduce the computational consumption, a region of interest ({ROI}) is introduced. {T}o follow-up track the moving object smoothly, a closed loop control model based on speed regulation is applied to drive a {PTZ} camera to center the target. {T}he results of our experiments show that the active camera can follow-up track a fast moving object smoothly. {T}he system is computationally efficient and can run in real-time speed.},
  Doi                      = {10.1109/CISP.2009.5303457},
  File                     = {:../sources/05303457.pdf:PDF},
  Keywords                 = {Cameras, Colored noise, Histograms, Mathematical model, Probability distribution, Real time systems, Robustness, Target tracking, Tracking loops, Video sequences},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Xiang2009a,
  Title                    = {{T}racking object under intense disturbance based on adaptive histogram with an active camera},
  Author                   = {Xiang, Guishan and Lin, Zhijie},
  Booktitle                = {International Conference on Mechatronics and Automation},
  Year                     = {2009},

  Address                  = {August},
  Pages                    = {1162 - 1166},
  Publisher                = {IEEE},

  Abstract                 = {{A} novel method based on adaptive histogram is proposed for follow-up tracking moving object under intense disturbance with an active camera. {T}he {M}ean {S}hift algorithm shows an excellent performance on robust and fast object tracking, however, it is prone to fail when facing intense disturbance from background. {T}his paper addresses to solve these problems. {A}t first how the number of dimensions and the number of bins of histogram affects the target representation is analyzed, then an adaptive selection strategy for dimensions number and bins number is proposed depending on the intensity of disturbance from background. {T}o follow-up tracking moving object with an active camera, a closed loop control model based on speed regulation is proposed to drive a {PTZ} camera to center the target. {T}he results of experiments show that the active camera can follow-up track moving target stably, even when encountering large area intense disturbance from background. {T}he algorithm is computationally efficient and can run in realtime speed.},
  Doi                      = {10.1109/ICMA.2009.5246399},
  File                     = {:../sources/05246399.pdf:PDF},
  Keywords                 = {Image processing, Target tracking, Video signal processing, Adaptive histogram, Intense disturbance, Mean Shift, Speed regulation},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@Article{Yilmaz2006,
  Title                    = {{O}bject {T}racking: {A} {S}urvey},
  Author                   = {Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
  Journal                  = {ACM Computing Surveys},
  Year                     = {2006},

  Month                    = {December},
  Number                   = {4},
  Pages                    = {1-45},
  Volume                   = {38},

  Abstract                 = {{T}he goal of this article is to review the state-of-the-art tracking methods, classify them into different categories, and identify new trends. {O}bject tracking, in general, is a challenging problem. {D}ifficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. {T}racking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. {T}ypically, assumptions are made to constrain the tracking problem in the context of a particular application. {I}n this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. {M}oreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
  Doi                      = {10.1145/1177352.1177355},
  File                     = {:../sources/Yilmaz.pdf:PDF},
  Keywords                 = {Appearance models, contour evolution, feature selection, object detection, object representation, point tracking, shape tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.11}
}

@InProceedings{Yu2008,
  Title                    = {{D}etection and tracking of moving object with a mobile robot using laser scanner},
  Author                   = {Yu, Jin-Xia and Cai, Zi-xing and Duan, Zhuo-Hua},
  Booktitle                = {International Conference on Machine Learning and Cybernetics},
  Year                     = {2008},

  Address                  = {China, Kunming},
  Month                    = {July},
  Pages                    = {1947 - 1952},
  Publisher                = {IEEE},
  Volume                   = {4},

  Abstract                 = {{A}n autonomous approach for detection and tracking of moving object with a mobile robot using laser scanner is presented in this paper. {F}irstly, ranging data of environmental objects from laser scanner are clustered according to the dynamic clustering method of k-nearest neighbors. {T}hen, the movement parameter of clustering objects is computed by local grid map matching. {A}t the same time, the movement compensation of mobile robot is estimated. {A}fter obtaining the moving object, particle filter ({PF}) with the improved proposal distribution is adopted to track moving object so as to get the movement condition of the object. {A}t last, experiments with the mobile robot designed by us are implemented and the validity of this approach is verified.},
  Doi                      = {10.1109/ICMLC.2008.4620725},
  File                     = {:../sources/04620725.pdf:PDF},
  Keywords                 = {Change detection algorithms, Cybernetics, Laser theory, Machine learning, Mobile robots, Motion detection, Object detection, Particle filters, Particle tracking, Proposals},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.12.08}
}

@InProceedings{Zhang2014,
  Title                    = {{UAV} tracking moving target scene using on-board {ISAR} sensor},
  Author                   = {Zhang, Liren and Karam, Ahmed},
  Booktitle                = {10th International Conference on Innovations in Information Technology},
  Year                     = {2014},

  Address                  = {United Arab Emirates, Al Ain},
  Month                    = {November},
  Pages                    = {88 - 92},
  Publisher                = {IEEE},

  Abstract                 = {{C}ompressive sensing ({CS}) based {I}nverse {S}ynthetic {A}perture {R}adar ({ISAR}) imaging exploits the sparsity of the target scene to achieve high resolution and effective denoising with limited measurements. {T}his paper extends the {CS} based {ISAR} imaging to further include the continuity structure of the target scene within a {B}ayesian framework. {A} correlated prior is imposed to statistically encourage the continuity structures in both the cross-range and range domains of the target region and the {G}ibbs sampling strategy is used for {B}ayesian inference. {B}ecause the resulted method requires to recover the whole target scene at a time with heavy computational complexity, an approximate strategy is proposed to alleviate the computational burden. {E}xperimental results demonstrate that the proposed algorithm can achieve substantial improvements in terms of preserving the weak scatterers and removing noise over other reported {CS} based {ISAR} imaging algorithms.},
  Doi                      = {10.1109/INNOVATIONS.2014.6987568},
  File                     = {:../sources/06987568.pdf:PDF},
  Keywords                 = {Approximation algorithms, Bayes methods, Compressed sensing, Imaging, Noise, Radar imaging, Signal processing algorithms},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Zhang2010,
  Title                    = {{A}n effective approach for active tracking with a {PTZ} camera},
  Author                   = {Zhang, Lei and Xu, Ke and Yu, Shiqi and Fu, Ruiqing Fu and Xu, Yangsheng},
  Booktitle                = {IEEE International Conference on Robotics and Biomimetics},
  Year                     = {2010},

  Address                  = {Tianjin, China},
  Month                    = {December},
  Pages                    = {1768 - 1773},
  Publisher                = {IEEE},

  Abstract                 = {{T}he concept of active tracking is presented to simulate the characteristics of human vision in intelligent visual surveillance. {T}he {P}an/{T}ilt/{Z}oom ({PTZ}) camera is generally used for active tracking. {I}n this paper, we present a novel and effective approach for active object tracking with a {PTZ} camera, and construct a near real-time system for indoor and outdoor scenes. {T}he tracking algorithm of our system is based on the feature matching, with the {PID} control to drive the camera. {T}he feature extracted from moving people is described as a region covariance matrix which combines the spatial and statistical properties of the targets (e.g. coordinates, color, and gradient). {R}esults from indoor and outdoor experiments demonstrate the effectiveness and accuracy of our approach.},
  Doi                      = {10.1109/ROBIO.2010.5723599},
  File                     = {:../sources/05723599.pdf:PDF},
  Keywords                 = {Cameras, Covariance matrix, Image color analysis, Image sequences, Pixel, Target tracking},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@InProceedings{Zhao2013,
  Title                    = {{V}ision based ground target tracking for rotor {UAV}},
  Author                   = {Zhao, Xuqiang and Fei, Qing and Geng, Qingbo},
  Booktitle                = {10th IEEE International Conference on Control and Automation (ICCA)},
  Year                     = {2013},

  Address                  = {China, Hangzhou},
  Month                    = {June},
  Pages                    = {1907 - 1911},
  Publisher                = {IEEE},

  Abstract                 = {{T}his paper studies an efficient ground target tracking algorithm for rotor {U}nmanned {A}erial {V}ehicle ({UAV}) to overcome the contradiction among the target tracking rapidity, precision and robustness for aerial vehicle. {F}irstly, {S}cale {I}nvariant {F}eature {T}ransform ({SIFT}) algorithm, which has a better robust performance during rotation, scaling and changes of illumination, is utilized to extract and match the feature points in order to realize target recognition and positioning. {S}econdly, using top-down tracking method, {K}alman filter is combined to estimate the target position in the next frame and search target in the predicted area, it can avoid blind matching, improve tracking rapidity and reduce the ratio of losing target. {F}inally, an experimental platform of rotor {UAV} visual tracking is set up and the ground target tracking algorithm is tested. {T}he experiment results show that the algorithm can achieve ground target tracking effectively and has good real-time performance and robustness.},
  Doi                      = {10.1109/ICCA.2013.6565085},
  File                     = {:../sources/06565085.pdf:PDF},
  Keywords                 = {Equations, Kalman filters, Mathematical model, Rotors, Target recognition, Target tracking, Visualization},
  Owner                    = {Stanisław Maciąg},
  Timestamp                = {2015.05.20}
}

@comment{jabref-meta: fileDirectory:.;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Analiza literaturowa\;0\;Alenya2014\;Bahn2011\;Campoy2
009\;Chen2012\;Deng2010\;Deng2010a\;Fernandez-Caballero2010\;Germa2010
\;Han2013\;Hashimoto2007\;Hu2015\;Hwang2010\;Kim2011\;Kim2012\;Lee2011
\;Li2007\;Liem2008\;Liu2014\;Markovic2014\;Mohamed2014\;Murakami2012\;
Olivares-Mendez2009\;Park2010\;Park2011\;Rui2009\;Sadeghi-Tehran2014\;
Tinh2014\;Yu2008\;Zhao2013\;;
2 ExplicitGroup:Przykłady - Przetwarzanie obrazów\;0\;Campoy2009\;Fern
andez-Caballero2010\;Hu2015\;Liem2008\;Markovic2014\;Mohamed2014\;Oliv
ares-Mendez2009\;Park2011\;Sadeghi-Tehran2014\;Zhao2013\;;
2 ExplicitGroup:Systemy sterowania\;0\;Bahn2011\;Campoy2009\;Chen2012\
;Deng2010\;Deng2010a\;Germa2010\;Han2013\;Hashimoto2007\;Hwang2010\;Ki
m2011\;Kim2012\;Lee2011\;Li2007\;Liem2008\;Liu2014\;Markovic2014\;Mura
kami2012\;Olivares-Mendez2009\;Park2010\;Park2011\;Rui2009\;Tinh2014\;
Yu2008\;;
2 ExplicitGroup:Przetwarzanie obrazów\;0\;;
3 ExplicitGroup:Monografie\;0\;Jaehne2005\;Pratt2007\;Szeliski2011\;Tr
eiber2010\;;
3 ExplicitGroup:Śledzenie obiektów\;0\;Smeulders2010\;Yilmaz2006\;;
3 ExplicitGroup:Ekstrakcja i deskrypcja cech\;0\;Campoy2009\;Lowe2004\
;Szeliski2011\;Treiber2010\;;
3 ExplicitGroup:Przepływ optyczny\;0\;Baker2011\;Jaehne2005\;Karasulu2
013\;Szeliski2011\;;
3 ExplicitGroup:Przykłady\;0\;;
1 ExplicitGroup:Kinematyka - pan-tilt\;0\;AlHaj2010\;Deng2010\;Doyle20
14\;Kim2012\;Murray1994\;;
}

